<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>XTong notes</title>
  
  <subtitle>Live beautifully, dream passionately, love completely</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="https://tongfan.xyz/"/>
  <updated>2019-04-26T13:31:54.478Z</updated>
  <id>https://tongfan.xyz/</id>
  
  <author>
    <name>Auggie Wang</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>HBASE中数据迁入迁出</title>
    <link href="https://tongfan.xyz/2019/03/10/hbase(4)%E2%80%94%E2%80%94snapshot/"/>
    <id>https://tongfan.xyz/2019/03/10/hbase(4)——snapshot/</id>
    <published>2019-03-10T07:05:58.000Z</published>
    <updated>2019-04-26T13:31:54.478Z</updated>
    
    <content type="html"><![CDATA[<p><strong>摘要</strong>：最近在集群中间进行数据的导入导出工作，接触到了Export工具和hbase的snapshot特性，下面对两种工具进行较为详细的说明，有什么错误之处希望大家及时交流，共同学习进步。</p><a id="more"></a><h1 id="export-amp-import"><a href="#export-amp-import" class="headerlink" title="export &amp; import"></a>export &amp; import</h1><h2 id="export"><a href="#export" class="headerlink" title="export"></a>export</h2><p>export工具使用比较简单，对hbase table在hdfs数据的序列化文件导出，其原理就是利用了hbase支持EndPoint和MapReduce程序两种方式而来的，使用就是hbase支持MapReduce这一特性，通过MapReduce程序实现，同时通过hbase Coprocessor的EndPoint也可以实现table的导出备份。</p><p><strong>MapReduce的使用方式</strong>：<code>bin/hbase org.apache.hadoop.hbase.mapreduce.Export &lt;tablename&gt; &lt;outputdir&gt; [&lt;versions&gt; [&lt;starttime&gt; [&lt;endtime&gt;]]]</code>，在进行数据不定期导出并且导出表的数量较少时，临时使用这种方式很方便。</p><p><strong>EndPoint的使用方式</strong>：<code>$ bin/hbase org.apache.hadoop.hbase.coprocessor.Export &lt;tablename&gt; &lt;outputdir&gt; [&lt;versions&gt; [&lt;starttime&gt; [&lt;endtime&gt;]]]</code>，在使用EndPoint方式时要注意，export coprocessor是可用的——要将<code>org.apache.hadoop.hbase.coprocessor.Export</code>添加到<code>hbase.coprocessor.region.classes</code>里边。</p><p>两种导出方式都是将table导出到hdfs上，下面进行基于EndPoint方式和基于MapReduce方式的两种导出方式的比较。</p><table><thead><tr><th style="text-align:center"></th><th style="text-align:center">EndPoint-based export</th><th style="text-align:center">MapReduce-based export</th></tr></thead><tbody><tr><td style="text-align:center">hbase版本支持</td><td style="text-align:center">2.0+</td><td style="text-align:center">0.2.1+</td></tr><tr><td style="text-align:center">maven依赖</td><td style="text-align:center">hbase-endpoint</td><td style="text-align:center">hbase-mapreduce(2.0+), hbase-server(-2.0)</td></tr><tr><td style="text-align:center">前置操作</td><td style="text-align:center">挂载EndPoint在target table上</td><td style="text-align:center">部署MapReduce框架</td></tr><tr><td style="text-align:center">读延迟</td><td style="text-align:center">低，直接从region上读取数据</td><td style="text-align:center">正常，传统的rpc scan</td></tr><tr><td style="text-align:center">读扩展性</td><td style="text-align:center">依赖于region的数量</td><td style="text-align:center">依赖于mapper的数量(TableInputFormatBase#getSplit)</td></tr><tr><td style="text-align:center">超时</td><td style="text-align:center">操作超时，hbase.client.operation.timeout配置</td><td style="text-align:center">scan超时，hbase.client.scanner.timeout.period配置</td></tr><tr><td style="text-align:center">权限设置</td><td style="text-align:center">读，执行</td><td style="text-align:center">读</td></tr><tr><td style="text-align:center">容错</td><td style="text-align:center">没有</td><td style="text-align:center">依赖于MapReduce框架</td></tr></tbody></table><p>注意：hbase执行MapReduce程序的过程中，其map的数量由TableInputFormatBase类中的getSplits决定，一般是一个split对应一个完整的region，每个表中的开始region是没有startrow的，对应的最后一个没有endrow，详细地可以看一下splits这个方法的源码。</p><p>默认情况下，export工具不管存储数据版本的多少，仅仅导出给定cell的最新版本，为了导出多版本的数据，可以通过\<versions>参数指定版本的数量。<strong>input scan 缓存的设置可以通过 hbase.client.scanner.caching 参数来进行指定</strong>。</versions></p><p><strong>附加内容</strong>：由于我们使用的hbase版本为1.2.2，通过命令来查看了一下hbase通过MapReduce方式进行export可以指定的参数，具体参数如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">Usage: Export [-D &lt;property=value&gt;]* &lt;tablename&gt; &lt;outputdir&gt; [&lt;versions&gt; [&lt;starttime&gt; [&lt;endtime&gt;]] [^[regex pattern] or [Prefix] to filter]]</span><br><span class="line"></span><br><span class="line">  Note: -D properties will be applied to the conf used. </span><br><span class="line">  For example: </span><br><span class="line">   -D mapreduce.output.fileoutputformat.compress=<span class="literal">true</span></span><br><span class="line">   -D mapreduce.output.fileoutputformat.compress.codec=org.apache.hadoop.io.compress.GzipCodec</span><br><span class="line">   -D mapreduce.output.fileoutputformat.compress.type=BLOCK</span><br><span class="line">  Additionally, the following SCAN properties can be specified</span><br><span class="line">  to control/<span class="built_in">limit</span> what is exported..</span><br><span class="line">   -D hbase.mapreduce.scan.column.family=&lt;familyName&gt;</span><br><span class="line">   -D hbase.mapreduce.include.deleted.rows=<span class="literal">true</span></span><br><span class="line">   -D hbase.mapreduce.scan.row.start=&lt;ROWSTART&gt;</span><br><span class="line">   -D hbase.mapreduce.scan.row.stop=&lt;ROWSTOP&gt;</span><br><span class="line">For performance consider the following properties:</span><br><span class="line">   -Dhbase.client.scanner.caching=100</span><br><span class="line">   -Dmapreduce.map.speculative=<span class="literal">false</span></span><br><span class="line">   -Dmapreduce.reduce.speculative=<span class="literal">false</span></span><br><span class="line">For tables with very wide rows consider setting the batch size as below:</span><br><span class="line">   -Dhbase.export.scanner.batch=10</span><br></pre></td></tr></table></figure><h2 id="import"><a href="#import" class="headerlink" title="import"></a>import</h2><p>介绍完export，大家其实对import的操作基本上也就了解的差不多了，import就是export操作的逆过程，其基本命令如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hbase org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;inputdir&gt;</span><br></pre></td></tr></table></figure><p>如果是不同hbase版本的集群，可以通过这个命令来进行导入，但是需要对导入的hbase table文件的hbase 版本进行指定，例如hbase 0.94的导出文件要导入hbase 0.96的hbase集群中，那么其执行命令如下:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hbase -Dhbase.import.version=0.94 org.apache.hadoop.hbase.mapreduce.Import &lt;tablename&gt; &lt;inputdir&gt;</span><br></pre></td></tr></table></figure><p>附加内容：</p><p>对hbase 1.2.2的import的参数进行说明如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">usage: general options are:</span><br><span class="line"> -archives &lt;paths&gt;              comma separated archives to be unarchived</span><br><span class="line">                                on the compute machines.</span><br><span class="line"> -conf &lt;configuration file&gt;     specify an application configuration file</span><br><span class="line"> -D &lt;property=value&gt;            use value <span class="keyword">for</span> given property</span><br><span class="line"> -files &lt;paths&gt;                 comma separated files to be copied to the</span><br><span class="line">                                map reduce cluster</span><br><span class="line"> -fs &lt;<span class="built_in">local</span>|namenode:port&gt;      specify a namenode</span><br><span class="line"> -jt &lt;<span class="built_in">local</span>|jobtracker:port&gt;    specify a job tracker</span><br><span class="line"> -libjars &lt;paths&gt;               comma separated jar files to include <span class="keyword">in</span></span><br><span class="line">                                the classpath.</span><br><span class="line"> -tokenCacheFile &lt;tokensFile&gt;   name of the file with the tokens</span><br><span class="line">ERROR: Wrong number of arguments: 0</span><br><span class="line">Usage: Import [options] &lt;tablename&gt; &lt;inputdir&gt;</span><br><span class="line">By default Import will load data directly into HBase. To instead generate</span><br><span class="line">HFiles of data to prepare <span class="keyword">for</span> a bulk data load, pass the option:</span><br><span class="line">  -Dimport.bulk.output=/path/<span class="keyword">for</span>/output</span><br><span class="line"> To apply a generic org.apache.hadoop.hbase.filter.Filter to the input, use</span><br><span class="line">  -Dimport.filter.class=&lt;name of filter class&gt;</span><br><span class="line">  -Dimport.filter.args=&lt;comma separated list of args <span class="keyword">for</span> filter</span><br><span class="line"> NOTE: The filter will be applied BEFORE doing key renames via the HBASE_IMPORTER_RENAME_CFS property. Futher, filters will only use the Filter<span class="comment">#filterRowKey(byte[] buffer, int offset, int length) method to identify  whether the current row needs to be ignored completely for processing and  Filter#filterKeyValue(KeyValue) method to determine if the KeyValue should be added; Filter.ReturnCode#INCLUDE and #INCLUDE_AND_NEXT_COL will be considered as including the KeyValue.</span></span><br><span class="line">To import data exported from HBase 0.94, use</span><br><span class="line">  -Dhbase.import.version=0.94</span><br><span class="line">For performance consider the following options:</span><br><span class="line">  -Dmapreduce.map.speculative=<span class="literal">false</span></span><br><span class="line">  -Dmapreduce.reduce.speculative=<span class="literal">false</span></span><br><span class="line">  -Dimport.wal.durability=&lt;Used <span class="keyword">while</span> writing data to hbase. Allowed values are the supported durability values like SKIP_WAL/ASYNC_WAL/SYNC_WAL/</span><br><span class="line">...&gt;</span><br></pre></td></tr></table></figure><h2 id="importTsv"><a href="#importTsv" class="headerlink" title="importTsv"></a>importTsv</h2><p>当看到tsv的时候，我并不知道是什么意思，这个有点尴尬，下面进行一下解释，tsv就是table separate value，当然了csv就是comma separate value，这个也是很好理解，看来平时的积累还是不够，废话不多说，介绍hbase自带的这个功能，顾名思义这个命令就是import的近亲，它有两种使用方式，一种是通过puts将tsv格式的数据从hdfs加载到hbase，另外一种是将数据转变成storefile通过completebulkload来进行数据加载。</p><p>1、通过puts进行数据加载（不是bulk load的方式）:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=a,b,c</span><br><span class="line">&lt;tablename&gt; &lt;hdfs-inputdir&gt;</span><br></pre></td></tr></table></figure><p>2、通过bulk-loading生成storefile的方式：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hbase org.apache.hadoop.hbase.mapreduce.ImportTsv -Dimporttsv.columns=a,b,c</span><br><span class="line">-Dimporttsv.bulk.output=hdfs://storefile-outputdir &lt;tablename&gt; &lt;hdfs-data-inputdir&gt;</span><br></pre></td></tr></table></figure><p>importTsv的参数内容，如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line">Usage: importtsv -Dimporttsv.columns=a,b,c &lt;tablename&gt; &lt;inputdir&gt;</span><br><span class="line"></span><br><span class="line">Imports the given input directory of TSV data into the specified table.</span><br><span class="line"></span><br><span class="line">The column names of the TSV data must be specified using the -Dimporttsv.columns</span><br><span class="line">option. This option takes the form of comma-separated column names, <span class="built_in">where</span> each</span><br><span class="line">column name is either a simple column family, or a columnfamily:qualifier. The special</span><br><span class="line">column name HBASE_ROW_KEY is used to designate that this column should be used</span><br><span class="line">as the row key <span class="keyword">for</span> each imported record. You must specify exactly one column</span><br><span class="line">to be the row key, and you must specify a column name <span class="keyword">for</span> every column that exists <span class="keyword">in</span> the</span><br><span class="line">input data. Another special columnHBASE_TS_KEY designates that this column should be</span><br><span class="line">used as timestamp <span class="keyword">for</span> each record. Unlike HBASE_ROW_KEY, HBASE_TS_KEY is optional.</span><br><span class="line">You must specify at most one column as timestamp key <span class="keyword">for</span> each imported record.</span><br><span class="line">Record with invalid timestamps (blank, non-numeric) will be treated as bad record.</span><br><span class="line">Note: <span class="keyword">if</span> you use this option, <span class="keyword">then</span> <span class="string">'importtsv.timestamp'</span> option will be ignored.</span><br><span class="line"></span><br><span class="line">Other special columns that can be specified are HBASE_CELL_TTL and HBASE_CELL_VISIBILITY.</span><br><span class="line">HBASE_CELL_TTL designates that this column will be used as a Cell<span class="string">'s Time To Live (TTL) attribute.</span></span><br><span class="line"><span class="string">HBASE_CELL_VISIBILITY designates that this column contains the visibility label expression.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">HBASE_ATTRIBUTES_KEY can be used to specify Operation Attributes per record.</span></span><br><span class="line"><span class="string"> Should be specified as key=&gt;value where -1 is used </span></span><br><span class="line"><span class="string"> as the seperator.  Note that more than one OperationAttributes can be specified.</span></span><br><span class="line"><span class="string">By default importtsv will load data directly into HBase. To instead generate</span></span><br><span class="line"><span class="string">HFiles of data to prepare for a bulk data load, pass the option:</span></span><br><span class="line"><span class="string">  -Dimporttsv.bulk.output=/path/for/output</span></span><br><span class="line"><span class="string">  Note: if you do not use this option, then the target table must already exist in HBase</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">Other options that may be specified with -D include:</span></span><br><span class="line"><span class="string">  -Dimporttsv.skip.bad.lines=false - fail if encountering an invalid line</span></span><br><span class="line"><span class="string">  '</span>-Dimporttsv.separator=|<span class="string">' - eg separate on pipes instead of tabs</span></span><br><span class="line"><span class="string">  -Dimporttsv.timestamp=currentTimeAsLong - use the specified timestamp for the import</span></span><br><span class="line"><span class="string">  -Dimporttsv.mapper.class=my.Mapper - A user-defined Mapper to use instead of org.apache.hadoop.hbase.mapreduce.TsvImporterMapper</span></span><br><span class="line"><span class="string">  -Dmapreduce.job.name=jobName - use the specified mapreduce job name for the import</span></span><br><span class="line"><span class="string">  -Dcreate.table=no - can be used to avoid creation of table by this tool</span></span><br><span class="line"><span class="string">  Note: if you set this to '</span>no<span class="string">', then the target table must already exist in HBase</span></span><br><span class="line"><span class="string">  -Dno.strict=true - ignore column family check in hbase table. Default is false</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">For performance consider the following options:</span></span><br><span class="line"><span class="string">  -Dmapreduce.map.speculative=false</span></span><br><span class="line"><span class="string">  -Dmapreduce.reduce.speculative=false</span></span><br></pre></td></tr></table></figure><h3 id="示例"><a href="#示例" class="headerlink" title="示例"></a>示例</h3><p>如例，有表datatsv，列簇d和两列c1、c2，数据格式如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">row1 c1 c2</span><br><span class="line">row2 c1 c2</span><br></pre></td></tr></table></figure><p>其bulk-load加载方式的命令如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_CLASSPATH=`<span class="variable">$&#123;HBASE_HOME&#125;</span>/bin/hbase classpath` <span class="variable">$&#123;HADOOP_HOME&#125;</span>/bin/hadoop jar</span><br><span class="line"><span class="variable">$&#123;HBASE_HOME&#125;</span>/hbase-mapreduce-VERSION.jar importtsv</span><br><span class="line">-Dimporttsv.columns=HBASE_ROW_KEY,d:c1,d:c2</span><br><span class="line">-Dimporttsv.bulk.output=hdfs://storefileoutput datatsv hdfs://inputfile</span><br></pre></td></tr></table></figure><p>注意：如果要通过这种方式导入大量数据，首先要确保hbase的表预分区已经做了，否则会造成split-merge  storm。</p><h3 id="CompleteBulkLoad"><a href="#CompleteBulkLoad" class="headerlink" title="CompleteBulkLoad"></a>CompleteBulkLoad</h3><p>completebulkload将生成的StoreFiles导入hbase table，有如下两种方式：</p><p>explicit classname:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hbase org.apache.hadoop.hbase.tool.LoadIncrementalHFiles</span><br><span class="line">&lt;hdfs://storefileoutput&gt; &lt;tablename&gt;</span><br></pre></td></tr></table></figure><p>driver:</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">HADOOP_CLASSPATH=`<span class="variable">$&#123;HBASE_HOME&#125;</span>/bin/hbase classpath` <span class="variable">$&#123;HADOOP_HOME&#125;</span>/bin/hadoop jar</span><br><span class="line"><span class="variable">$&#123;HBASE_HOME&#125;</span>/hbase-server-VERSION.jar completebulkload &lt;hdfs://storefileoutput&gt;</span><br><span class="line">&lt;tablename&gt;</span><br></pre></td></tr></table></figure><p>注意：通过MapReduce生成的数据文件通常和hbase的处理线程是不兼容的，所以hdfs权限是可用的，在运行completeBulkLoad之前对这些权限必须要进行更新。</p><h1 id="snapshot"><a href="#snapshot" class="headerlink" title="snapshot"></a>snapshot</h1><p>hbase 快照允许对某一时刻的表信息进行备份，并且这种方式是资源消耗最小的一种。snapshot是table元数据的不变集合并且获取快照那一时刻table的hfile列表，snapshot克隆操作会根据该snapshot创建一个新table，而恢复操作会将快照中的数据恢复至表中。“克隆”和“恢复”操作并不需要任何的数据拷贝，下层的hfile并不会进行任何的修改操作。相似地，将一个snapshot导入至另外一个集群会对本集群的regionserver有小的影响。</p><p>hbase0.94.6之前的版本，想要对表进行克隆备份只能通过CopyTable和ExportTable，或者将表下线后，拷贝这个表的所有的hfile文件，这种操作的会导致region server性能下降，或者你需要对表进行下线操作，造成该表的读写受影响。</p><p>在使用hbase snapshot属性时，需要进行配置，在hbase的版本是0.95+是默认开启的：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">name</span>&gt;</span>hbase.snapshot.enabled<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">value</span>&gt;</span>true<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure><h2 id="快照生成"><a href="#快照生成" class="headerlink" title="快照生成"></a>快照生成</h2><p>命令行生成快照的命令如下，这个操作会生成快照，不论table是否在线，同时不会涉及数据的拷贝操作。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ ./bin/hbase shell</span><br><span class="line">hbase&gt; snapshot <span class="string">'myTable'</span>, <span class="string">'myTableSnapshot-122112'</span></span><br></pre></td></tr></table></figure><p>###无刷写生成快照</p><p>默认情况下，在生成快照之前，会执行一次hbase memstore的数据刷写，这意味着快照的内容会包含内存中的数据，大多数情况下，这是期望的操作。然而，如果你在内存中设置了容错数据并且期望不包含在snapshot中，可以使用<code>skip_flush</code>属性在生成快照时不进行数据的刷写操作，具体命令如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase&gt; snapshot <span class="string">'mytable'</span>, <span class="string">'snapshot123'</span>, &#123;SKIP_FLUSH =&gt; <span class="literal">true</span>&#125;</span><br></pre></td></tr></table></figure><p><strong>注意</strong>：无论flush是否开启，都无法确定或者预测一个并发的插入或者更新操作是否会包含在给定的snapshot中，也就是说，简而言之两个东西没有直接关系；snapshot仅仅是一个表在一段时间窗口的呈现，而且生成snapshot中到达每个regionserver的时间从几秒到几分钟是不确定的，这个依赖于资源加载、硬件、网络等因素，当然了一个给定的插入或者更新操作是在内存中还是已经刷写到磁盘上这个也是不可知的。</p><h2 id="list-snapshot"><a href="#list-snapshot" class="headerlink" title="list snapshot"></a>list snapshot</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase&gt; list_snapshots</span><br></pre></td></tr></table></figure><h2 id="delete-snapshot"><a href="#delete-snapshot" class="headerlink" title="delete snapshot"></a>delete snapshot</h2><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase&gt; delete_snapshot <span class="string">'myTableSnapshot-122112'</span></span><br></pre></td></tr></table></figure><h2 id="从snapshot克隆table"><a href="#从snapshot克隆table" class="headerlink" title="从snapshot克隆table"></a>从snapshot克隆table</h2><p>从一个快照可以生成一张新表，当然这张新表中包含了snapshot生成之前的所有的数据，克隆操作并不涉及到数据的复制迁移，当然克隆出来的新表并不会对原始表或者快照有影响。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hbase&gt; clone_snapshot <span class="string">'myTableSnapshot-122112'</span>, <span class="string">'myNewTestTable'</span></span><br></pre></td></tr></table></figure><h2 id="restore-snapshot"><a href="#restore-snapshot" class="headerlink" title="restore snapshot"></a>restore snapshot</h2><p>restore操作需要一个表示disable的，表将会被恢复至快照生成之刻的状态和数据，如果需要的话还会更改表的数据和schema。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">hbase&gt; <span class="built_in">disable</span> <span class="string">'myTable'</span></span><br><span class="line">hbase&gt; restore_snapshot <span class="string">'myTableSnapshot-122112'</span></span><br></pre></td></tr></table></figure><p><strong>注意</strong>： 因为副本在log层面 work，而snapshot是在文件系统层面，所以在恢复操作之后，副本将会和master节点的状态不同，如果要使用snapshot恢复表中的数据，需要停止replication并且重启bootstrap。</p><h2 id="snapshot的操作和ACLs"><a href="#snapshot的操作和ACLs" class="headerlink" title="snapshot的操作和ACLs"></a>snapshot的操作和ACLs</h2><p>如果使用访问控制<code>AccessController</code>来进行安全配置，那么仅仅全局管理员可以执行生成、克隆和恢复快照的操作，同时这些操作不会占用/捕获（capture）acl权限，这意味着恢复表的操作会保护已存在表的ACL权限，同时如果管理员不添加权限，由snapshot克隆新表也是没有权限的。</p><h2 id="导至其他集群"><a href="#导至其他集群" class="headerlink" title="导至其他集群"></a>导至其他集群</h2><p>ExportSnapshot工具可以复制和snapshot相关数据（hfile， logs， snapshot metadata）至另一个集群，工具执行MR job， 类似于distcp工具，因为snapshot是文件系统层面的，所以hbase集群不必是online的。命令如下：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot MySnapshot -copy-to hdfs://srv2:8082/hbase -mappers 16</span><br></pre></td></tr></table></figure><p>这样就把一个叫snapshot的的快照复制到了srv2集群。当然在这个命令中还包括了多个命令参数，可以通过bandwidth来限制复制过程中的带宽使用。</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">$ bin/hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot -snapshot MySnapshot -copy</span><br><span class="line">-to hdfs://srv2:8082/hbase -mappers 16 -bandwidth 200</span><br></pre></td></tr></table></figure><p>下面是对hbase这个工具参数的介绍：</p><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">Usage: bin/hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot [options]</span><br><span class="line"> <span class="built_in">where</span> [options] are:</span><br><span class="line">  -h|-<span class="built_in">help</span>                Show this <span class="built_in">help</span> and <span class="built_in">exit</span>.</span><br><span class="line">  -snapshot NAME          Snapshot to restore.</span><br><span class="line">  -copy-to NAME           Remote destination hdfs://</span><br><span class="line">  -copy-from NAME         Input folder hdfs:// (default hbase.rootdir)</span><br><span class="line">  -no-checksum-verify     Do not verify checksum, use name+length only.</span><br><span class="line">  -no-target-verify       Do not verify the integrity of the \exported snapshot.</span><br><span class="line">  -overwrite              Rewrite the snapshot manifest <span class="keyword">if</span> already exists</span><br><span class="line">  -chuser USERNAME        Change the owner of the files to the specified one.</span><br><span class="line">  -chgroup GROUP          Change the group of the files to the specified one.</span><br><span class="line">  -chmod MODE             Change the permission of the files to the specified one.</span><br><span class="line">  -mappers                Number of mappers to use during the copy (mapreduce.job.maps).</span><br><span class="line">  -bandwidth              Limit bandwidth to this value <span class="keyword">in</span> MB/second.</span><br><span class="line"></span><br><span class="line">Examples:</span><br><span class="line">  hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot \</span><br><span class="line">    -snapshot MySnapshot -copy-to hdfs://srv2:8082/hbase \</span><br><span class="line">    -chuser MyUser -chgroup MyGroup -chmod 700 -mappers 16</span><br><span class="line"></span><br><span class="line">  hbase org.apache.hadoop.hbase.snapshot.ExportSnapshot \</span><br><span class="line">    -snapshot MySnapshot -copy-from hdfs://srv2:8082/hbase \</span><br><span class="line">    -copy-to hdfs://srv1:50070/hbase \</span><br></pre></td></tr></table></figure><p>上面每个命令都有较为完整的说明和示例，这里不再解释。当然hbase说明文档也说明了在Amazon的s3和微软的Auzure上的导出和恢复，我们现在还没有涉及到这一部分，因此不再这里介绍了。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本篇博客主要根据hbase的说明文档和自己最近导数据场景的实践，总结了两种在hbase集群中导入导出数据的两种方式，在进行hbase文档（2.0）查看时，发现hbase其实已经做了很多的工作，很多平时工作的场景基本在hbase中都有覆盖，平时没事还是要多多看看文档，多多积累，在碰到陌生场景是才不至于“惊慌”。</p><div class="note primary">            <h4 id="本文作者：tongtong"><a href="#本文作者：tongtong" class="headerlink" title="本文作者：tongtong"></a>本文作者：tongtong</h4><h4 id="本文链接：https-stongtong-github-io"><a href="#本文链接：https-stongtong-github-io" class="headerlink" title="本文链接：https://stongtong.github.io/"></a>本文链接：<a href="https://stongtong.github.io/">https://stongtong.github.io/</a></h4><h4 id="版权申明：网站内容为tongtong所有，转载请注明出处。"><a href="#版权申明：网站内容为tongtong所有，转载请注明出处。" class="headerlink" title="版权申明：网站内容为tongtong所有，转载请注明出处。"></a>版权申明：网站内容为tongtong所有，转载请注明出处。</h4>          </div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;：最近在集群中间进行数据的导入导出工作，接触到了Export工具和hbase的snapshot特性，下面对两种工具进行较为详细的说明，有什么错误之处希望大家及时交流，共同学习进步。&lt;/p&gt;
    
    </summary>
    
      <category term="Hbase" scheme="https://tongfan.xyz/categories/Hbase/"/>
    
    
      <category term="Hbase" scheme="https://tongfan.xyz/tags/Hbase/"/>
    
      <category term="snapshot" scheme="https://tongfan.xyz/tags/snapshot/"/>
    
      <category term="export" scheme="https://tongfan.xyz/tags/export/"/>
    
      <category term="import" scheme="https://tongfan.xyz/tags/import/"/>
    
  </entry>
  
  <entry>
    <title>HBASE中的数据刷写</title>
    <link href="https://tongfan.xyz/2019/02/17/HBASE%E4%B8%AD%E7%9A%84%E6%95%B0%E6%8D%AE%E5%88%B7%E5%86%99/"/>
    <id>https://tongfan.xyz/2019/02/17/HBASE中的数据刷写/</id>
    <published>2019-02-17T11:27:58.000Z</published>
    <updated>2019-02-18T12:18:01.725Z</updated>
    
    <content type="html"><![CDATA[<p><strong>摘要</strong>：由于HBASE中使用LSM，所以在HBASE中插入数据时，是批量从内存中flush到磁盘（HDFS），本文介绍总结HBASE中触发flush动作的详细条件，从而对flush过程进行优化。（本文参考自<a href="https://mp.weixin.qq.com/s/lEfTrbd7bB4Xi0s86drr3g">HBASE社区文章</a>）</p><a id="more"></a><h1 id="写数据过程简介"><a href="#写数据过程简介" class="headerlink" title="写数据过程简介"></a>写数据过程简介</h1><p>HBASE数据写入都是先将数据写入内存，在内存中存在memstore的结构（在写入内存之前会先通过wal进行日志的备份，相信大家都知道这个），memstore中的数据最终会持久化到磁盘，持久化的原理就是之前文章说的LSM数据结构（内存树与磁盘树的合并），最终在磁盘上以hfile的方式进行存储，这里附HBASE的一张整体结构图，该图较为清晰的对memstore、hfile以及其他HBASE的概念的关系进行说明。</p><p><img src="https://i.loli.net/2019/02/17/5c694ac4a683b.png" alt></p><p>理解 MemStore 的刷写对优化 MemStore 有很重要的意义，大部分人遇到的性能问题都是写操作被阻塞(Block)无法写入HBase。本文基于 HBase 2.0.2，并对 MemStore 的 Flush 进行说明，包括哪几种条件触发 Memstore Flush 和目前常见的刷写策略(FlushPolicy)。</p><h1 id="触发Flush条件"><a href="#触发Flush条件" class="headerlink" title="触发Flush条件"></a>触发Flush条件</h1><p>有很多情况会触发 MemStore 的 Flush 操作，所以我们最好需要了解每种情况在什么时候触发 Memstore Flush。总的来说，主要有以下几种情况会触发 Memstore Flush：</p><ul><li>Region中所有MemStore占用的内存超过阈值范围</li><li>整个RegionServer的MemStore占用内存总和大于相关阈值</li><li>WAL数量大于相关阈值</li><li>定期自动刷写</li><li>数据更新超过一定阈值</li><li>手动触发刷写</li></ul><p>下面对如上集中情况进行详细说明。</p><h2 id="Region中所有memstore占用内存超过阈值"><a href="#Region中所有memstore占用内存超过阈值" class="headerlink" title="Region中所有memstore占用内存超过阈值"></a>Region中所有memstore占用内存超过阈值</h2><p>当一个 Region 中所有 MemStore 占用的内存(包括 OnHeap + OffHeap)大小超过<strong>刷写阈值</strong>的时候会触发一次刷写，这个阈值由 <code>hbase.hregion.memstore.flush.size</code> 参数控制，默认为128MB。我们每次调用 put、delete 等操作都会检查的这个条件的。</p><p>但是如果我们的数据增加得很快，达到 <code>hbase.hregion.memstore.flush.size * hbase.hregion.memstore.block.multiplier</code> 的大小，<code>hbase.hregion.memstore.block.multiplier</code> 默认值为4，也就是128*4=512MB的时候，那么除了触发 MemStore 刷写之外，HBase 还会在刷写的时候同时阻塞所有写入该 Store 的写请求（这是因为写入速率太快，如果不限制写入数据的请求，很可能导致HBASE内存空间被占满的情况）！这时候如果你往对应的 Store 写数据，会出现 <code>RegionTooBusyException</code> 异常。</p><h2 id="RegionServer-的-MemStore-占用内存总和大于相关阈值"><a href="#RegionServer-的-MemStore-占用内存总和大于相关阈值" class="headerlink" title="RegionServer 的 MemStore 占用内存总和大于相关阈值"></a>RegionServer 的 MemStore 占用内存总和大于相关阈值</h2><p>HBase 为 RegionServer 的 MemStore 分配一定的写缓存，大小等于 <code>hbase_heapsize(RegionServer占用的堆内存大小)* hbase.regionserver.global.memstore.size</code>。<code>hbase.regionserver.global.memstore.size</code> 的默认值是 0.4，也就是说写缓存大概占用 RegionServer 整个 JVM 内存使用量的 40%。</p><p>如果整个 RegionServer 的 MemStore 占用内存总和大于 <code>hbase.regionserver.global.memstore.size.lower.limit * hbase.regionserver.global.memstore.size * hbase_heapsize</code> 的时候，将会触发 MemStore 的刷写。其中 <code>hbase.regionserver.global.memstore.size.lower.limit</code> 的默认值为 0.95。</p><p>举个例子，如果我们 HBase 堆内存总共是 32G，按照默认的比例，那么触发 RegionServer 级别的 Flush 是 RegionServer 中所有的 MemStore 占用内存为：32 <em> 0.4 </em> 0.95 = 12.16G。</p><p><strong>注意</strong>：0.99.0 之前 <code>hbase.regionserver.global.memstore.size</code> 是 <code>hbase.regionserver.global.memstore.upperLimit</code> 参数；<code>hbase.regionserver.global.memstore.size.lower.limit</code> 是 <code>hbase.regionserver.global.memstore.lowerLimit</code>，参见 HBASE-5349</p><p>RegionServer 级别的 Flush 策略是每次找到 RS 中占用内存最大的 Region 对它进行刷写，这个操作是循环进行的，直到总体内存的占用低于全局 MemStore 刷写下<br>限(<code>hbase.regionserver.global.memstore.size.lower.limit * hbase.regionserver.global.memstore.size * hbase_heapsize</code>)才会停止。</p><p>需要<strong>注意</strong>的是，如果达到了 RegionServer 级别的 Flush，那么当前 RegionServer 的所有写操作将会被阻塞，而且这个阻塞可能会持续到分钟级别。（敏感的人已经发现在这应该尽量避免RegionServer级别的flush）</p><h2 id="WAL数量大于相关阈值"><a href="#WAL数量大于相关阈值" class="headerlink" title="WAL数量大于相关阈值"></a>WAL数量大于相关阈值</h2><p>WAL(write-ahead log，预写日志)用来解决宕机之后的操作恢复问题的。数据到达 Region 的时候是先写入 WAL，然后再被写到 Memstore 的。如果 WAL 的数量越来越大，这就意味着 MemStore 中未持久化到磁盘的数据越来越多。当 RS 挂掉的时候，恢复时间将会变长，所以有必要在 WAL 到达一定的数量时进行一次刷写操作。阈值 maxLogs 的计算公式如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// Schedule a WAL roll when the WAL is 50% of the HDFS block size. Scheduling at 50% of block</span></span><br><span class="line"><span class="comment">// size should make it so WAL rolls before we get to the end-of-block (Block transitions cost</span></span><br><span class="line"><span class="comment">// some latency). In hbase-1 we did this differently. We scheduled a roll when we hit 95% of</span></span><br><span class="line"><span class="comment">// the block size but experience from the field has it that this was not enough time for the</span></span><br><span class="line"><span class="comment">// roll to happen before end-of-block. So the new accounting makes WALs of about the same</span></span><br><span class="line"><span class="comment">// size as those made in hbase-1 (to prevent surprise), we now have default block size as</span></span><br><span class="line"><span class="comment">// 2 times the DFS default: i.e. 2 * DFS default block size rolling at 50% full will generally</span></span><br><span class="line"><span class="comment">// make similar size logs to 1 * DFS default block size rolling at 95% full. See HBASE-19148.</span></span><br><span class="line"><span class="keyword">this</span>.blocksize = WALUtil.getWALBlockSize(<span class="keyword">this</span>.conf, <span class="keyword">this</span>.fs, <span class="keyword">this</span>.walDir);</span><br><span class="line"><span class="keyword">float</span> multiplier = conf.getFloat(<span class="string">"hbase.regionserver.logroll.multiplier"</span>, <span class="number">0.5f</span>);</span><br><span class="line"><span class="keyword">this</span>.logrollsize = (<span class="keyword">long</span>)(<span class="keyword">this</span>.blocksize * multiplier);</span><br><span class="line"><span class="keyword">this</span>.maxLogs = conf.getInt(<span class="string">"hbase.regionserver.maxlogs"</span>,</span><br><span class="line">      Math.max(<span class="number">32</span>, calculateMaxLogFiles(conf, logrollsize)));</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">  * Public because of FSHLog. Should be package-private</span></span><br><span class="line"><span class="comment">  * <span class="doctag">@param</span> isRecoverEdits the created writer is for recovered edits or WAL.</span></span><br><span class="line"><span class="comment">  *                       For recovered edits, it is true and for WAL it is false.</span></span><br><span class="line"><span class="comment">  */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">long</span> <span class="title">getWALBlockSize</span><span class="params">(Configuration conf, FileSystem fs, Path dir,</span></span></span><br><span class="line"><span class="function"><span class="params">     <span class="keyword">boolean</span> isRecoverEdits)</span> <span class="keyword">throws</span> IOException </span>&#123;</span><br><span class="line">  <span class="keyword">long</span> defaultBlockSize = CommonFSUtils.getDefaultBlockSize(fs, dir) * <span class="number">2</span>;</span><br><span class="line">  <span class="keyword">if</span> (isRecoverEdits) &#123;</span><br><span class="line">     <span class="keyword">return</span> conf.getLong(<span class="string">"hbase.regionserver.recoverededits.blocksize"</span>, defaultBlockSize);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> conf.getLong(<span class="string">"hbase.regionserver.hlog.blocksize"</span>, defaultBlockSize);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">calculateMaxLogFiles</span><span class="params">(Configuration conf, <span class="keyword">long</span> logRollSize)</span> </span>&#123;</span><br><span class="line">  Pair&lt;Long, MemoryType&gt; globalMemstoreSize = MemorySizeUtil.getGlobalMemStoreSize(conf);</span><br><span class="line">  <span class="keyword">return</span> (<span class="keyword">int</span>) ((globalMemstoreSize.getFirst() * <span class="number">2</span>) / logRollSize);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>也就是说，如果设置 <code>hbase.regionserver.maxlogs</code>，那就是这个参数的值；否则是 <code>max(32, hbase_heapsize * hbase.regionserver.global.memstore.size * 2 / logRollSize)</code>。如果某个 RegionServer 的 WAL 数量大于 maxLogs 就会触发 MemStore 的刷写。</p><p>WAL 数量触发的刷写策略是，找到最旧的 un-archived WAL 文件，并找到这个 WAL 文件对应的 Regions， 然后对这些 Regions 进行刷写。</p><h2 id="定期自动刷写"><a href="#定期自动刷写" class="headerlink" title="定期自动刷写"></a>定期自动刷写</h2><p>如果我们很久没有对 HBase 的数据进行更新，这时候就可以依赖定期刷写策略了。RegionServer 在启动的时候会启动一个线程 PeriodicMemStoreFlusher 每隔 <code>hbase.server.thread.wakefrequency</code> 时间检查属于这个 RegionServer 的 Region 有没有超过一定时间都没有刷写，这个时间是由 <code>hbase.regionserver.optionalcacheflushinterval</code> 参数控制的，默认是 3600000，单位ms，也就是1小时会进行一次刷写。如果设定为0，则意味着关闭定时自动刷写。</p><p>为了防止一次性有过多的 MemStore 刷写，定期自动刷写会有 0 ~ 5 分钟的延迟，具体参见 PeriodicMemStoreFlusher 类的实现。</p><h2 id="数据更新超过一定阈值"><a href="#数据更新超过一定阈值" class="headerlink" title="数据更新超过一定阈值"></a>数据更新超过一定阈值</h2><p>如果 HBase 的某个 Region 更新的很频繁，而且既没有达到自动刷写阀值，也没有达到内存的使用限制，但是内存中的更新数量已经足够多，比如超过 <code>hbase.regionserver.flush.per.changes</code> 参数配置，默认为30000000，那么也是会触发刷写的。</p><h2 id="手动触发刷写"><a href="#手动触发刷写" class="headerlink" title="手动触发刷写"></a>手动触发刷写</h2><p>除了 HBase 内部一些条件触发的刷写之外，我们还可以通过执行相关命令或 API 来触发 MemStore 的刷写操作。比如调用可以调用 Admin 接口提供的方法：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Flush a table. Synchronous operation.</span><br><span class="line">  *</span><br><span class="line">  * @param tableName table to flush</span><br><span class="line">  * @throws IOException if a remote or network exception occurs</span><br><span class="line">  */</span><br><span class="line">void flush(TableName tableName) throws IOException;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">  * Flush an individual region. Synchronous operation.</span><br><span class="line">  *</span><br><span class="line">  * @param regionName region to flush</span><br><span class="line">  * @throws IOException if a remote or network exception occurs</span><br><span class="line">  */</span><br><span class="line">void flushRegion(byte[] regionName) throws IOException;</span><br><span class="line"></span><br><span class="line">/**</span><br><span class="line">  * Flush all regions on the region server. Synchronous operation.</span><br><span class="line">  * @param serverName the region server name to flush</span><br><span class="line">  * @throws IOException if a remote or network exception occurs</span><br><span class="line">  */</span><br><span class="line">void flushRegionServer(ServerName serverName) throws IOException;</span><br></pre></td></tr></table></figure><p>分别对某张表、某个 Region 或者某个 RegionServer 进行刷写操作。也可以在 Shell 中通过执行 flush 命令：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hbase&gt; flush &apos;TABLENAME&apos;</span><br><span class="line">hbase&gt; flush &apos;REGIONNAME&apos;</span><br><span class="line">hbase&gt; flush &apos;ENCODED_REGIONNAME&apos;</span><br><span class="line">hbase&gt; flush &apos;REGION_SERVER_NAME&apos;</span><br></pre></td></tr></table></figure><p>需要注意的是，以上所有条件触发的刷写操作最后都会检查对应的 HStore 包含的 StoreFiles 文件超过 <code>hbase.hstore.blockingStoreFiles</code> 参数配置的个数，默认值是16。如果满足这个条件，那么当前刷写会被推迟到 <code>hbase.hstore.blockingWaitTime</code> 参数设置的时间后再刷写。在阻塞刷写的同时，HBase 还会请求 Split 或 Compaction 操作。（在flush之前，先对region进行分离或者合并操作）</p><h1 id="哪些操作触发-MemStore-Flush"><a href="#哪些操作触发-MemStore-Flush" class="headerlink" title="哪些操作触发 MemStore Flush"></a>哪些操作触发 <strong>MemStore Flush</strong></h1><p>我们常见的 put、delete、append、increment、调用 flush 命令、Region 分裂、Region Merge、bulkLoad HFiles 以及给表做快照操作都会对上面的相关条件做检查，以便判断要不要做刷写操作。</p><h1 id="MemStore-Flush策略-FlushPolicy"><a href="#MemStore-Flush策略-FlushPolicy" class="headerlink" title="MemStore Flush策略(FlushPolicy)"></a>MemStore Flush策略(FlushPolicy)</h1><p>在 HBase 1.1 之前，MemStore 刷写是 Region 级别的。就是说，如果要刷写某个 MemStore ，MemStore 所在的 Region 中其他 MemStore 也是会被一起刷写的！这会造成一定的问题，比如小文件问题，具体参见 <a href="https://mp.weixin.qq.com/s?__biz=MzA5MTc0NTMwNQ==&amp;mid=2650716323&amp;idx=1&amp;sn=ccace248f650d4a3bcc2ff60103fc4a1&amp;scene=21#wechat_redirect">《为什么不建议在 HBase 中使用过多的列族》</a>。针对这个问题，HBASE-10201/HBASE-3149引入列族级别的刷写。我们可以通过 <code>hbase.regionserver.flush.policy</code> 参数选择不同的刷写策略。</p><p>目前 HBase 2.0.2 的刷写策略全部都是实现 FlushPolicy 抽象类的。并且自带三种刷写策略：<code>FlushAllLargeStoresPolicy</code>、<code>FlushNonSloppyStoresFirstPolicy</code> 以及 <code>FlushAllStoresPolicy</code>。</p><h2 id="FlushAllStoresPolicy"><a href="#FlushAllStoresPolicy" class="headerlink" title="FlushAllStoresPolicy"></a><strong>FlushAllStoresPolicy</strong></h2><p>这种刷写策略实现最简单，直接返回当前 Region 对应的所有 MemStore。也就是每次刷写都是对 Region 里面所有的 MemStore 进行的，这个行为和 HBase 1.1 之前是一样的。</p><h2 id="FlushAllLargeStoresPolicy"><a href="#FlushAllLargeStoresPolicy" class="headerlink" title="FlushAllLargeStoresPolicy"></a><strong>FlushAllLargeStoresPolicy</strong></h2><p>在 HBase 2.0 之前版本是 FlushLargeStoresPolicy，后面被拆分成分 FlushAllLargeStoresPolicy 和FlushNonSloppyStoresFirstPolicy，参见 HBASE-14920。</p><p>这种策略会先判断 Region 中每个 MemStore 的使用内存(OnHeap+OffHeap)是否大于某个阀值，大于这个阀值的 MemStore 将会被刷写。阀值的计算是由 <code>hbase.hregion.percolumnfamilyflush.size.lower.bound</code> 、<code>hbase.hregion.percolumnfamilyflush.size.lower.bound.min</code> 以及 <code>hbase.hregion.memstore.flush.size</code> 参数决定的。计算逻辑如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//region.getMemStoreFlushSize() / familyNumber</span></span><br><span class="line"><span class="comment">//就是 hbase.hregion.memstore.flush.size 参数的值除以相关表列族的个数</span></span><br><span class="line">flushSizeLowerBound = max(region.getMemStoreFlushSize() / familyNumber, hbase.hregion.percolumnfamilyflush.size.lower.bound.min)</span><br><span class="line"></span><br><span class="line"><span class="comment">//如果设置了 hbase.hregion.percolumnfamilyflush.size.lower.bound</span></span><br><span class="line">flushSizeLowerBound = hbase.hregion.percolumnfamilyflush.size.lower.bound</span><br></pre></td></tr></table></figure><p><code>hbase.hregion.percolumnfamilyflush.size.lower.bound.min</code> 默认值为 16MB，而 <code>hbase.hregion.percolumnfamilyflush.size.lower.bound</code> 没有设置。</p><p>比如当前表有3个列族，其他用默认的值，那么 flushSizeLowerBound = max((long)128 / 3, 16) = 42。</p><p><strong>如果当前 Region 中没有 MemStore 的使用内存大于上面的阀值，FlushAllLargeStoresPolicy 策略就退化成 FlushAllStoresPolicy 策略了，也就是会对 Region 里面所有的 MemStore 进行 Flush。</strong></p><h2 id="FlushNonSloppyStoresFirstPolicy"><a href="#FlushNonSloppyStoresFirstPolicy" class="headerlink" title="FlushNonSloppyStoresFirstPolicy"></a><strong>FlushNonSloppyStoresFirstPolicy</strong></h2><p>HBase 2.0 引入了 in-memory compaction，参见 HBASE-13408。如果我们对相关列族 <code>hbase.hregion.compacting.memstore.type</code> 参数的值不是 NONE，那么这个 MemStore 的 isSloppyMemStore 值就是 true，否则就是 false。</p><p>FlushNonSloppyStoresFirstPolicy 策略将 Region 中的 MemStore 按照 isSloppyMemStore 分到两个 HashSet 里面（sloppyStores 和 regularStores）。然后</p><ul><li>判断 regularStores 里面是否有 MemStore 内存占用大于相关阀值的 MemStore ，有的话就会对这些 MemStore 进行刷写，其他的不做处理，这个阀值计算和 FlushAllLargeStoresPolicy 的阀值计算逻辑一致。</li><li>如果 regularStores 里面没有 MemStore 内存占用大于相关阀值的 MemStore，这时候就开始在 sloppyStores 里面寻找是否有 MemStore 内存占用大于相关阀值的 MemStore，有的话就会对这些 MemStore 进行刷写，其他的不做处理。</li><li>如果上面 sloppyStores 和 regularStores 都没有满足条件的 MemStore 需要刷写，这时候就 FlushNonSloppyStoresFirstPolicy 策略久退化成 FlushAllStoresPolicy 策略了。</li></ul><h1 id="MemStore-Flush-刷写过程"><a href="#MemStore-Flush-刷写过程" class="headerlink" title="MemStore Flush 刷写过程"></a><strong>MemStore Flush</strong> <strong>刷写过程</strong></h1><p>MemStore 的刷写过程很复杂，很多操作都可能触发，但是这些条件触发的刷写最终都是调用 HRegion 类中的 internalFlushcache 方法。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">  * Flush the memstore. Flushing the memstore is a little tricky. We have a lot of updates in the</span><br><span class="line">  * memstore, all of which have also been written to the wal. We need to write those updates in the</span><br><span class="line">  * memstore out to disk, while being able to process reads/writes as much as possible during the</span><br><span class="line">  * flush operation.</span><br><span class="line">  * &lt;p&gt;</span><br><span class="line">  * This method may block for some time. Every time you call it, we up the regions sequence id even</span><br><span class="line">  * if we don&apos;t flush; i.e. the returned region id will be at least one larger than the last edit</span><br><span class="line">  * applied to this region. The returned id does not refer to an actual edit. The returned id can</span><br><span class="line">  * be used for say installing a bulk loaded file just ahead of the last hfile that was the result</span><br><span class="line">  * of this flush, etc.</span><br><span class="line">  * @param wal Null if we&apos;re NOT to go via wal.</span><br><span class="line">  * @param myseqid The seqid to use if &lt;code&gt;wal&lt;/code&gt; is null writing out flush file.</span><br><span class="line">  * @param storesToFlush The list of stores to flush.</span><br><span class="line">  * @return object describing the flush&apos;s state</span><br><span class="line">  * @throws IOException general io exceptions</span><br><span class="line">  * @throws DroppedSnapshotException Thrown when replay of WAL is required.</span><br><span class="line">  */</span><br><span class="line">protected FlushResultImpl internalFlushcache(WAL wal, long myseqid,</span><br><span class="line">     Collection&lt;HStore&gt; storesToFlush, MonitoredTask status, boolean writeFlushWalMarker,</span><br><span class="line">     FlushLifeCycleTracker tracker) throws IOException &#123;</span><br><span class="line">   PrepareFlushResult result =</span><br><span class="line">       internalPrepareFlushCache(wal, myseqid, storesToFlush, status, writeFlushWalMarker, tracker);</span><br><span class="line">   if (result.result == null) &#123;</span><br><span class="line">     return internalFlushCacheAndCommit(wal, status, result, storesToFlush);</span><br><span class="line">   &#125; else &#123;</span><br><span class="line">     return result.result; // early exit due to failure from prepare stage</span><br><span class="line">   &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从上面的实现可以看出，Flush 操作主要分以下几步做的</p><ul><li><strong>prepareFlush</strong> 阶段：刷写的第一步是对 MemStore 做 snapshot，为了防止刷写过程中更新的数据同时在 snapshot 和 MemStore 中而造成后续处理的困难，所以在刷写期间需要持有 updateLock 。持有了 updateLock 之后，这将阻塞客户端的写操作。所以只在创建 snapshot 期间持有 updateLock，而且 snapshot 的创建非常快，所以此锁期间对客户的影响一般非常小。对 MemStore 做 snapshot 是 internalPrepareFlushCache 里面进行的。</li><li><strong>flushCache</strong> 阶段：如果创建快照没问题，那么返回的 result.result 将为 null。这时候我们就可以进行下一步 internalFlushCacheAndCommit。其实 internalFlushCacheAndCommit 里面包含两个步骤：flushCache 和 commit 阶段。flushCache 阶段其实就是将 prepareFlush 阶段创建好的快照写到临时文件里面，临时文件是存放在对应 Region 文件夹下面的 .tmp 目录里面。</li><li><strong>commit</strong> 阶段：将 flushCache 阶段生产的临时文件移到(rename)对应的列族目录下面，并做一些清理工作，比如删除第一步生成的 snapshot。</li></ul><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>HBASE的使用并不是很复杂，甚至很多人认为有点简单，但是涉及到大数据量时，往往会出现很多“意想不到”的问题，这种情况下深入到数据库的实现细节，根据其原理去定向的优化数据库就很是重要的，说到底还是要在源码级别对一个东西有系统性的认识。</p><div class="note primary">            <h4 id="本文作者：tongtong"><a href="#本文作者：tongtong" class="headerlink" title="本文作者：tongtong"></a>本文作者：tongtong</h4><h4 id="本文链接：https-stongtong-github-io"><a href="#本文链接：https-stongtong-github-io" class="headerlink" title="本文链接：https://stongtong.github.io/"></a>本文链接：<a href="https://stongtong.github.io/">https://stongtong.github.io/</a></h4><h4 id="版权申明：网站内容为tongtong所有，转载请注明出处。"><a href="#版权申明：网站内容为tongtong所有，转载请注明出处。" class="headerlink" title="版权申明：网站内容为tongtong所有，转载请注明出处。"></a>版权申明：网站内容为tongtong所有，转载请注明出处。</h4>          </div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;：由于HBASE中使用LSM，所以在HBASE中插入数据时，是批量从内存中flush到磁盘（HDFS），本文介绍总结HBASE中触发flush动作的详细条件，从而对flush过程进行优化。（本文参考自&lt;a href=&quot;https://mp.weixin.qq.com/s/lEfTrbd7bB4Xi0s86drr3g&quot;&gt;HBASE社区文章&lt;/a&gt;）&lt;/p&gt;
    
    </summary>
    
      <category term="Hbase" scheme="https://tongfan.xyz/categories/Hbase/"/>
    
    
      <category term="Hbase" scheme="https://tongfan.xyz/tags/Hbase/"/>
    
      <category term="flush" scheme="https://tongfan.xyz/tags/flush/"/>
    
  </entry>
  
  <entry>
    <title>HBASE中region offline</title>
    <link href="https://tongfan.xyz/2018/09/05/hbase%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://tongfan.xyz/2018/09/05/hbase问题记录（一）/</id>
    <published>2018-09-05T03:32:44.000Z</published>
    <updated>2019-03-25T13:00:52.211Z</updated>
    
    <content type="html"><![CDATA[<p><strong>摘要</strong>： 对处理hbase集群中遇到的问题进行记录，需要注意的一点是在hbase遇到问题后，首先从hbase和Hadoop的日志中定位问题，这一点特别重要，可以少走很多弯路。</p><a id="more"></a><h1 id="hbase-region-offline问题"><a href="#hbase-region-offline问题" class="headerlink" title="hbase region offline问题"></a>hbase region offline问题</h1><p>今天在进行数据访问的时候，hbase集群报出region掉线的问题，从网上看是由于hbase进行split操作，老的region下线分裂导致的，而我们这边可能是部分磁盘掉线导致region掉线的问题，定位到问题，开始解决。</p><p>从网上看到很多人说，重启集群可以解决，但是我尝试过后不起作用，最后找到一个解决方式，具体操作如下：</p><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">前提：HDFS fsck确保hbase根目录下文件没有损坏丢失，如果有，则先进行corrupt block移除。</span><br><span class="line">切记：一定要在所有Region都上线之后再修复，否则修复之后可能出现重复Region。</span><br><span class="line"></span><br><span class="line">步骤1. hbase hbck 检查输出所以ERROR信息，每个ERROR都会说明错误信息。</span><br><span class="line">步骤2. hbase hbck -fixTableOrphans 先修复tableinfo缺失问题，根据内存cache或者hdfs table 目录结构，重新生成tableinfo文件。</span><br><span class="line">步骤3. hbase hbck -fixHdfsOrphans 修复regioninfo缺失问题，根据region目录下的hfile重新生成regioninfo文件。</span><br><span class="line">步骤4. hbase hbck -fixHdfsOverlaps 修复region重叠问题，merge重叠的region为一个region目录，并从新生成一个regioninfo。</span><br><span class="line">步骤5. hbase hbck -fixHdfsHoles 修复region缺失，利用缺失的rowkey范围边界，生成新的region目录以及regioninfo填补这个空洞。</span><br><span class="line">步骤6. hbase hbck -fixMeta 修复meta表信息，利用regioninfo信息，重新生成对应meta row填写到meta表中，并为其填写默认的分配regionserver。</span><br><span class="line">步骤7. hbase hbck -fixAssignments 把这些offline的region触发上线，当region开始重新open 上线的时候，会被重新分配到真实的RegionServer上 , 并更新meta表上对应的行信息。</span><br></pre></td></tr></table></figure><p>按步骤执行完毕后，解决<code>0 inconsistencies detected. Status: OK</code>数据不一致的问题。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p><code>hbase hbck</code>检查发现集群的不健康的状态，对问题进行具体定位，寻找适合的在线修复命令，修复之后，用<code>hbase hbck</code>进行检测，一般能够对问题进行修复， 如果数据底层被破坏，那么先要对底层的坏块进行处理，然后进行hbase的修复。在hbase使用过程中，很多时候会碰到一些奇怪的问题，尤其是在一个集群中安装多种组件的时候，不同组件的相互影响，搞的平台人员晕头转向的，很累，哈哈。</p>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;： 对处理hbase集群中遇到的问题进行记录，需要注意的一点是在hbase遇到问题后，首先从hbase和Hadoop的日志中定位问题，这一点特别重要，可以少走很多弯路。&lt;/p&gt;
    
    </summary>
    
      <category term="Hbase" scheme="https://tongfan.xyz/categories/Hbase/"/>
    
    
      <category term="Hbase" scheme="https://tongfan.xyz/tags/Hbase/"/>
    
      <category term="region" scheme="https://tongfan.xyz/tags/region/"/>
    
  </entry>
  
  <entry>
    <title>spark接入HBASE</title>
    <link href="https://tongfan.xyz/2018/08/29/hbase%E7%9A%84spark%E6%8E%A5%E5%8F%A3/"/>
    <id>https://tongfan.xyz/2018/08/29/hbase的spark接口/</id>
    <published>2018-08-29T08:52:43.000Z</published>
    <updated>2019-03-17T09:15:54.317Z</updated>
    
    <content type="html"><![CDATA[<p><strong>摘要：</strong>spark对大数据量的数据处理很擅长，而hbase现在的定位基本上是作为数据仓库级别的，因此spark获取hbase数据是比较常见的搭配，本文主要介绍hbase对spark数据加载的支持，本文从基本操作、streaming、bulkload、SQL四方面对spark使用hbase数据进行介绍。</p><a id="more"></a><h1 id="基本操作"><a href="#基本操作" class="headerlink" title="基本操作"></a>基本操作</h1><p>基本操作顾名思义就是spark接入hbase最为基础的操作，后面部分都是以这部分为 基础的。spark与hbase发生关系的最为根本的就是HBASEContext，它自身拥有hbase的配置并将hbase的配置分发到spark executor上，让每个executor都会有一个hbase的链接。</p><p>作为参考，Spark executor可以位于与region server相同的节点上，也可以位于不同的节点，他们之间没有依赖（co-location），考虑将每个Spark executor看作一个多线程客户应用程序。这允许运行在executor上的任何Spark任务访问共享的连接对象。代码示例如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="string">"local"</span>, <span class="string">"test"</span>)</span><br><span class="line"><span class="keyword">val</span> config = <span class="keyword">new</span> <span class="type">HBaseConfiguration</span>()</span><br><span class="line">...</span><br><span class="line"><span class="keyword">val</span> hbaseContext = <span class="keyword">new</span> <span class="type">HBaseContext</span>(sc, config)</span><br><span class="line">rdd.hbaseForeachPartition(hbaseContext, (it, conn) =&gt; &#123;</span><br><span class="line"> <span class="keyword">val</span> bufferedMutator = conn.getBufferedMutator(<span class="type">TableName</span>.valueOf(<span class="string">"t1"</span>))</span><br><span class="line"> it.foreach((putRecord) =&gt; &#123;</span><br><span class="line">. <span class="keyword">val</span> put = <span class="keyword">new</span> <span class="type">Put</span>(putRecord._1)</span><br><span class="line">. putRecord._2.foreach((putValue) =&gt; put.addColumn(putValue._1, putValue._2,</span><br><span class="line">putValue._3))</span><br><span class="line">. bufferedMutator.mutate(put)</span><br><span class="line"> &#125;)</span><br><span class="line"> bufferedMutator.flush()</span><br><span class="line"> bufferedMutator.close()</span><br><span class="line">&#125;)</span><br></pre></td></tr></table></figure><p>上面例子中，每个partition拥有一个hbase connection，当然spark中还有一些其他的基本操作，具体如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">bulkPut</span><br><span class="line">大规模并行数据写入</span><br><span class="line">bulkDelete</span><br><span class="line">大规模并行数据删除</span><br><span class="line">bulkGet</span><br><span class="line">大规模并行数据查找并创建新的rdd</span><br><span class="line">mapPartition</span><br><span class="line">使用连接对象执行Spark Map函数，以允许完全访问HBase</span><br><span class="line">hBaseRDD</span><br><span class="line">简化分布式scan并创建RDD</span><br></pre></td></tr></table></figure><h1 id="spark-streaming"><a href="#spark-streaming" class="headerlink" title="spark streaming"></a>spark streaming</h1><p>spark streaming和hbase的结合能够有以下好处：</p><ul><li>处理过程中抓取引用信息或者配置信息</li><li>可以存储计数或者聚合结果来支持streaming只执行一次</li></ul><p>streaming的操作和spark的普通操作区别不是很大，具体代码如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="string">"local"</span>, <span class="string">"test"</span>)</span><br><span class="line"><span class="keyword">val</span> config = <span class="keyword">new</span> <span class="type">HBaseConfiguration</span>()</span><br><span class="line"><span class="keyword">val</span> hbaseContext = <span class="keyword">new</span> <span class="type">HBaseContext</span>(sc, config)</span><br><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sc, <span class="type">Milliseconds</span>(<span class="number">200</span>))</span><br><span class="line"><span class="keyword">val</span> rdd1 = ...</span><br><span class="line"><span class="keyword">val</span> rdd2 = ...</span><br><span class="line"><span class="keyword">val</span> queue = mutable.<span class="type">Queue</span>[<span class="type">RDD</span>[(<span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[(<span class="type">Array</span>[<span class="type">Byte</span>],</span><br><span class="line">  <span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">Array</span>[<span class="type">Byte</span>])])]]()</span><br><span class="line">queue += rdd1</span><br><span class="line">queue += rdd2</span><br><span class="line"><span class="keyword">val</span> dStream = ssc.queueStream(queue)</span><br><span class="line">dStream.hbaseBulkPut(</span><br><span class="line">  hbaseContext,</span><br><span class="line">  <span class="type">TableName</span>.valueOf(tableName),</span><br><span class="line">  (putRecord) =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> put = <span class="keyword">new</span> <span class="type">Put</span>(putRecord._1)</span><br><span class="line">  putRecord._2.foreach((putValue) =&gt; put.addColumn(putValue._1, putValue._2,</span><br><span class="line">putValue._3))</span><br><span class="line">  put</span><br><span class="line">  &#125;)</span><br></pre></td></tr></table></figure><h1 id="bulk-load"><a href="#bulk-load" class="headerlink" title="bulk load"></a>bulk load</h1><p>spark中提供了两种方式，第一种就是基本的bulkload操作，适用于列比较多的情况（million columns），列不固定并且没有分区的情况；第二种适用于每行的列数少于10k的场景，第二种会有更高的吞吐量，在shuffle中有更少的load。</p><p>实现中基本与mr的bulk load相同，分区根据rowkey region split进行划分，然后发按序送到reduce端，所以hfile可以从reduce的短语中直接写入。</p><p>spark中bulkload的实现是根据foreachPartition实现的repartitionAndSortWitPartition,下面代码是基本bulkload的实现</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="string">"local"</span>, <span class="string">"test"</span>)</span><br><span class="line"><span class="keyword">val</span> config = <span class="keyword">new</span> <span class="type">HBaseConfiguration</span>()</span><br><span class="line"><span class="keyword">val</span> hbaseContext = <span class="keyword">new</span> <span class="type">HBaseContext</span>(sc, config)</span><br><span class="line"><span class="keyword">val</span> stagingFolder = ...</span><br><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>(</span><br><span class="line">  (<span class="type">Bytes</span>.toBytes(<span class="string">"1"</span>),</span><br><span class="line">  (<span class="type">Bytes</span>.toBytes(columnFamily1), <span class="type">Bytes</span>.toBytes(<span class="string">"a"</span>),</span><br><span class="line"><span class="type">Bytes</span>.toBytes(<span class="string">"foo1"</span>))),</span><br><span class="line">  (<span class="type">Bytes</span>.toBytes(<span class="string">"3"</span>),</span><br><span class="line">  (<span class="type">Bytes</span>.toBytes(columnFamily1), <span class="type">Bytes</span>.toBytes(<span class="string">"b"</span>),</span><br><span class="line"><span class="type">Bytes</span>.toBytes(<span class="string">"foo2.b"</span>))), ...</span><br><span class="line">rdd.hbaseBulkLoad(<span class="type">TableName</span>.valueOf(tableName),</span><br><span class="line">  t =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> rowKey = t._1</span><br><span class="line">  <span class="keyword">val</span> family:<span class="type">Array</span>[<span class="type">Byte</span>] = t._2(<span class="number">0</span>)._1</span><br><span class="line">  <span class="keyword">val</span> qualifier = t._2(<span class="number">0</span>)._2</span><br><span class="line">  <span class="keyword">val</span> value = t._2(<span class="number">0</span>)._3</span><br><span class="line">  <span class="keyword">val</span> keyFamilyQualifier= <span class="keyword">new</span> <span class="type">KeyFamilyQualifier</span>(rowKey, family, qualifier)</span><br><span class="line">  <span class="type">Seq</span>((keyFamilyQualifier, value)).iterator</span><br><span class="line">  &#125;,</span><br><span class="line">  stagingFolder.getPath)</span><br><span class="line"><span class="keyword">val</span> load = <span class="keyword">new</span> <span class="type">LoadIncrementalHFiles</span>(config)</span><br><span class="line">load.doBulkLoad(<span class="keyword">new</span> <span class="type">Path</span>(stagingFolder.getPath),</span><br><span class="line">  conn.getAdmin, table, conn.getRegionLocator(<span class="type">TableName</span>.valueOf(tableName)))</span><br></pre></td></tr></table></figure><p>hbaseBulkLoad函数的三个参数：第一个就是表名；第三个是hfile写的临时路径；第二个是一个函数，它将rdd中的路径转换为kv结构，里边会包含rowkey、columnfamily、columnqualifier，shuffle操作会按rowkey进行分区并按照三个value的顺序进行排序。</p><p>在spark bulkload命令中，使用hbase的LoadIncrementHFiles可以加载新的HFile进入hbase；可以指定其他的参数，例如hfile最大的size，不做compaction的HFile标记，在列簇上进行压缩、bloom、块大小、块编码等一些设置</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> config = <span class="keyword">new</span> <span class="type">HBaseConfiguration</span>()</span><br><span class="line"><span class="keyword">val</span> hbaseContext = <span class="keyword">new</span> <span class="type">HBaseContext</span>(sc, config)</span><br><span class="line"><span class="keyword">val</span> stagingFolder = ...</span><br><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>(</span><br><span class="line">  (<span class="type">Bytes</span>.toBytes(<span class="string">"1"</span>),</span><br><span class="line">  (<span class="type">Bytes</span>.toBytes(columnFamily1), <span class="type">Bytes</span>.toBytes(<span class="string">"a"</span>),</span><br><span class="line"><span class="type">Bytes</span>.toBytes(<span class="string">"foo1"</span>))),</span><br><span class="line">  (<span class="type">Bytes</span>.toBytes(<span class="string">"3"</span>),</span><br><span class="line">  (<span class="type">Bytes</span>.toBytes(columnFamily1), <span class="type">Bytes</span>.toBytes(<span class="string">"b"</span>),</span><br><span class="line"><span class="type">Bytes</span>.toBytes(<span class="string">"foo2.b"</span>))), ...</span><br><span class="line"><span class="keyword">val</span> familyHBaseWriterOptions = <span class="keyword">new</span> java.util.<span class="type">HashMap</span>[<span class="type">Array</span>[<span class="type">Byte</span>],</span><br><span class="line"><span class="type">FamilyHFileWriteOptions</span>]</span><br><span class="line"><span class="keyword">val</span> f1Options = <span class="keyword">new</span> <span class="type">FamilyHFileWriteOptions</span>(<span class="string">"GZ"</span>, <span class="string">"ROW"</span>, <span class="number">128</span>, <span class="string">"PREFIX"</span>)</span><br><span class="line">familyHBaseWriterOptions.put(<span class="type">Bytes</span>.toBytes(<span class="string">"columnFamily1"</span>), f1Options)</span><br><span class="line">rdd.hbaseBulkLoad(<span class="type">TableName</span>.valueOf(tableName),</span><br><span class="line">  t =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> rowKey = t._1</span><br><span class="line">  <span class="keyword">val</span> family:<span class="type">Array</span>[<span class="type">Byte</span>] = t._2(<span class="number">0</span>)._1</span><br><span class="line">  <span class="keyword">val</span> qualifier = t._2(<span class="number">0</span>)._2</span><br><span class="line">  <span class="keyword">val</span> value = t._2(<span class="number">0</span>)._3</span><br><span class="line">  <span class="keyword">val</span> keyFamilyQualifier= <span class="keyword">new</span> <span class="type">KeyFamilyQualifier</span>(rowKey, family, qualifier)</span><br><span class="line">  <span class="type">Seq</span>((keyFamilyQualifier, value)).iterator</span><br><span class="line">  &#125;,</span><br><span class="line">  stagingFolder.getPath,</span><br><span class="line">  familyHBaseWriterOptions,</span><br><span class="line">  compactionExclude = <span class="literal">false</span>,</span><br><span class="line">  <span class="type">HConstants</span>.<span class="type">DEFAULT_MAX_FILE_SIZE</span>)</span><br><span class="line"><span class="keyword">val</span> load = <span class="keyword">new</span> <span class="type">LoadIncrementalHFiles</span>(config)</span><br><span class="line">load.doBulkLoad(<span class="keyword">new</span> <span class="type">Path</span>(stagingFolder.getPath),</span><br><span class="line">  conn.getAdmin, table, conn.getRegionLocator(<span class="type">TableName</span>.valueOf(tableName)))</span><br></pre></td></tr></table></figure><p>下面如何调用列数比较少的spark的接口:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(<span class="string">"local"</span>, <span class="string">"test"</span>)</span><br><span class="line"><span class="keyword">val</span> config = <span class="keyword">new</span> <span class="type">HBaseConfiguration</span>()</span><br><span class="line"><span class="keyword">val</span> hbaseContext = <span class="keyword">new</span> <span class="type">HBaseContext</span>(sc, config)</span><br><span class="line"><span class="keyword">val</span> stagingFolder = ...</span><br><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(<span class="type">Array</span>(</span><br><span class="line">  (<span class="string">"1"</span>,</span><br><span class="line">  (<span class="type">Bytes</span>.toBytes(columnFamily1), <span class="type">Bytes</span>.toBytes(<span class="string">"a"</span>),</span><br><span class="line"><span class="type">Bytes</span>.toBytes(<span class="string">"foo1"</span>))),</span><br><span class="line">  (<span class="string">"3"</span>,</span><br><span class="line">  (<span class="type">Bytes</span>.toBytes(columnFamily1), <span class="type">Bytes</span>.toBytes(<span class="string">"b"</span>),</span><br><span class="line"><span class="type">Bytes</span>.toBytes(<span class="string">"foo2.b"</span>))), ...</span><br><span class="line">rdd.hbaseBulkLoadThinRows(hbaseContext,</span><br><span class="line">  <span class="type">TableName</span>.valueOf(tableName),</span><br><span class="line">  t =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> rowKey = t._1</span><br><span class="line">  <span class="keyword">val</span> familyQualifiersValues = <span class="keyword">new</span> <span class="type">FamiliesQualifiersValues</span></span><br><span class="line">  t._2.foreach(f =&gt; &#123;</span><br><span class="line">  <span class="keyword">val</span> family:<span class="type">Array</span>[<span class="type">Byte</span>] = f._1</span><br><span class="line">  <span class="keyword">val</span> qualifier = f._2</span><br><span class="line">  <span class="keyword">val</span> value:<span class="type">Array</span>[<span class="type">Byte</span>] = f._3</span><br><span class="line">  familyQualifiersValues +=(family, qualifier, value)</span><br><span class="line">  &#125;)</span><br><span class="line">  (<span class="keyword">new</span> <span class="type">ByteArrayWrapper</span>(<span class="type">Bytes</span>.toBytes(rowKey)), familyQualifiersValues)</span><br><span class="line">  &#125;,</span><br><span class="line">  stagingFolder.getPath,</span><br><span class="line">  <span class="keyword">new</span> java.util.<span class="type">HashMap</span>[<span class="type">Array</span>[<span class="type">Byte</span>], <span class="type">FamilyHFileWriteOptions</span>],</span><br><span class="line">  compactionExclude = <span class="literal">false</span>,</span><br><span class="line">  <span class="number">20</span>)</span><br><span class="line"><span class="keyword">val</span> load = <span class="keyword">new</span> <span class="type">LoadIncrementalHFiles</span>(config)</span><br><span class="line">load.doBulkLoad(<span class="keyword">new</span> <span class="type">Path</span>(stagingFolder.getPath),</span><br><span class="line">  conn.getAdmin, table, conn.getRegionLocator(<span class="type">TableName</span>.valueOf(tableName)))</span><br></pre></td></tr></table></figure><h1 id="spark-sql"><a href="#spark-sql" class="headerlink" title="spark sql"></a>spark sql</h1><p>spark sq是spark中操作数据较为简单的一种的方式，在sql中提供了一系列的封装好的高级api可供调用，而且sql支持加载过个数据源，在spark SQL操作hbase时，比较核心的技术为分区调优、列调优，predicate pushdown和数据本地性。</p><p>使用hbase-spark connector（有hbase提供的一个连接spark的插件）， 必须先要定义catalog作为hbase和spark中的数据表的schema 映射，准备数据并定位hbase数据表，然后加载hbase dataframe，在此之后，用户可以通过sql查询来聚合或者访问记录，其基本过程如下：</p><h2 id="定义catalog"><a href="#定义catalog" class="headerlink" title="定义catalog"></a>定义catalog</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">catalog</span> </span>= <span class="string">s""</span><span class="string">"&#123;</span></span><br><span class="line"><span class="string">       |"</span><span class="string">table":&#123;"</span><span class="string">namespace":"</span><span class="string">default", "</span><span class="string">name":"</span>table1<span class="string">"&#125;,</span></span><br><span class="line"><span class="string">       |"</span><span class="string">rowkey":"</span><span class="string">key",</span></span><br><span class="line"><span class="string">       |"</span><span class="string">columns":&#123;</span></span><br><span class="line"><span class="string">         |"</span>col0<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">rowkey", "</span><span class="string">col":"</span><span class="string">key", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col1<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col1<span class="string">", "</span><span class="string">type":"</span><span class="string">boolean"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col2<span class="string">":&#123;"</span><span class="string">cf":"</span>cf2<span class="string">", "</span><span class="string">col":"</span>col2<span class="string">", "</span><span class="string">type":"</span><span class="string">double"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col3<span class="string">":&#123;"</span><span class="string">cf":"</span>cf3<span class="string">", "</span><span class="string">col":"</span>col3<span class="string">", "</span><span class="string">type":"</span><span class="string">float"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col4<span class="string">":&#123;"</span><span class="string">cf":"</span>cf4<span class="string">", "</span><span class="string">col":"</span>col4<span class="string">", "</span><span class="string">type":"</span><span class="string">int"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col5<span class="string">":&#123;"</span><span class="string">cf":"</span>cf5<span class="string">", "</span><span class="string">col":"</span>col5<span class="string">", "</span><span class="string">type":"</span><span class="string">bigint"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col6<span class="string">":&#123;"</span><span class="string">cf":"</span>cf6<span class="string">", "</span><span class="string">col":"</span>col6<span class="string">", "</span><span class="string">type":"</span><span class="string">smallint"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col7<span class="string">":&#123;"</span><span class="string">cf":"</span>cf7<span class="string">", "</span><span class="string">col":"</span>col7<span class="string">", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">         |"</span>col8<span class="string">":&#123;"</span><span class="string">cf":"</span>cf8<span class="string">", "</span><span class="string">col":"</span>col8<span class="string">", "</span><span class="string">type":"</span><span class="string">tinyint"&#125;</span></span><br><span class="line"><span class="string">       |&#125;</span></span><br><span class="line"><span class="string">     |&#125;"</span><span class="string">""</span>.stripMargin</span><br></pre></td></tr></table></figure><p>catalog定义了hbase和spark SQL表中的映射关系，主要包含两个关键部分，一是rowkey定义，另一个是spark中的table列和hbase中的列和列簇的对应关系，上面的catalog，hbase表名是table1，8列，rowkey为key。特别需要说明的是col0是必须要进行说明的，并且其cf就是rowkey。</p><h2 id="保存df"><a href="#保存df" class="headerlink" title="保存df"></a>保存df</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">HBaseRecord</span>(<span class="params"></span></span></span><br><span class="line"><span class="class"><span class="params">  col0: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">  col1: <span class="type">Boolean</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">  col2: <span class="type">Double</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">  col3: <span class="type">Float</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">  col4: <span class="type">Int</span>,       </span></span></span><br><span class="line"><span class="class"><span class="params">  col5: <span class="type">Long</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">  col6: <span class="type">Short</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">  col7: <span class="type">String</span>,</span></span></span><br><span class="line"><span class="class"><span class="params">  col8: <span class="type">Byte</span></span>)</span></span><br><span class="line"><span class="class"><span class="title">object</span> <span class="title">HBaseRecord</span></span>&#123;                                           </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(i: <span class="type">Int</span>, t: <span class="type">String</span>): <span class="type">HBaseRecord</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> s = <span class="string">s""</span><span class="string">"row$&#123;"</span>%<span class="number">03</span><span class="string">d".format(i)&#125;"</span><span class="string">""</span>       </span><br><span class="line">  <span class="type">HBaseRecord</span>(s,</span><br><span class="line">  i % <span class="number">2</span> == <span class="number">0</span>,</span><br><span class="line">  i.toDouble,</span><br><span class="line">  i.toFloat,  </span><br><span class="line">  i,</span><br><span class="line">  i.toLong,</span><br><span class="line">  i.toShort,  </span><br><span class="line">  <span class="string">s"String<span class="subst">$i</span>: <span class="subst">$t</span>"</span>,      </span><br><span class="line">  i.toByte)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> data = (<span class="number">0</span> to <span class="number">255</span>).map &#123; i =&gt;  <span class="type">HBaseRecord</span>(i, <span class="string">"extra"</span>)&#125;</span><br><span class="line">sc.parallelize(data).toDF.write.options(</span><br><span class="line"> <span class="type">Map</span>(<span class="type">HBaseTableCatalog</span>.tableCatalog -&gt; catalog, <span class="type">HBaseTableCatalog</span>.newTable -&gt; <span class="string">"5"</span>))</span><br><span class="line"> .format(<span class="string">"org.apache.hadoop.hbase.spark "</span>)</span><br><span class="line"> .save()</span><br></pre></td></tr></table></figure><p>save函数将会创建一个5个region的hbase table，并且把dataframe保存在表中。</p><h2 id="加载df"><a href="#加载df" class="headerlink" title="加载df"></a>加载df</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">withCatalog</span></span>(cat: <span class="type">String</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  sqlContext</span><br><span class="line">  .read</span><br><span class="line">  .options(<span class="type">Map</span>(<span class="type">HBaseTableCatalog</span>.tableCatalog-&gt;cat))</span><br><span class="line">  .format(<span class="string">"org.apache.hadoop.hbase.spark"</span>)</span><br><span class="line">  .load()</span><br><span class="line">&#125;</span><br><span class="line"><span class="keyword">val</span> df = withCatalog(catalog)</span><br></pre></td></tr></table></figure><h2 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> s = df.filter(($<span class="string">"col0"</span> &lt;= <span class="string">"row050"</span> &amp;&amp; $<span class="string">"col0"</span> &gt; <span class="string">"row040"</span>) ||</span><br><span class="line">  $<span class="string">"col0"</span> === <span class="string">"row005"</span> ||</span><br><span class="line">  $<span class="string">"col0"</span> &lt;= <span class="string">"row005"</span>)</span><br><span class="line">  .select(<span class="string">"col0"</span>, <span class="string">"col1"</span>, <span class="string">"col4"</span>)</span><br><span class="line">s.show</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">df.registerTempTable(<span class="string">"table1"</span>)</span><br><span class="line">sqlContext.sql(<span class="string">"select count(col1) from table1"</span>).show</span><br></pre></td></tr></table></figure><h2 id="其他参数"><a href="#其他参数" class="headerlink" title="其他参数"></a>其他参数</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = sqlContext.read</span><br><span class="line">  .options(<span class="type">Map</span>(<span class="type">HBaseTableCatalog</span>.tableCatalog -&gt; writeCatalog,</span><br><span class="line"><span class="type">HBaseSparkConf</span>.<span class="type">TIMESTAMP</span> -&gt; tsSpecified.toString))</span><br><span class="line">  .format(<span class="string">"org.apache.hadoop.hbase.spark"</span>)</span><br><span class="line">  .load()</span><br></pre></td></tr></table></figure><p>可以通过HbaseSparkConf来指定hbase中的记录的版本，来访问不同版本的记录具体示例如上下</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> df = sqlContext.read</span><br><span class="line">  .options(<span class="type">Map</span>(<span class="type">HBaseTableCatalog</span>.tableCatalog -&gt; writeCatalog,</span><br><span class="line"><span class="type">HBaseSparkConf</span>.<span class="type">MIN_TIMESTAMP</span> -&gt; <span class="string">"0"</span>,</span><br><span class="line">  <span class="type">HBaseSparkConf</span>.<span class="type">MAX_TIMESTAMP</span> -&gt; oldMs.toString))</span><br><span class="line">  .format(<span class="string">"org.apache.hadoop.hbase.spark"</span>)</span><br><span class="line">  .load()</span><br></pre></td></tr></table></figure><p>hbase-spark connector支持不同数据格式，例如Avro，JSON等等，使用中可以直接将avro记录持久化到hbase中，实际上，avro schema转换为本地spark catalyst 数据类型，hbase中的kv部分也可以被定义成avro格式，下面代码将展示，spark保存到加载avro格式数据的全过程。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">catalog</span> </span>= <span class="string">s""</span><span class="string">"&#123;</span></span><br><span class="line"><span class="string">  |"</span><span class="string">table":&#123;"</span><span class="string">namespace":"</span><span class="string">default", "</span><span class="string">name":"</span><span class="type">Avrotable</span><span class="string">"&#125;,</span></span><br><span class="line"><span class="string">  |"</span><span class="string">rowkey":"</span><span class="string">key",</span></span><br><span class="line"><span class="string">  |"</span><span class="string">columns":&#123;</span></span><br><span class="line"><span class="string">  |"</span>col0<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">rowkey", "</span><span class="string">col":"</span><span class="string">key", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">  |"</span>col1<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col1<span class="string">", "</span><span class="string">type":"</span><span class="string">binary"&#125;</span></span><br><span class="line"><span class="string">  |&#125;</span></span><br><span class="line"><span class="string">  |&#125;"</span><span class="string">""</span>.stripMargin</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"> <span class="class"><span class="keyword">object</span> <span class="title">AvroHBaseRecord</span> </span>&#123;</span><br><span class="line">  <span class="keyword">val</span> schemaString =</span><br><span class="line">  <span class="string">s""</span><span class="string">"&#123;"</span><span class="string">namespace": "</span>example.<span class="string">avro",</span></span><br><span class="line"><span class="string">  | "</span><span class="string">type": "</span><span class="string">record", "</span><span class="string">name": "</span><span class="type">User</span><span class="string">",</span></span><br><span class="line"><span class="string">  | "</span><span class="string">fields": [</span></span><br><span class="line"><span class="string">  | &#123;"</span><span class="string">name": "</span><span class="string">name", "</span><span class="string">type": "</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">  | &#123;"</span><span class="string">name": "</span>favorite_<span class="string">number", "</span><span class="string">type": ["</span><span class="string">int", "</span><span class="string">null"]&#125;,</span></span><br><span class="line"><span class="string">  | &#123;"</span><span class="string">name": "</span>favorite_<span class="string">color", "</span><span class="string">type": ["</span><span class="string">string", "</span><span class="string">null"]&#125;,</span></span><br><span class="line"><span class="string">  | &#123;"</span><span class="string">name": "</span>favorite_<span class="string">array", "</span><span class="string">type": &#123;"</span><span class="string">type": "</span><span class="string">array", "</span><span class="string">items":</span></span><br><span class="line"><span class="string">"</span><span class="string">string"&#125;&#125;,</span></span><br><span class="line"><span class="string">  | &#123;"</span><span class="string">name": "</span>favorite_<span class="string">map", "</span><span class="string">type": &#123;"</span><span class="string">type": "</span><span class="string">map", "</span><span class="string">values":</span></span><br><span class="line"><span class="string">"</span><span class="string">int"&#125;&#125;</span></span><br><span class="line"><span class="string">  | ] &#125;"</span><span class="string">""</span>.stripMargin</span><br><span class="line">  <span class="keyword">val</span> avroSchema: <span class="type">Schema</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> p = <span class="keyword">new</span> <span class="type">Schema</span>.<span class="type">Parser</span></span><br><span class="line">  p.parse(schemaString)</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">apply</span></span>(i: <span class="type">Int</span>): <span class="type">AvroHBaseRecord</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> user = <span class="keyword">new</span> <span class="type">GenericData</span>.<span class="type">Record</span>(avroSchema);</span><br><span class="line">  user.put(<span class="string">"name"</span>, <span class="string">s"name<span class="subst">$&#123;"%03d".format(i)&#125;</span>"</span>)</span><br><span class="line">  user.put(<span class="string">"favorite_number"</span>, i)</span><br><span class="line">  user.put(<span class="string">"favorite_color"</span>, <span class="string">s"color<span class="subst">$&#123;"%03d".format(i)&#125;</span>"</span>)</span><br><span class="line">  <span class="keyword">val</span> favoriteArray = <span class="keyword">new</span> <span class="type">GenericData</span>.<span class="type">Array</span>[<span class="type">String</span>](<span class="number">2</span>,</span><br><span class="line">avroSchema.getField(<span class="string">"favorite_array"</span>).schema())</span><br><span class="line">  favoriteArray.add(<span class="string">s"number<span class="subst">$&#123;i&#125;</span>"</span>)</span><br><span class="line">  favoriteArray.add(<span class="string">s"number<span class="subst">$&#123;i+1&#125;</span>"</span>)</span><br><span class="line">  user.put(<span class="string">"favorite_array"</span>, favoriteArray)</span><br><span class="line">  <span class="keyword">import</span> collection.<span class="type">JavaConverters</span>._</span><br><span class="line">  <span class="keyword">val</span> favoriteMap = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">Int</span>]((<span class="string">"key1"</span> -&gt; i), (<span class="string">"key2"</span> -&gt; (i+<span class="number">1</span>))).asJava</span><br><span class="line">  user.put(<span class="string">"favorite_map"</span>, favoriteMap)</span><br><span class="line">  <span class="keyword">val</span> avroByte = <span class="type">AvroSedes</span>.serialize(user, avroSchema)</span><br><span class="line">  <span class="type">AvroHBaseRecord</span>(<span class="string">s"name<span class="subst">$&#123;"%03d".format(i)&#125;</span>"</span>, avroByte)</span><br><span class="line">  &#125;</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">val</span> data = (<span class="number">0</span> to <span class="number">255</span>).map &#123; i =&gt;</span><br><span class="line">  <span class="type">AvroHBaseRecord</span>(i)</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"> sc.parallelize(data).toDF.write.options(</span><br><span class="line">  <span class="type">Map</span>(<span class="type">HBaseTableCatalog</span>.tableCatalog -&gt; catalog, <span class="type">HBaseTableCatalog</span>.newTable -&gt;</span><br><span class="line"><span class="string">"5"</span>))</span><br><span class="line">  .format(<span class="string">"org.apache.spark.sql.execution.datasources.hbase"</span>)</span><br><span class="line">  .save()</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">avroCatalog</span> </span>= <span class="string">s""</span><span class="string">"&#123;</span></span><br><span class="line"><span class="string">  |"</span><span class="string">table":&#123;"</span><span class="string">namespace":"</span><span class="string">default", "</span><span class="string">name":"</span><span class="string">avrotable"&#125;,</span></span><br><span class="line"><span class="string">  |"</span><span class="string">rowkey":"</span><span class="string">key",</span></span><br><span class="line"><span class="string">  |"</span><span class="string">columns":&#123;</span></span><br><span class="line"><span class="string">  |"</span>col0<span class="string">":&#123;"</span><span class="string">cf":"</span><span class="string">rowkey", "</span><span class="string">col":"</span><span class="string">key", "</span><span class="string">type":"</span><span class="string">string"&#125;,</span></span><br><span class="line"><span class="string">  |"</span>col1<span class="string">":&#123;"</span><span class="string">cf":"</span>cf1<span class="string">", "</span><span class="string">col":"</span>col1<span class="string">", "</span><span class="string">avro":"</span>avroS<span class="string">chema"&#125;</span></span><br><span class="line"><span class="string">  |&#125;</span></span><br><span class="line"><span class="string">  |&#125;"</span><span class="string">""</span>.stripMargin</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">withCatalog</span></span>(cat: <span class="type">String</span>): <span class="type">DataFrame</span> = &#123;</span><br><span class="line">  sqlContext</span><br><span class="line">  .read</span><br><span class="line">  .options(<span class="type">Map</span>(<span class="string">"avroSchema"</span> -&gt; <span class="type">AvroHBaseRecord</span>.schemaString,</span><br><span class="line"><span class="type">HBaseTableCatalog</span>.tableCatalog -&gt; avroCatalog))</span><br><span class="line">  .format(<span class="string">"org.apache.spark.sql.execution.datasources.hbase"</span>)</span><br><span class="line">  .load()</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="keyword">val</span> df = withCatalog(catalog)</span><br></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>在spark操作hbase的过程中，正常的操作如上所示，但是在遇到大数据量的时候，尤其是在出现数据倾斜，机器处理能力有限等问题的时候，数据处理的调优，例如分区、列等，这些是比较重要的，没有任何一种技术是单独存在的，就像人一样。</p><div class="note primary">            <h4 id="本文作者：tongtong"><a href="#本文作者：tongtong" class="headerlink" title="本文作者：tongtong"></a>本文作者：tongtong</h4><h4 id="本文链接：https-stongtong-github-io"><a href="#本文链接：https-stongtong-github-io" class="headerlink" title="本文链接：https://stongtong.github.io/"></a>本文链接：<a href="https://stongtong.github.io/">https://stongtong.github.io/</a></h4><h4 id="版权申明：网站内容为tongtong所有，转载请注明出处。"><a href="#版权申明：网站内容为tongtong所有，转载请注明出处。" class="headerlink" title="版权申明：网站内容为tongtong所有，转载请注明出处。"></a>版权申明：网站内容为tongtong所有，转载请注明出处。</h4>          </div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt;spark对大数据量的数据处理很擅长，而hbase现在的定位基本上是作为数据仓库级别的，因此spark获取hbase数据是比较常见的搭配，本文主要介绍hbase对spark数据加载的支持，本文从基本操作、streaming、bulkload、SQL四方面对spark使用hbase数据进行介绍。&lt;/p&gt;
    
    </summary>
    
      <category term="hbase" scheme="https://tongfan.xyz/categories/hbase/"/>
    
    
      <category term="hbase" scheme="https://tongfan.xyz/tags/hbase/"/>
    
      <category term="spark" scheme="https://tongfan.xyz/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Hadoop集群中遇到的一些问题</title>
    <link href="https://tongfan.xyz/2018/07/15/hadoop%E5%AE%89%E8%A3%85%E9%97%AE%E9%A2%98%E8%AE%B0%E5%BD%95%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <id>https://tongfan.xyz/2018/07/15/hadoop安装问题记录（一）/</id>
    <published>2018-07-15T02:30:44.000Z</published>
    <updated>2019-03-19T14:17:00.644Z</updated>
    
    <content type="html"><![CDATA[<p><strong>摘要：</strong>本篇文章对最近在帮同事搭建hadoop集群以及在调试升级部门集群时遇到的问题进行记录。</p><a id="more"></a><p><strong>问题一</strong>：在机器之间配置了ssh免密登录之后，启动集群还是需要输入密码</p><p>这个问题很困惑，因为机器之间都是ssh进行通信。其他机器ssh配置互通互信都没有问题，但是在集群启动的时候还是需要进行密码的输入，有点想不通，在进行ssh 本地ip（localhost或者127.0.0.1）的时候是也需要输入密码，这样的话就有点明白了，原因就是本地ip和配置的本机的集群ip没有对应起来，导致ssh不识别的问题，解决方式就是本地ip，ssh 本地ip，可以很快解决这个问题。当然了也可以通过配置hosts文件进行解决。</p><p><strong>问题二</strong>：在三台机器启动之后（1 master + 2 slave），会出现两个slave交替运行，但是同时只有一个存活的节点</p><p>这个问题有点搞笑，程序起起来以后，slave1和slave2来回的跳跃，但是集群中存活的节点只有一个，如果停止其中的一个节点，那就变成了另外一个永久活着，直觉告诉我肯定哪里的配置让集群不能分辨出两个slave节点了，结果是同时把hdfs的数据路径放在安装目录下（一口老血没忍住），造成了不同节点上“生成了”相同的数据节点标识，哈哈，这个问题很是致命，有时候很简单的一个就是让你束手无策，但是知道原理，一步一步排查，总能找到问题所在，问题解决了还是很高兴的。</p><p><strong>问题三</strong>：在安装centos 7时，直接用刻盘软件刻好优盘，但是提示超时，这个也记录一下。</p><p>这周部门集群系统升级，直接用软碟刻盘软件刻了一个优盘，在进行cent 7 安装的时候老是提示<code>centos dracut timeout</code>超时，然后就是直接进入命令行，不能进入到优盘的安装系统界面，从网上找到的解决方法，还是挺好用的，现记录一下，在进行安装系统选择的时候，直接选择<code>install centos 7</code>，然后根据提示 按e，进入系统安装编辑界面，将第一个的<code>inst.stage2=hd:LABEL=CentOS\x207\x20x86_64.check quiet</code>更改为<code>inst.stage2=hd:/dev/sdbX quiet</code>，然后Ctrl+x执行，能够解决优盘刻录安装不进入系统安装界面的问题。</p><p><strong>问题四</strong>：对hdfs一系列问题的尝试，这个真是让人很是头疼的问题，现记录如下。</p><p>经过反复的折腾，起始最终问题也很简单了，就是有人对集群的master的fsimage和editlog进行了删除，准确的说是格式化，我的天呢，来回折腾了几个回合才意识到做了什么，然后赶紧补救，结果就不多说了，这里记录一下这次事故中学到的东西。</p><ol><li>hdfs知识：hdfs在master节点format的时候，会生成原始的fsimage文件和editlog文件，并且在配置文件指定的master目录中生成本集群的cluster id和BP号，cluster id就是集群的唯一的标识，BP就是block pool，简单的理解就是DataNode上面存储数据的路径，master在启动时会将这两个东西分发到配置的各个slave节点上，如果是第一次启动，那么datanode节点会新建相关的路径以及用两个id初始化相关目录并在目录生成current，这都很自然……那么问题来了，在master重新format之后生成新的id，如果原来DataNode上面在原来路径上已经生成了相关文件，那么数据节点就挂不上了，现在有两种方式：（1）如果是新装的集群，把原来目录删了重建这时候还可以；（2）如果集群原来的DataNode上有数据，这个时候就需要对信息进行修改了，最方便的做法是在原来的DataNode节点上找到原来集群的两个id，直接修改现在master节点新生成的id至原来的id，这个时候你启动集群，会发现节点都挂上去了，数据也显示正常了。</li><li>解决问题的方法：碰到比较棘手的问题时，除了检测一下常见的问题，比如ip配置、硬盘的挂载、网络、基础库的配置外，最为主要的还是针对出现的问题，从组件的原理层面去深入的分析，一步一步的去定位，所以了解一个组件的运行原理很是重要，深入了解及其重要，不仅是在故障恢复，在平时的使用过程中作用会更大，最后说一句，<strong>面对任何一个问题，不论难易，都要心怀敬畏，认真对待</strong>。</li></ol>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt;本篇文章对最近在帮同事搭建hadoop集群以及在调试升级部门集群时遇到的问题进行记录。&lt;/p&gt;
    
    </summary>
    
      <category term="Hadoop" scheme="https://tongfan.xyz/categories/Hadoop/"/>
    
    
      <category term="hadoop" scheme="https://tongfan.xyz/tags/hadoop/"/>
    
  </entry>
  
  <entry>
    <title>HBASE中的meta表</title>
    <link href="https://tongfan.xyz/2018/03/10/hbase%E4%B8%AD%E7%9A%84meta%E8%A1%A8/"/>
    <id>https://tongfan.xyz/2018/03/10/hbase中的meta表/</id>
    <published>2018-03-10T07:05:58.000Z</published>
    <updated>2019-03-25T13:01:06.244Z</updated>
    
    <content type="html"><![CDATA[<p><strong>摘要</strong>：HBASE中存在一张隐含的meta表，隐含是因为其重要，可以说这张表示HBASE的管家表，不管是存数据还是查数据，这张表的作用都是十分重要的，本片文章详细对meta表进行说明。</p><a id="more"></a><h1 id="说明"><a href="#说明" class="headerlink" title="说明"></a>说明</h1><p>HBASE中的meta表，又称作catalog table，其作为HBASE中的table存储在hbase中，有人会问，如果这是一张存储在HBASE中的表，为什么执行list命令的时候，没有见过这张表呢？这里要进行说明一下，这张表是被HBASE shell 中的list命令排除了的，所以在list的时候，是没有显示的。</p><p>其在HBASE中存储的全名问<code>hbase:meta</code>，hbase是namespace，meta为表名，之前其表名为<code>.MATE.</code>，这张表存储的内容为HBASE中所有的region的列表，同时这张表的位置（location）是存储在ZooKeeper中的。</p><h1 id="结构"><a href="#结构" class="headerlink" title="结构"></a>结构</h1><p><code>hbase：meta</code>这张表的结构与一般hbase表并无区别，为rowkey和values（这是句废话），具体如下所示：</p><p>rowkey格式</p><ul><li><code>[table],[region start key],[region id]</code></li></ul><p>value格式为：</p><ul><li><code>info:regioninfo</code>:序列化这个region的regioninfo的实例；</li><li><code>info:server</code>：包含这个region的regionserver的端口；</li><li><code>info:serverstartcode</code>：包含这个region的regionserver线程运行的开始时间</li></ul><p>当表进行split操作时，另外的两个列将会被创建，<code>info:splitA</code>和<code>info:splitB</code>，这两列表示两个自region，这两列的权值就是序列化的regioninfo实例，在region split操作完成之后，最终这些行将会被删除。</p><h1 id="regioninfo"><a href="#regioninfo" class="headerlink" title="regioninfo"></a>regioninfo</h1><p>空key被用来表示table的起止，如果一个region中的startKey为空，那么这个region为table中的第一个region；如果一个region的startkey、endKey都为空，那么这个table中只有这一个region。</p><p>这里说明一下regioninfo这个实例，大家说这里边都存储着些什么东西，这里进行说明：</p><ul><li>tableName：表名</li><li>startKey：region的startKey</li><li>regionId：该region创建的时候的timestamp</li><li>replicaId：从0开始的，用以区分同个region分布在不同server上的replica；相同的region range可以返回多个位置；</li><li>encodedName：MD5对regionName的编码；</li></ul><p>还有一些其他的属性：</p><ul><li>endKey：region的endKey（独占的）</li><li>split：标识region是否发生了split操作；</li><li>offline：标识region是否offline；</li></ul><p>regionInfo在hbase 3.0中将有RegionInfoBuilder来进行创建，这个api会废弃；在hbase0.98和之前的版本中，表的region列表能够全部覆盖全部的keyspace，同时在任何时间点，一个rowkey总是属于一个region，定位在一个特定的服务器；在0.99之后的版本，一个region可以有多个示例（replicas），这样一个row range能够与多个regioninfo保持一致；多个regioninfo除了replicaId之外，其他字段都是共享的，replicaId默认为0，这个与之前的单regioninfo的版本是兼容的。</p><h1 id="开始序列化"><a href="#开始序列化" class="headerlink" title="开始序列化"></a>开始序列化</h1><p>首先在zk中查找<code>hbase:meta</code>，其次，对表中的server和startcode进行更新；</p><p>更多的信息参考<a href>hbase中的region和regionserver</a></p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>每个数据库都有系统维护的表，就和pg中的用户表、catalog表一样，hbase中也维护着自己的meta表，不过是hbase的功能配置、使用方式没有传统数据库的强大功能和管理（这和产品的定位的有关），所以其系统表稍微少一点，更多hbase中的细节，后续博客会介绍。</p><div class="note primary">            <h4 id="本文作者：tongtong"><a href="#本文作者：tongtong" class="headerlink" title="本文作者：tongtong"></a>本文作者：tongtong</h4><h4 id="本文链接：https-stongtong-github-io"><a href="#本文链接：https-stongtong-github-io" class="headerlink" title="本文链接：https://stongtong.github.io/"></a>本文链接：<a href="https://stongtong.github.io/">https://stongtong.github.io/</a></h4><h4 id="版权申明：网站内容为tongtong所有，转载请注明出处。"><a href="#版权申明：网站内容为tongtong所有，转载请注明出处。" class="headerlink" title="版权申明：网站内容为tongtong所有，转载请注明出处。"></a>版权申明：网站内容为tongtong所有，转载请注明出处。</h4>          </div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;：HBASE中存在一张隐含的meta表，隐含是因为其重要，可以说这张表示HBASE的管家表，不管是存数据还是查数据，这张表的作用都是十分重要的，本片文章详细对meta表进行说明。&lt;/p&gt;
    
    </summary>
    
      <category term="Hbase" scheme="https://tongfan.xyz/categories/Hbase/"/>
    
    
      <category term="Hbase" scheme="https://tongfan.xyz/tags/Hbase/"/>
    
      <category term="meta" scheme="https://tongfan.xyz/tags/meta/"/>
    
  </entry>
  
  <entry>
    <title>spark中的Broadcast原理</title>
    <link href="https://tongfan.xyz/2018/01/14/spark%E4%B8%ADbroadcast%E5%8E%9F%E7%90%86/"/>
    <id>https://tongfan.xyz/2018/01/14/spark中broadcast原理/</id>
    <published>2018-01-14T11:12:24.000Z</published>
    <updated>2019-02-26T14:47:16.653Z</updated>
    
    <content type="html"><![CDATA[<p><strong>摘要：</strong>spark中的broadcast提供了一种广播的机制，提供了共享变量、配置文件等共享变量，本文通过源码分析broadcast机制，spark版本为2.3.1。</p><a id="more"></a><h1 id="Broadcast"><a href="#Broadcast" class="headerlink" title="Broadcast"></a>Broadcast</h1><p>简而言之，bc就是把原来只保存一份在driver上的数据，由于每个task都能用到该数据，所以bc是策略就是在每个运行的machine上保存一份该类数据，而不必每个task都去driver去拉去，提高程序的处理效率。在为每一个节点保存一份较大的输入数据集而不是每个task保存一份，这是一种很高效的手段，而且spark还尝试用高效的高效broadcast算法去减少传输过程中网络通信开销。具体使用如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"><span class="comment">// 申明</span></span><br><span class="line"><span class="keyword">val</span> bcValue: <span class="type">Broadcast</span>[<span class="type">HashMap</span>[<span class="type">Int</span>, <span class="type">Int</span>]] = sc.broadcast(<span class="type">HashMap</span>(<span class="number">1</span> -&gt; <span class="number">2</span>, <span class="number">2</span> -&gt; <span class="number">3</span>, <span class="number">3</span> -&gt; <span class="number">4</span>, <span class="number">4</span> -&gt; <span class="number">5</span>))</span><br><span class="line"><span class="comment">// 使用</span></span><br><span class="line">bcValue.value.contains(key)</span><br><span class="line"><span class="comment">// 异步删除executor上的缓存备份，如果要再次使用则会重新发送，默认不会blocking，除非显示指定</span></span><br><span class="line">bcValue.unpersist</span><br><span class="line"><span class="comment">// 销毁和此bc相关的所有数据和元数据，使用要谨慎，因为一旦销毁bc变量，将不能再使用，此方法会blocking，直到此操作完成</span></span><br><span class="line">bcValue.destroy</span><br></pre></td></tr></table></figure><h1 id="实现方式"><a href="#实现方式" class="headerlink" title="实现方式"></a>实现方式</h1><p>下面说明spark中bc的具体实现，不同类的代码逻辑上连续的会贴在一起，这里注明类名；</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// SparkContext </span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">broadcast</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](value: <span class="type">T</span>): <span class="type">Broadcast</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">   assertNotStopped()</span><br><span class="line">   require(!classOf[<span class="type">RDD</span>[_]].isAssignableFrom(classTag[<span class="type">T</span>].runtimeClass),</span><br><span class="line">     <span class="string">"Can not directly broadcast RDDs; instead, call collect() and broadcast the result."</span>)</span><br><span class="line">   <span class="comment">//bc manager进行初始化， bc的变量是只读变量，不能修改</span></span><br><span class="line">   <span class="keyword">val</span> bc = env.broadcastManager.newBroadcast[<span class="type">T</span>](value, isLocal)</span><br><span class="line">   <span class="keyword">val</span> callSite = getCallSite</span><br><span class="line">   logInfo(<span class="string">"Created broadcast "</span> + bc.id + <span class="string">" from "</span> + callSite.shortForm)</span><br><span class="line">   cleaner.foreach(_.registerBroadcastForCleanup(bc))</span><br><span class="line">   bc</span><br><span class="line"> &#125;</span><br><span class="line"> <span class="comment">// BroadcastManager</span></span><br><span class="line"> <span class="keyword">private</span> <span class="keyword">val</span> nextBroadcastId = <span class="keyword">new</span> <span class="type">AtomicLong</span>(<span class="number">0</span>)</span><br><span class="line"> <span class="function"><span class="keyword">def</span> <span class="title">newBroadcast</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](value_ : <span class="type">T</span>, isLocal: <span class="type">Boolean</span>): <span class="type">Broadcast</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">   <span class="comment">// 生成bc id，factory 根据 不是不本地、value和id生成新的bc实例</span></span><br><span class="line">   broadcastFactory.newBroadcast[<span class="type">T</span>](value_, isLocal, nextBroadcastId.getAndIncrement())</span><br><span class="line"> &#125;</span><br></pre></td></tr></table></figure><h1 id="TorrentBroadcast"><a href="#TorrentBroadcast" class="headerlink" title="TorrentBroadcast"></a>TorrentBroadcast</h1><p>spark中实现bc的方式为TorrentBroadcast，类申明如下：</p><p>其实现原理为driver将序列化的对象划分成多个chuck，并将多个chuck保存在driver的BlockManager中，在每个executor上，executor第一次尝试从它本身的BlockManager中获取此对象时，如果不存在，它会从driver和其他executor上去拉取（如果其他executor上的object是可用的），一旦回去这些chuck，它会将其保存在它自己的BlockManager中，准备为其他executor（还未获取对象）的拉取最好准备。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">private</span>[spark] <span class="class"><span class="keyword">class</span> <span class="title">TorrentBroadcast</span>[<span class="type">T</span>: <span class="type">ClassTag</span>](<span class="params">obj: <span class="type">T</span>, id: <span class="type">Long</span></span>)</span></span><br><span class="line"><span class="class">  <span class="keyword">extends</span> <span class="title">Broadcast</span>[<span class="type">T</span>](<span class="params">id</span>) <span class="keyword">with</span> <span class="title">Logging</span> <span class="keyword">with</span> <span class="title">Serializable</span> </span>&#123;</span><br><span class="line">  <span class="comment">// bc 的读取操作是lazy的</span></span><br><span class="line">  <span class="meta">@transient</span> <span class="keyword">private</span> <span class="keyword">lazy</span> <span class="keyword">val</span> _value: <span class="type">T</span> = readBroadcastBlock()</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** The compression codec to use, or None if compression is disabled */</span></span><br><span class="line">  <span class="comment">// spark.broadcast.compress</span></span><br><span class="line">  <span class="meta">@transient</span> <span class="keyword">private</span> <span class="keyword">var</span> compressionCodec: <span class="type">Option</span>[<span class="type">CompressionCodec</span>] = _</span><br><span class="line">  <span class="comment">/** Size of each block. Default value is 4MB.  This value is only read by the broadcaster. */</span></span><br><span class="line">  <span class="comment">// spark.broadcast.blockSize</span></span><br><span class="line">  <span class="meta">@transient</span> <span class="keyword">private</span> <span class="keyword">var</span> blockSize: <span class="type">Int</span> = _</span><br><span class="line">  <span class="comment">// spark.broadcast.checksum</span></span><br><span class="line">  <span class="comment">// 在类初始化的过程中，会先从配置的sparkconf中查找上面三个配置项，如果存在进行加载，否则使用默认，然后会获取bc 生成的id，这个id就是上面实例化bc时生成的原子id；</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> broadcastId = <span class="type">BroadcastBlockId</span>(id)</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Total number of blocks this broadcast variable contains. */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">val</span> numBlocks: <span class="type">Int</span> = writeBlocks(obj)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">calcChecksum</span></span>(block: <span class="type">ByteBuffer</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> adler = <span class="keyword">new</span> <span class="type">Adler32</span>()</span><br><span class="line">    <span class="keyword">if</span> (block.hasArray) &#123;</span><br><span class="line">      adler.update(block.array, block.arrayOffset + block.position(), block.limit()</span><br><span class="line">        - block.position())</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">val</span> bytes = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Byte</span>](block.remaining())</span><br><span class="line">      block.duplicate.get(bytes)</span><br><span class="line">      adler.update(bytes)</span><br><span class="line">    &#125;</span><br><span class="line">    adler.getValue.toInt</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Divide the object into multiple blocks and put those blocks in the block manager.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   * @param value the object to divide</span></span><br><span class="line"><span class="comment">   * @return number of blocks this broadcast variable is divided into</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">writeBlocks</span></span>(value: <span class="type">T</span>): <span class="type">Int</span> = &#123;</span><br><span class="line">    <span class="keyword">import</span> <span class="type">StorageLevel</span>._</span><br><span class="line">    <span class="comment">// Store a copy of the broadcast variable in the driver so that tasks run on the driver</span></span><br><span class="line">    <span class="comment">// do not create a duplicate copy of the broadcast variable's value.</span></span><br><span class="line">    <span class="keyword">val</span> blockManager = <span class="type">SparkEnv</span>.get.blockManager</span><br><span class="line">    <span class="comment">// 注意：这里的默认的bc storage级别为M&amp;D</span></span><br><span class="line">    <span class="keyword">if</span> (!blockManager.putSingle(broadcastId, value, <span class="type">MEMORY_AND_DISK</span>, tellMaster = <span class="literal">false</span>)) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s"Failed to store <span class="subst">$broadcastId</span> in BlockManager"</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">val</span> blocks =</span><br><span class="line">      <span class="type">TorrentBroadcast</span>.blockifyObject(value, blockSize, <span class="type">SparkEnv</span>.get.serializer, compressionCodec)</span><br><span class="line">    <span class="keyword">if</span> (checksumEnabled) &#123;</span><br><span class="line">      checksums = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">Int</span>](blocks.length)</span><br><span class="line">    &#125;</span><br><span class="line">    blocks.zipWithIndex.foreach &#123; <span class="keyword">case</span> (block, i) =&gt;</span><br><span class="line">      <span class="keyword">if</span> (checksumEnabled) &#123;</span><br><span class="line">        checksums(i) = calcChecksum(block)</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">val</span> pieceId = <span class="type">BroadcastBlockId</span>(id, <span class="string">"piece"</span> + i)</span><br><span class="line">      <span class="keyword">val</span> bytes = <span class="keyword">new</span> <span class="type">ChunkedByteBuffer</span>(block.duplicate())</span><br><span class="line">      <span class="keyword">if</span> (!blockManager.putBytes(pieceId, bytes, <span class="type">MEMORY_AND_DISK_SER</span>, tellMaster = <span class="literal">true</span>)) &#123;</span><br><span class="line">        <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s"Failed to store <span class="subst">$pieceId</span> of <span class="subst">$broadcastId</span> in local BlockManager"</span>)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    blocks.length</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Fetch torrent blocks from the driver and/or other executors. */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">readBlocks</span></span>(): <span class="type">Array</span>[<span class="type">BlockData</span>] = &#123;</span><br><span class="line">    <span class="comment">// Fetch chunks of data. Note that all these chunks are stored in the BlockManager and reported</span></span><br><span class="line">    <span class="comment">// to the driver, so other executors can pull these chunks from this executor as well.</span></span><br><span class="line">    <span class="keyword">val</span> blocks = <span class="keyword">new</span> <span class="type">Array</span>[<span class="type">BlockData</span>](numBlocks)</span><br><span class="line">    <span class="keyword">val</span> bm = <span class="type">SparkEnv</span>.get.blockManager</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> (pid &lt;- <span class="type">Random</span>.shuffle(<span class="type">Seq</span>.range(<span class="number">0</span>, numBlocks))) &#123;</span><br><span class="line">      <span class="keyword">val</span> pieceId = <span class="type">BroadcastBlockId</span>(id, <span class="string">"piece"</span> + pid)</span><br><span class="line">      logDebug(<span class="string">s"Reading piece <span class="subst">$pieceId</span> of <span class="subst">$broadcastId</span>"</span>)</span><br><span class="line">      <span class="comment">// First try getLocalBytes because there is a chance that previous attempts to fetch the</span></span><br><span class="line">      <span class="comment">// broadcast blocks have already fetched some of the blocks. In that case, some blocks</span></span><br><span class="line">      <span class="comment">// would be available locally (on this executor).</span></span><br><span class="line">      bm.getLocalBytes(pieceId) <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(block) =&gt;</span><br><span class="line">          blocks(pid) = block</span><br><span class="line">          releaseLock(pieceId)</span><br><span class="line">        <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">          bm.getRemoteBytes(pieceId) <span class="keyword">match</span> &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="type">Some</span>(b) =&gt;</span><br><span class="line">              <span class="keyword">if</span> (checksumEnabled) &#123;</span><br><span class="line">                <span class="keyword">val</span> sum = calcChecksum(b.chunks(<span class="number">0</span>))</span><br><span class="line">                <span class="keyword">if</span> (sum != checksums(pid)) &#123;</span><br><span class="line">                  <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s"corrupt remote block <span class="subst">$pieceId</span> of <span class="subst">$broadcastId</span>:"</span> +</span><br><span class="line">                    <span class="string">s" <span class="subst">$sum</span> != <span class="subst">$&#123;checksums(pid)&#125;</span>"</span>)</span><br><span class="line">                &#125;</span><br><span class="line">              &#125;</span><br><span class="line">              <span class="comment">// We found the block from remote executors/driver's BlockManager, so put the block</span></span><br><span class="line">              <span class="comment">// in this executor's BlockManager.</span></span><br><span class="line">              <span class="keyword">if</span> (!bm.putBytes(pieceId, b, <span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK_SER</span>, tellMaster = <span class="literal">true</span>)) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(</span><br><span class="line">                  <span class="string">s"Failed to store <span class="subst">$pieceId</span> of <span class="subst">$broadcastId</span> in local BlockManager"</span>)</span><br><span class="line">              &#125;</span><br><span class="line">              blocks(pid) = <span class="keyword">new</span> <span class="type">ByteBufferBlockData</span>(b, <span class="literal">true</span>)</span><br><span class="line">            <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s"Failed to get <span class="subst">$pieceId</span> of <span class="subst">$broadcastId</span>"</span>)</span><br><span class="line">          &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    blocks</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Remove all persisted state associated with this Torrent broadcast on the executors.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">doUnpersist</span></span>(blocking: <span class="type">Boolean</span>) &#123;</span><br><span class="line">    <span class="type">TorrentBroadcast</span>.unpersist(id, removeFromDriver = <span class="literal">false</span>, blocking)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Remove all persisted state associated with this Torrent broadcast on the executors</span></span><br><span class="line"><span class="comment">   * and driver.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">override</span> <span class="keyword">protected</span> <span class="function"><span class="keyword">def</span> <span class="title">doDestroy</span></span>(blocking: <span class="type">Boolean</span>) &#123;</span><br><span class="line">    <span class="type">TorrentBroadcast</span>.unpersist(id, removeFromDriver = <span class="literal">true</span>, blocking)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/** Used by the JVM when serializing this object. */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">writeObject</span></span>(out: <span class="type">ObjectOutputStream</span>): <span class="type">Unit</span> = <span class="type">Utils</span>.tryOrIOException &#123;</span><br><span class="line">    assertValid()</span><br><span class="line">    out.defaultWriteObject()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">readBroadcastBlock</span></span>(): <span class="type">T</span> = <span class="type">Utils</span>.tryOrIOException &#123;</span><br><span class="line">    <span class="type">TorrentBroadcast</span>.synchronized &#123;</span><br><span class="line">      <span class="keyword">val</span> broadcastCache = <span class="type">SparkEnv</span>.get.broadcastManager.cachedValues</span><br><span class="line"></span><br><span class="line">      <span class="type">Option</span>(broadcastCache.get(broadcastId)).map(_.asInstanceOf[<span class="type">T</span>]).getOrElse &#123;</span><br><span class="line">        setConf(<span class="type">SparkEnv</span>.get.conf)</span><br><span class="line">        <span class="keyword">val</span> blockManager = <span class="type">SparkEnv</span>.get.blockManager</span><br><span class="line">        blockManager.getLocalValues(broadcastId) <span class="keyword">match</span> &#123;</span><br><span class="line">          <span class="keyword">case</span> <span class="type">Some</span>(blockResult) =&gt;</span><br><span class="line">            <span class="keyword">if</span> (blockResult.data.hasNext) &#123;</span><br><span class="line">              <span class="keyword">val</span> x = blockResult.data.next().asInstanceOf[<span class="type">T</span>]</span><br><span class="line">              releaseLock(broadcastId)</span><br><span class="line"></span><br><span class="line">              <span class="keyword">if</span> (x != <span class="literal">null</span>) &#123;</span><br><span class="line">                broadcastCache.put(broadcastId, x)</span><br><span class="line">              &#125;</span><br><span class="line"></span><br><span class="line">              x</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">              <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s"Failed to get locally stored broadcast data: <span class="subst">$broadcastId</span>"</span>)</span><br><span class="line">            &#125;</span><br><span class="line">          <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">            logInfo(<span class="string">"Started reading broadcast variable "</span> + id)</span><br><span class="line">            <span class="keyword">val</span> startTimeMs = <span class="type">System</span>.currentTimeMillis()</span><br><span class="line">            <span class="keyword">val</span> blocks = readBlocks()</span><br><span class="line">            logInfo(<span class="string">"Reading broadcast variable "</span> + id + <span class="string">" took"</span> + <span class="type">Utils</span>.getUsedTimeMs(startTimeMs))</span><br><span class="line"></span><br><span class="line">            <span class="keyword">try</span> &#123;</span><br><span class="line">              <span class="keyword">val</span> obj = <span class="type">TorrentBroadcast</span>.unBlockifyObject[<span class="type">T</span>](</span><br><span class="line">                blocks.map(_.toInputStream()), <span class="type">SparkEnv</span>.get.serializer, compressionCodec)</span><br><span class="line">              <span class="comment">// Store the merged copy in BlockManager so other tasks on this executor don't</span></span><br><span class="line">              <span class="comment">// need to re-fetch it.</span></span><br><span class="line">              <span class="keyword">val</span> storageLevel = <span class="type">StorageLevel</span>.<span class="type">MEMORY_AND_DISK</span></span><br><span class="line">              <span class="keyword">if</span> (!blockManager.putSingle(broadcastId, obj, storageLevel, tellMaster = <span class="literal">false</span>)) &#123;</span><br><span class="line">                <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s"Failed to store <span class="subst">$broadcastId</span> in BlockManager"</span>)</span><br><span class="line">              &#125;</span><br><span class="line"></span><br><span class="line">              <span class="keyword">if</span> (obj != <span class="literal">null</span>) &#123;</span><br><span class="line">                broadcastCache.put(broadcastId, obj)</span><br><span class="line">              &#125;</span><br><span class="line"></span><br><span class="line">              obj</span><br><span class="line">            &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">              blocks.foreach(_.dispose())</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * If running in a task, register the given block's locks for release upon task completion.</span></span><br><span class="line"><span class="comment">   * Otherwise, if not running in a task then immediately release the lock.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">releaseLock</span></span>(blockId: <span class="type">BlockId</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> blockManager = <span class="type">SparkEnv</span>.get.blockManager</span><br><span class="line">    <span class="type">Option</span>(<span class="type">TaskContext</span>.get()) <span class="keyword">match</span> &#123;</span><br><span class="line">      <span class="keyword">case</span> <span class="type">Some</span>(taskContext) =&gt;</span><br><span class="line">        taskContext.addTaskCompletionListener(_ =&gt; blockManager.releaseLock(blockId))</span><br><span class="line">      <span class="keyword">case</span> <span class="type">None</span> =&gt;</span><br><span class="line">        <span class="comment">// This should only happen on the driver, where broadcast variables may be accessed</span></span><br><span class="line">        <span class="comment">// outside of running tasks (e.g. when computing rdd.partitions()). In order to allow</span></span><br><span class="line">        <span class="comment">// broadcast variables to be garbage collected we need to free the reference here</span></span><br><span class="line">        <span class="comment">// which is slightly unsafe but is technically okay because broadcast variables aren't</span></span><br><span class="line">        <span class="comment">// stored off-heap.</span></span><br><span class="line">        blockManager.releaseLock(blockId)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="class"><span class="keyword">object</span> <span class="title">TorrentBroadcast</span> <span class="keyword">extends</span> <span class="title">Logging</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">blockifyObject</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      obj: <span class="type">T</span>,</span><br><span class="line">      blockSize: <span class="type">Int</span>,</span><br><span class="line">      serializer: <span class="type">Serializer</span>,</span><br><span class="line">      compressionCodec: <span class="type">Option</span>[<span class="type">CompressionCodec</span>]): <span class="type">Array</span>[<span class="type">ByteBuffer</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> cbbos = <span class="keyword">new</span> <span class="type">ChunkedByteBufferOutputStream</span>(blockSize, <span class="type">ByteBuffer</span>.allocate)</span><br><span class="line">    <span class="keyword">val</span> out = compressionCodec.map(c =&gt; c.compressedOutputStream(cbbos)).getOrElse(cbbos)</span><br><span class="line">    <span class="keyword">val</span> ser = serializer.newInstance()</span><br><span class="line">    <span class="keyword">val</span> serOut = ser.serializeStream(out)</span><br><span class="line">    <span class="type">Utils</span>.tryWithSafeFinally &#123;</span><br><span class="line">      serOut.writeObject[<span class="type">T</span>](obj)</span><br><span class="line">    &#125; &#123;</span><br><span class="line">      serOut.close()</span><br><span class="line">    &#125;</span><br><span class="line">    cbbos.toChunkedByteBuffer.getChunks()</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">unBlockifyObject</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      blocks: <span class="type">Array</span>[<span class="type">InputStream</span>],</span><br><span class="line">      serializer: <span class="type">Serializer</span>,</span><br><span class="line">      compressionCodec: <span class="type">Option</span>[<span class="type">CompressionCodec</span>]): <span class="type">T</span> = &#123;</span><br><span class="line">    require(blocks.nonEmpty, <span class="string">"Cannot unblockify an empty array of blocks"</span>)</span><br><span class="line">    <span class="keyword">val</span> is = <span class="keyword">new</span> <span class="type">SequenceInputStream</span>(blocks.iterator.asJavaEnumeration)</span><br><span class="line">    <span class="keyword">val</span> in: <span class="type">InputStream</span> = compressionCodec.map(c =&gt; c.compressedInputStream(is)).getOrElse(is)</span><br><span class="line">    <span class="keyword">val</span> ser = serializer.newInstance()</span><br><span class="line">    <span class="keyword">val</span> serIn = ser.deserializeStream(in)</span><br><span class="line">    <span class="keyword">val</span> obj = <span class="type">Utils</span>.tryWithSafeFinally &#123;</span><br><span class="line">      serIn.readObject[<span class="type">T</span>]()</span><br><span class="line">    &#125; &#123;</span><br><span class="line">      serIn.close()</span><br><span class="line">    &#125;</span><br><span class="line">    obj</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Remove all persisted blocks associated with this torrent broadcast on the executors.</span></span><br><span class="line"><span class="comment">   * If removeFromDriver is true, also remove these persisted blocks on the driver.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">unpersist</span></span>(id: <span class="type">Long</span>, removeFromDriver: <span class="type">Boolean</span>, blocking: <span class="type">Boolean</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    logDebug(<span class="string">s"Unpersisting TorrentBroadcast <span class="subst">$id</span>"</span>)</span><br><span class="line">    <span class="type">SparkEnv</span>.get.blockManager.master.removeBroadcast(id, removeFromDriver, blocking)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">  </span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h1 id="关于HttpBroadcast的说明"><a href="#关于HttpBroadcast的说明" class="headerlink" title="关于HttpBroadcast的说明"></a>关于HttpBroadcast的说明</h1><p>在网上看了很多人的blog，大家说还要一种HttpBroadcast，但是在最新的2.3.1中的源码中，并未发现这种机制的的广播方式，这种方式主要是依靠http协议来实现bc，通过运行在driver上的http server来获取序列化的对象，删除删除这种方式的主要原因我觉得就是因为服务器模式的单点压力太大，而且发送的数据量偏大时，可能会造成driver网络、资源很快被耗尽，影响程序整体的执行，甚至造成程序故障。</p><div class="note primary">            <h4 id="本文作者：tongtong"><a href="#本文作者：tongtong" class="headerlink" title="本文作者：tongtong"></a>本文作者：tongtong</h4><h4 id="本文链接：https-stongtong-github-io"><a href="#本文链接：https-stongtong-github-io" class="headerlink" title="本文链接：https://stongtong.github.io/"></a>本文链接：<a href="https://stongtong.github.io/">https://stongtong.github.io/</a></h4><h4 id="版权申明：网站内容为tongtong所有，转载请注明出处。"><a href="#版权申明：网站内容为tongtong所有，转载请注明出处。" class="headerlink" title="版权申明：网站内容为tongtong所有，转载请注明出处。"></a>版权申明：网站内容为tongtong所有，转载请注明出处。</h4>          </div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt;spark中的broadcast提供了一种广播的机制，提供了共享变量、配置文件等共享变量，本文通过源码分析broadcast机制，spark版本为2.3.1。&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="https://tongfan.xyz/categories/Spark/"/>
    
    
      <category term="Spark，Broadcast" scheme="https://tongfan.xyz/tags/Spark%EF%BC%8CBroadcast/"/>
    
  </entry>
  
  <entry>
    <title>markdown中的绘图语法</title>
    <link href="https://tongfan.xyz/2018/01/13/markdown%E4%B8%AD%E7%9A%84%E7%BB%98%E5%9B%BE%E8%AF%AD%E6%B3%95-md/"/>
    <id>https://tongfan.xyz/2018/01/13/markdown中的绘图语法-md/</id>
    <published>2018-01-13T11:12:24.000Z</published>
    <updated>2019-03-19T14:17:23.169Z</updated>
    
    <content type="html"><![CDATA[<p><strong>前言：</strong>一直在用七牛云作为博客的图床，但是之前发现在七牛云的图床网址必须要进行备案了，太委屈！我自己的个人网站，就是在写写自己的日记、技术笔记、工作感悟、巴拉巴拉巴拉，还要让我备案，真是没地说理去；辗转反侧，也不能因为这个事情不写、不记录啊，所以最近萌生了博客无图的想法，之前用markdown的时候正好接触过markdown的绘图功能，身材很好（轻量级）并且该有的都有，哈哈，今天用这篇博客记录一下markdown中的常用绘图语法，方便以后使用。</p><a id="more"></a><h1 id="1-语法介绍"><a href="#1-语法介绍" class="headerlink" title="1 语法介绍"></a>1 语法介绍</h1><p>markdown能够绘制的图形有很多种，例如最常见的框图、流程图、甘特图、时序图等等，这里先对其关键字进行介绍。</p><h2 id="1-1-基本关键字"><a href="#1-1-基本关键字" class="headerlink" title="1.1 基本关键字"></a>1.1 基本关键字</h2><ul><li><p>mermaid：主要用来绘制框图、甘特图、时序图等，该关键字是比较标准的用法，配合其他关键字能够完成特定图形的绘制；</p><ul><li><p>gantt：绘制甘特图；</p></li><li><p>sequenceDiagram：绘制时序图（UML）；</p></li><li><p>graph ：绘制框图时，图形的位置，后面可接收方向关键词，具体如下（比较简单，不再解释）：</p><ul><li><p>TB - top bottom </p></li><li><p>BT - bottom top </p></li><li>RL - right left </li><li>LR - left right </li><li>TD - as same TB</li></ul></li></ul></li><li><p>flow：用来绘制完整的流程图；</p></li><li><p>sequence：用来绘制时序图；</p></li></ul><h2 id="1-2-基本元素使用"><a href="#1-2-基本元素使用" class="headerlink" title="1.2 基本元素使用"></a>1.2 基本元素使用</h2><h3 id="框"><a href="#框" class="headerlink" title="框"></a>框</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">id1</span><br></pre></td></tr></table></figure><pre class="mermaid">graph LRid1</pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">id[I am wxt, I am very happy.]</span><br></pre></td></tr></table></figure><pre class="mermaid">graph LRid[I am wxt, I am very happy.文本框]</pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">id(I am wxt, I am very happy.圆角)</span><br></pre></td></tr></table></figure><pre class="mermaid">graph LRid(I am wxt, I am very happy.圆角)</pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">id((This is the text in the circle.圆节点));</span><br></pre></td></tr></table></figure><pre class="mermaid">graph LRid((This is the text in the circle));</pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">id&gt;This is the text in the box非对称节点]</span><br></pre></td></tr></table></figure><pre class="mermaid">graph LRid>This is the text in the box非对称节点]</pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">id&#123;This is the text in the box&#125;</span><br></pre></td></tr></table></figure><pre class="mermaid">graph LRid{This is the text in the box菱形节点}</pre><h3 id="连接线"><a href="#连接线" class="headerlink" title="连接线"></a>连接线</h3><p>接线中加入标签：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR;</span><br><span class="line">  A--&gt;B;</span><br><span class="line">%% 箭头链接</span><br></pre></td></tr></table></figure><pre class="mermaid">graph LR;  A-->B;</pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A --- B</span><br><span class="line">%% 无箭头</span><br></pre></td></tr></table></figure><pre class="mermaid">graph LRA --- B%% 无箭头</pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A -- This is the label text --- B;</span><br><span class="line">%% 标签</span><br></pre></td></tr></table></figure><pre class="mermaid">graph LRA -- This is the label text --- B;%% 标签</pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A-- text --&gt;B</span><br><span class="line">%% 箭头标签</span><br></pre></td></tr></table></figure><pre class="mermaid">graph LRA--text-->B</pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A-.-&gt;B</span><br><span class="line">%% 虚线</span><br></pre></td></tr></table></figure><pre class="mermaid">graph LRA-.->B%% 虚线</pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A-.text.-&gt;B</span><br><span class="line">%% 标签虚线</span><br></pre></td></tr></table></figure><pre class="mermaid">graph LRA-.text.->B%% 标签虚线</pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A==&gt;B</span><br><span class="line">%% 有向粗实线</span><br></pre></td></tr></table></figure><pre class="mermaid">graph LRA==>B%% 有向粗实线</pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A===B</span><br><span class="line">%% 无向粗实线</span><br></pre></td></tr></table></figure><pre class="mermaid">graph LRA===B%% 无向粗实线</pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A==text==&gt;B</span><br><span class="line">%% 标签粗线 有向</span><br></pre></td></tr></table></figure><pre class="mermaid">graph LRA==text==>B%% 标签粗线 有向</pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">A==text===B</span><br><span class="line">%% 标签粗线 无向</span><br></pre></td></tr></table></figure><pre class="mermaid">graph LRA==text===B%% 标签粗线 无向</pre><h3 id="特殊语法"><a href="#特殊语法" class="headerlink" title="特殊语法"></a>特殊语法</h3><p>1、引号可以抑制一些特殊字符的使用，避免一些不必要的麻烦。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">d1[&quot;This is the (text) in the box&quot;]</span><br></pre></td></tr></table></figure><p>2、html字符的转义字符</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">        A[&quot;A double quote:#quot;&quot;]--&gt;B[&quot;A dec char:#9829;&quot;]</span><br></pre></td></tr></table></figure><pre class="mermaid">graph LR        A["A double quote:#quot;"]-->B["A dec char:#9829;"]</pre><h3 id="子图"><a href="#子图" class="headerlink" title="子图"></a>子图</h3><p>基本语法如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">subgraph title </span><br><span class="line">graph definition </span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>具体示例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">        subgraph one</span><br><span class="line">        a1 --&gt; a2</span><br><span class="line">        end</span><br><span class="line">        subgraph two</span><br><span class="line">        b2 --&gt; b2</span><br><span class="line">        end</span><br><span class="line">        subgraph three</span><br><span class="line">        c1 --&gt; c2</span><br><span class="line">        end</span><br><span class="line">        c1 --&gt; a2</span><br></pre></td></tr></table></figure><pre class="mermaid">graph TB        subgraph one        a1 --> a2        end        subgraph two        b2 --> b2        end        subgraph three        c1 --> c2        end        c1 --> a2</pre><h3 id="基础fontawesome支持"><a href="#基础fontawesome支持" class="headerlink" title="基础fontawesome支持"></a>基础fontawesome支持</h3><p>在图片之中要加入来自frontawesome的图表字体，需要像frontawesome网站上那样引用的那样。详情：<a href="http://fontawesome.io/">fontawdsome</a></p><p>具体示例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">graph TD</span><br><span class="line">      B[&quot;fa:fa-twitter for peace&quot;]</span><br><span class="line">      B--&gt;C[fa:fa-ban forbidden]</span><br><span class="line">      B--&gt;D(fa:fa-spinner);</span><br><span class="line">      B--&gt;E(A fa:fa-camerra-retro perhaps?);</span><br></pre></td></tr></table></figure><pre class="mermaid">graph TD      B["fa:fa-twitter for peace"]      B-->C[fa:fa-ban forbidden]      B-->D(fa:fa-spinner);      B-->E(A fa:fa-camerra-retro perhaps?);</pre><h2 id="1-3-渲染"><a href="#1-3-渲染" class="headerlink" title="1.3 渲染"></a>1.3 渲染</h2><p>定义连接线的样式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">     id1(Start)--&gt;id2(Stop)</span><br><span class="line">     style id1 fill:#f9f,stroke:#333,stroke-width:4px;</span><br><span class="line">     style id2 fill:#ccf,stroke:#f66,stroke-width:2px,stroke-dasharray:5,5;</span><br></pre></td></tr></table></figure><pre class="mermaid">graph LR     id1(Start)-->id2(Stop)     style id1 fill:#f9f,stroke:#333,stroke-width:4px;     style id2 fill:#ccf,stroke:#f66,stroke-width:2px,stroke-dasharray:5,5;</pre><h3 id="样式"><a href="#样式" class="headerlink" title="样式"></a>样式</h3><p>为了方便样式的使用，可以定义类来使用样式，类的定义示例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">classDef className fill:#f9f,stroke:#333,stroke-width:4px;</span><br></pre></td></tr></table></figure><p>对节点使用样式类：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class nodeId className;</span><br></pre></td></tr></table></figure><p>同时对多个节点使用相同的样式类：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">class nodeId1,nodeId2 className;</span><br></pre></td></tr></table></figure><p>可以在CSS中提前定义样式类，应用在图表的定义中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">      A--&gt;B[AAABBB];</span><br><span class="line">      B--&gt;D;</span><br><span class="line">      class A cssClass;</span><br></pre></td></tr></table></figure><p>默认样式类：当没有指定样式的时候，默认采用。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">classDef default fill:#f9f,stroke:#333,stroke-width:4px;</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph LR</span><br><span class="line">    classDef default fill:#f90,stroke:#555,stroke-width:4px;</span><br><span class="line">    id1(Start)--&gt;id2(Stop)</span><br></pre></td></tr></table></figure><pre class="mermaid">graph LR    classDef default fill:#f90,stroke:#555,stroke-width:4px;    id1(Start)-->id2(Stop)</pre><h1 id="2-时序图-sequence-diagram"><a href="#2-时序图-sequence-diagram" class="headerlink" title="2 时序图(sequence diagram)"></a>2 时序图(sequence diagram)</h1><h2 id="2-1-基本语法"><a href="#2-1-基本语法" class="headerlink" title="2.1 基本语法"></a>2.1 基本语法</h2><p>时序图，又叫序列图，为uml中比较常用的图形，在md中如下制作：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">　　Alice-&gt;&gt;John: Hello John, how are you ?</span><br><span class="line">　　John--&gt;&gt;Alice: Great!</span><br><span class="line">　　Alice-&gt;&gt;John: Huang,you are better .</span><br><span class="line">　　John--&gt;&gt;Alice: yeah, Just not bad.</span><br></pre></td></tr></table></figure><pre class="mermaid">sequenceDiagram　　Alice->>John: Hello John, how are you ?　　John-->>Alice: Great!　　Alice->>John: Huang,you are better .　　John-->>Alice: yeah, Just not bad.</pre><p>观察上图，如果想让John出现在前面，让Alice在后面，原图的结构不发生改变，那要如何操作。mermaid通过设定参与者(participants)的顺序控制二者的顺序，示例如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">　　participant John</span><br><span class="line">　　participant Alice</span><br><span class="line">　　Alice-xJohn:Hello John,how are you?</span><br><span class="line">　　John--&gt;&gt;Alice:Great!</span><br></pre></td></tr></table></figure><pre class="mermaid">sequenceDiagram　　participant John　　participant Alice　　Alice-xJohn:Hello John,how are you?　　John-->>Alice:Great!</pre><h2 id="2-2-消息语法"><a href="#2-2-消息语法" class="headerlink" title="2.2 消息语法"></a>2.2 消息语法</h2><p>实线或者虚线的使用，Arrow的六种样式</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">-&gt;</span><br><span class="line">--&gt;</span><br><span class="line">-&gt;&gt;</span><br><span class="line">--&gt;&gt;</span><br><span class="line">-x</span><br><span class="line">--x</span><br></pre></td></tr></table></figure><p>具体示例如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">    Alice-&gt;John: Hello John, how are you ?</span><br><span class="line">    John--&gt;Alice:Great!</span><br><span class="line">    Alice-&gt;&gt;John: dont borther me !</span><br><span class="line">    John--&gt;&gt;Alice:Great!</span><br><span class="line">    Alice-xJohn: wait!</span><br><span class="line">    John--xAlice: Ok!</span><br></pre></td></tr></table></figure><pre class="mermaid">sequenceDiagram    Alice->John: Hello John, how are you ?    John-->Alice:Great!    Alice->>John: dont borther me !    John-->>Alice:Great!    Alice-xJohn: wait!    John--xAlice: Ok!</pre><h2 id="2-3-便签"><a href="#2-3-便签" class="headerlink" title="2.3 便签"></a>2.3 便签</h2><p>序列图增加便签，具体规则<code>[right of | left of | over][Actor]:Text</code> ，具体示例。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">　　participant John</span><br><span class="line">　　Note left of John: Text in note</span><br></pre></td></tr></table></figure><pre class="mermaid">sequenceDiagram　　participant John　　Note left of John: Text in note</pre><p>跨越两个Actor的便签：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">Alice-&gt;&gt;John:Hello John, how are you?</span><br><span class="line">Note over Alice,John:A typical interaction</span><br></pre></td></tr></table></figure><pre class="mermaid">sequenceDiagram    Alice->>John:Hello John, how are you?    Note over Alice,John:A typical interaction</pre><h2 id="2-4-循环Loops"><a href="#2-4-循环Loops" class="headerlink" title="2.4 循环Loops"></a>2.4 循环Loops</h2><p>在序列图中，也可以使用循环，具体规则如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loop Loop text</span><br><span class="line">... statements...</span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">　　Alice-&gt;&gt;John: Hello!</span><br><span class="line">　　loop Reply every minute</span><br><span class="line">　　　　John-&gt;&gt;Alice:Great!</span><br><span class="line">　　end</span><br></pre></td></tr></table></figure><pre class="mermaid">sequenceDiagram　　Alice->>John: Hello!　　loop Reply every minute　　　　John->>Alice:Great!　　end</pre><h2 id="2-5-选择ALT"><a href="#2-5-选择ALT" class="headerlink" title="2.5 选择ALT"></a>2.5 选择ALT</h2><p>在序列图中选择的表达。规则如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">alt Describing text</span><br><span class="line">...statements...</span><br><span class="line">else</span><br><span class="line">...statements...</span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>或者使用opt(推荐在没有else的情况下使用)</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">opt Describing text</span><br><span class="line">...statements...</span><br><span class="line">end</span><br></pre></td></tr></table></figure><p>示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">sequenceDiagram</span><br><span class="line">　　Alice-&gt;&gt;Bob: Hello Bob, how are you?</span><br><span class="line">　　alt is sick</span><br><span class="line">　　　　Bob-&gt;&gt;Alice:not so good :(</span><br><span class="line">　　else is well</span><br><span class="line">　　　　Bob-&gt;&gt;Alice:Feeling fresh like a daisy:)</span><br><span class="line">　　end</span><br><span class="line">　　opt Extra response</span><br><span class="line">　　　　Bob-&gt;&gt;Alice:Thanks for asking</span><br><span class="line">　　end</span><br></pre></td></tr></table></figure><pre class="mermaid">sequenceDiagram　　Alice->>Bob: Hello Bob, how are you?　　alt is sick　　　　Bob->>Alice:not so good :(　　else is well　　　　Bob->>Alice:Feeling fresh like a daisy:)　　end　　opt Extra response　　　　Bob->>Alice:Thanks for asking　　end</pre><h1 id="3-甘特图-gantt"><a href="#3-甘特图-gantt" class="headerlink" title="3 甘特图(gantt)"></a>3 甘特图(gantt)</h1><p>甘特图是一类条形图，由Karol Adamiechi在1896年提出, 而在1910年Henry Gantt也独立的提出了此种图形表示。通常用在对项目终端元素和总结元素的开始及完成时间进行的描述。</p><p>示例：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">gantt</span><br><span class="line">dateFormat YYYY-MM-DD</span><br><span class="line"></span><br><span class="line">section S1</span><br><span class="line">T1: 2014-01-01, 9d</span><br><span class="line"></span><br><span class="line">section S2</span><br><span class="line">T2: 2014-01-11, 9d</span><br><span class="line"></span><br><span class="line">section S3</span><br><span class="line">T3: 2014-01-02, 9d</span><br><span class="line">gantt</span><br><span class="line">dateFormat YYYY-MM-DD</span><br><span class="line">section S1</span><br><span class="line">T1: 2014-01-01, 9d</span><br><span class="line">section S2</span><br><span class="line">T2: 2014-01-11, 9d</span><br><span class="line">section S3</span><br><span class="line">T3: 2014-01-02, 9d</span><br></pre></td></tr></table></figure><p>先来看一个的例子：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">gantt</span><br><span class="line">dateFormat  YYYY-MM-DD</span><br><span class="line">title Adding GANTT diagram functionality to mermaid</span><br><span class="line"></span><br><span class="line">section A section</span><br><span class="line">Completed task            :done,    des1, 2014-01-06,2014-01-08</span><br><span class="line">Active task               :active,  des2, 2014-01-09, 3d</span><br><span class="line">Future task               :         des3, after des2, 5d</span><br><span class="line">Future task2               :         des4, after des3, 5d</span><br><span class="line"></span><br><span class="line">section Critical tasks</span><br><span class="line">Completed task in the critical line :crit, done, 2014-01-06,24h</span><br><span class="line">Implement parser and jison          :crit, done, after des1, 2d</span><br><span class="line">Create tests for parser             :crit, active, 3d</span><br><span class="line">Future task in critical line        :crit, 5d</span><br><span class="line">Create tests for renderer           :2d</span><br><span class="line">Add to mermaid                      :1d</span><br><span class="line"></span><br><span class="line">section Documentation</span><br><span class="line">Describe gantt syntax               :active, a1, after des1, 3d</span><br><span class="line">Add gantt diagram to demo page      :after a1  , 20h</span><br><span class="line">Add another diagram to demo page    :doc1, after a1  , 48h</span><br><span class="line"></span><br><span class="line">section Last section</span><br><span class="line">Describe gantt syntax               :after doc1, 3d</span><br><span class="line">Add gantt diagram to demo page      : 20h</span><br><span class="line">Add another diagram to demo page    : 48h</span><br></pre></td></tr></table></figure><pre class="mermaid">gantt    dateFormat  YYYY-MM-DD    title Adding GANTT diagram functionality to mermaid    section A section    Completed task            :done,    des1, 2014-01-06,2014-01-08    Active task               :active,  des2, 2014-01-09, 3d    Future task               :         des3, after des2, 5d    Future task2               :         des4, after des3, 5d    section Critical tasks    Completed task in the critical line :crit, done, 2014-01-06,24h    Implement parser and jison          :crit, done, after des1, 2d    Create tests for parser             :crit, active, 3d    Future task in critical line        :crit, 5d    Create tests for renderer           :2d    Add to mermaid                      :1d    section Documentation    Describe gantt syntax               :active, a1, after des1, 3d    Add gantt diagram to demo page      :after a1  , 20h    Add another diagram to demo page    :doc1, after a1  , 48h    section Last section    Describe gantt syntax               :after doc1, 3d    Add gantt diagram to demo page      : 20h    Add another diagram to demo page    : 48h</pre><p>具体关键字中英对应表。</p><table><thead><tr><th>名称</th><th>解释</th></tr></thead><tbody><tr><td>title</td><td>标题</td></tr><tr><td>dateFormat</td><td>日期格式</td></tr><tr><td>section</td><td>模块</td></tr><tr><td>Completed</td><td>已经完成</td></tr><tr><td>Active</td><td>当前正在进行</td></tr><tr><td>Future</td><td>后续待处理</td></tr><tr><td>crit</td><td>关键阶段</td></tr><tr><td>日期缺失</td><td>默认从上一项完成后</td></tr></tbody></table><p>关于日期的格式可以参考：  <a href="http://momentjs.com/docs/#/parsing/string-format/">string-format</a>    <a href="https://github.com/mbostock/d3/wiki/Time-Formatting">Time-Formatting</a></p><h1 id="4-Demo"><a href="#4-Demo" class="headerlink" title="4 Demo"></a>4 Demo</h1><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">graph TB</span><br><span class="line">    sq[Square shape] --&gt; ci((Circle shape))</span><br><span class="line"></span><br><span class="line">    subgraph A subgraph</span><br><span class="line">        di&#123;Diamond with  line break&#125; -.-&gt; ro(Rounded)</span><br><span class="line">        di==&gt;ro2(Rounded square shape)</span><br><span class="line">    end</span><br><span class="line"></span><br><span class="line">    e --&gt; od3&gt;Really long text with linebreak&lt;br&gt;in an Odd shape]</span><br><span class="line"></span><br><span class="line">    cyr[Cyrillic]--&gt;cyr2((Circle shape Начало));</span><br><span class="line"></span><br><span class="line">    classDef green fill:#9f6,stroke:#333,stroke-width:2px;</span><br><span class="line">    classDef orange fill:#f96,stroke:#333,stroke-width:4px;</span><br><span class="line">    class sq,e green</span><br><span class="line">    class di orange</span><br></pre></td></tr></table></figure><pre class="mermaid">graph TB    sq[Square shape] --> ci((Circle shape))    subgraph A subgraph        di{Diamond with  line break} -.-> ro(Rounded)        di==>ro2(Rounded square shape)    end    e --> od3>Really long text with linebreak<br>in an Odd shape]    cyr[Cyrillic]-->cyr2((Circle shape Начало));    classDef green fill:#9f6,stroke:#333,stroke-width:2px;    classDef orange fill:#f96,stroke:#333,stroke-width:4px;    class sq,e green    class di orange</pre><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>markdown中所有图形的绘制最终其实都是在使用mermaid，该插件的具体官方地址为<a href="https://mermaidjs.github.io/">mermaidis</a>，本文中没有涵盖的需求具体可以在该链接中找到，总结来说这个绘图功能还是很强大的，在写这篇博客的过程中有一个很深的感触，就是你自己认为的一个很特别或者是小众的需求，其实很多的前辈们早就在之前已经想到、做过或者已经给出了完美的解决方案，所有平时没事的时候还是要多接触、多了解、多积累，等到真正使用的时候才不至于<strong>抓狂</strong>。</p><div class="note primary">            <h4 id="本文作者：tongtong"><a href="#本文作者：tongtong" class="headerlink" title="本文作者：tongtong"></a>本文作者：tongtong</h4><h4 id="本文链接：https-stongtong-github-io"><a href="#本文链接：https-stongtong-github-io" class="headerlink" title="本文链接：https://stongtong.github.io/"></a>本文链接：<a href="https://stongtong.github.io/">https://stongtong.github.io/</a></h4><h4 id="版权申明：网站内容为tongtong所有，转载请注明出处。"><a href="#版权申明：网站内容为tongtong所有，转载请注明出处。" class="headerlink" title="版权申明：网站内容为tongtong所有，转载请注明出处。"></a>版权申明：网站内容为tongtong所有，转载请注明出处。</h4>          </div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;前言：&lt;/strong&gt;一直在用七牛云作为博客的图床，但是之前发现在七牛云的图床网址必须要进行备案了，太委屈！我自己的个人网站，就是在写写自己的日记、技术笔记、工作感悟、巴拉巴拉巴拉，还要让我备案，真是没地说理去；辗转反侧，也不能因为这个事情不写、不记录啊，所以最近萌生了博客无图的想法，之前用markdown的时候正好接触过markdown的绘图功能，身材很好（轻量级）并且该有的都有，哈哈，今天用这篇博客记录一下markdown中的常用绘图语法，方便以后使用。&lt;/p&gt;
    
    </summary>
    
      <category term="markdown" scheme="https://tongfan.xyz/categories/markdown/"/>
    
    
      <category term="markdown" scheme="https://tongfan.xyz/tags/markdown/"/>
    
      <category term="mermaid" scheme="https://tongfan.xyz/tags/mermaid/"/>
    
  </entry>
  
  <entry>
    <title>spark中的CheckPoint原理</title>
    <link href="https://tongfan.xyz/2018/01/12/spark%E4%B8%ADcheckpoint%E5%8E%9F%E7%90%86/"/>
    <id>https://tongfan.xyz/2018/01/12/spark中checkpoint原理/</id>
    <published>2018-01-12T11:12:24.000Z</published>
    <updated>2019-03-03T12:09:55.695Z</updated>
    
    <content type="html"><![CDATA[<p><strong>摘要：</strong>本文对spark中checkpoint机制进行分析说明，并对spark checkpoint的实现源码进行分析，使用到的spark版本为2.3.1。</p><a id="more"></a><h1 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h1><p>为什么要用checkpoint？什么情况下会使用checkpoint？带着这些问题，我们来认识checkpoint。</p><ol><li>Spark 在生产环境下经常会面临 Transformation 的 RDD 非常多(例如一个Job 中包含1万个RDD) 或者是具体的 Transformation 产生的 <strong>RDD 本身计算特别复杂和耗时</strong>(例如计算时常超过1个小时) , 可能业务比较复杂，此时我们必需考虑对中间计算结果的持久化。</li><li>Spark 是擅长<strong>多步骤迭代</strong>，同时擅长<strong>基于 Job 的复用</strong>。这个时候如果可以对曾经的计算过程进行复用，就可以极大的提升效率。因为有时候有共同的步骤，就可以免却重复计算的时间（具体过程需要明白spark中job的生成机制）。</li><li>如果采用 <strong>persists</strong> 把数据在内存（内存+磁盘，具体看配置）中的话，虽然最快速但是也是最不可靠的；如果放在磁盘上也不是完全可靠的，例如磁盘会损坏，系统管理员可能会清空磁盘。</li><li>Checkpoint 的产生就是为了相对而言更加可靠的持久化数据，在 Checkpoint 可以指定把数据放在本地并且是多副本的方式，但是在正常生产环境下放在 HDFS 上，这就自然借助HDFS 高可靠的特征来完成最大化的<strong>可靠的持久化数据的方式</strong>。</li><li>Checkpoint 是为了<strong>最大程度保证绝对可靠的复用 RDD</strong> 计算数据的 Spark 的高级功能，通过 Checkpoint 我们通过把数据持久化到 HDFS 上来保证数据的最大程度的安全、复用。</li><li>Checkpoint 就是针对整个RDD计算链条中<strong>特别需要数据持久化的环节</strong>(后面会反覆使用当前环节的RDD) 开始基于HDFS 等的<strong>数据持久化复用策略</strong>，通过对 RDD 启动 Checkpoint 机制来<strong>实现容错和高可用</strong>；</li></ol><p>以上六点，即spark中为什么要设置checkpoint机制以及在什么场景下应用checkpoint机制，该机制如果使用合适，将会成倍的减少程序的运行时间，尤其是在处理过程繁琐，spark处理步骤比较多并且处理分支多的情况下。</p><h1 id="使用"><a href="#使用" class="headerlink" title="使用"></a>使用</h1><p>在spark中使用checkpoint，首先在sc中申明checkpoint的路径，然后针对要store的rdd，直接调用rdd中的checkpoint方法即可，</p><h1 id="reliable-cp"><a href="#reliable-cp" class="headerlink" title="reliable cp"></a>reliable cp</h1><p>具体代码如下：</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"yarn"</span>).setAppName(<span class="string">"HBaseTest"</span>)</span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"><span class="keyword">val</span> hbaseRDD = sc.newAPIHadoopRDD(hBaseConf, classOf[<span class="type">TableInputFormat</span>],</span><br><span class="line">                                  classOf[<span class="type">ImmutableBytesWritable</span>], classOf[<span class="type">Result</span>])</span><br><span class="line">sc.setCheckpointDir(<span class="string">""</span>)</span><br><span class="line">...</span><br><span class="line"><span class="keyword">val</span> protoBufRDD = hbaseRDD.map(r =&gt;(<span class="type">Integer</span>.parseInt(<span class="type">Bytes</span>.toString(r._1.get).substring(<span class="number">8</span>, <span class="number">18</span>)),</span><br><span class="line">        r._2))</span><br><span class="line">protoBufRDD.checkpoint()</span><br></pre></td></tr></table></figure><ol><li>使用比较简单，这里要说明的是如果开启checkpoint机制，在rdd的iterator方法中，会有对应的查找操作。也就是說，如果設置了cp机制，RDD的加载会首先从cp里边进行加载。</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * spark rdd的内部方法，如果缓存可用则会从缓存加载，否则进行计算.</span></span><br><span class="line"><span class="comment">   * 该方法用户不能直接call， 但在实现RDD的子RDD中可用.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">final</span> <span class="function"><span class="keyword">def</span> <span class="title">iterator</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">    <span class="keyword">if</span> (storageLevel != <span class="type">StorageLevel</span>.<span class="type">NONE</span>) &#123;</span><br><span class="line">      getOrCompute(split, context)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      computeOrReadCheckpoint(split, context)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Compute an RDD partition or read it from a checkpoint if the RDD is checkpointing.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">computeOrReadCheckpoint</span></span>(split: <span class="type">Partition</span>, context: <span class="type">TaskContext</span>): <span class="type">Iterator</span>[<span class="type">T</span>] =</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">if</span> (isCheckpointedAndMaterialized) &#123;</span><br><span class="line">      firstParent[<span class="type">T</span>].iterator(split, context)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      compute(split, context)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span>[spark] <span class="function"><span class="keyword">def</span> <span class="title">isCheckpointedAndMaterialized</span></span>: <span class="type">Boolean</span> = checkpointData.exists(_.isCheckpointed)</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * Set the directory under which RDDs are going to be checkpointed.</span></span><br><span class="line"><span class="comment"> * @param directory path to the directory where checkpoint files will be stored</span></span><br><span class="line"><span class="comment"> * (must be HDFS path if running in cluster)</span></span><br><span class="line"><span class="comment"> * 可以设置多个目录，来提高checkpoint过程的效率</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setCheckpointDir</span></span>(directory: <span class="type">String</span>) &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// If we are running on a cluster, log a warning if the directory is local.</span></span><br><span class="line">  <span class="comment">// Otherwise, the driver may attempt to reconstruct the checkpointed RDD from</span></span><br><span class="line">  <span class="comment">// its own local file system, which is incorrect because the checkpoint files</span></span><br><span class="line">  <span class="comment">// are actually on the executor machines.</span></span><br><span class="line">  <span class="keyword">if</span> (!isLocal &amp;&amp; <span class="type">Utils</span>.nonLocalPaths(directory).isEmpty) &#123;</span><br><span class="line">    logWarning(<span class="string">"Spark is not running in local mode, therefore the checkpoint directory "</span> +</span><br><span class="line">      <span class="string">s"must not be on the local filesystem. Directory '<span class="subst">$directory</span>' "</span> +</span><br><span class="line">      <span class="string">"appears to be on the local filesystem."</span>)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  checkpointDir = <span class="type">Option</span>(directory).map &#123; dir =&gt;</span><br><span class="line">    <span class="keyword">val</span> path = <span class="keyword">new</span> <span class="type">Path</span>(dir, <span class="type">UUID</span>.randomUUID().toString)</span><br><span class="line">    <span class="keyword">val</span> fs = path.getFileSystem(hadoopConfiguration)</span><br><span class="line">    fs.mkdirs(path)</span><br><span class="line">    fs.getFileStatus(path).getPath.toString</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><ol><li>进行checkpoint操作都发生了什么？</li></ol><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Mark this RDD for checkpointing. It will be saved to a file inside the checkpoint</span></span><br><span class="line"><span class="comment">   * directory set with `SparkContext#setCheckpointDir` and all references to its parent</span></span><br><span class="line"><span class="comment">   * RDDs will be removed. This function must be called before any job has been</span></span><br><span class="line"><span class="comment">   * executed on this RDD. It is strongly recommended that this RDD is persisted in</span></span><br><span class="line"><span class="comment">   * memory, otherwise saving it on a file will require recomputation.</span></span><br><span class="line"><span class="comment">   * 此操作会删除该RDD的所有父RDD的引用，在此RDD的任何job执行之前进行调用，推荐将RDD持久化在</span></span><br><span class="line"><span class="comment">   * memory，否则将它保存在文件中将需要重新计算。（这就是官方强烈推荐的cp之前一定要进行memory）</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">checkpoint</span></span>(): <span class="type">Unit</span> = <span class="type">RDDCheckpointData</span>.synchronized &#123;</span><br><span class="line">  <span class="comment">// <span class="doctag">NOTE:</span> we use a global lock here due to complexities downstream with ensuring</span></span><br><span class="line">  <span class="comment">// children RDD partitions point to the correct parent partitions. In the future</span></span><br><span class="line">  <span class="comment">// we should revisit this consideration.</span></span><br><span class="line">  <span class="comment">// 设置全局锁的原因是确保下游子RDD分区正确的指向父RDD的分区，后续版本可能变动</span></span><br><span class="line">  <span class="keyword">if</span> (context.checkpointDir.isEmpty) &#123;</span><br><span class="line">    <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Checkpoint directory has not been set in the SparkContext"</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (checkpointData.isEmpty) &#123;</span><br><span class="line">    <span class="comment">// 调用ReliableRDDCheckpointData，将RDD保存在可靠的dfs上，允许drivers在失败时以先前计算</span></span><br><span class="line">    <span class="comment">// 的状态重新启动。</span></span><br><span class="line">    checkpointData = <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">ReliableRDDCheckpointData</span>(<span class="keyword">this</span>))</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>在ReliableRDDCheckpointData之后会触发真正的cp操作， checkpoint 是 lazy 级别，必需有 Job 的执行且在Job 执行完成后才会从后往前回溯那个 RDD 进行了Checkpoint 标记，然后对该标记了要进行 Checkpoint 的 RDD 新启动一个Job 执行具体 Checkpoint 的过程；还有一个重点：<strong>Checkpoint 改变了 RDD 的 Lineage</strong>，因为之前的依赖父引用全部删除了。</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Materialize this RDD and write its content to a reliable DFS.</span></span><br><span class="line"><span class="comment">   * This is called immediately after the first action invoked on this RDD has completed.(在第一个action)</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doCheckpoint</span></span>(): <span class="type">CheckpointRDD</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> newRDD = <span class="type">ReliableCheckpointRDD</span>.writeRDDToCheckpointDirectory(rdd, cpDir)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Optionally clean our checkpoint files if the reference is out of scope</span></span><br><span class="line">    <span class="keyword">if</span> (rdd.conf.getBoolean(<span class="string">"spark.cleaner.referenceTracking.cleanCheckpoints"</span>, <span class="literal">false</span>)) &#123;</span><br><span class="line">      rdd.context.cleaner.foreach &#123; cleaner =&gt;</span><br><span class="line">        cleaner.registerRDDCheckpointDataForCleanup(newRDD, rdd.id)</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    logInfo(<span class="string">s"Done checkpointing RDD <span class="subst">$&#123;rdd.id&#125;</span> to <span class="subst">$cpDir</span>, new parent is RDD <span class="subst">$&#123;newRDD.id&#125;</span>"</span>)</span><br><span class="line">    newRDD</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Write RDD to checkpoint files and return a ReliableCheckpointRDD representing the RDD.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">writeRDDToCheckpointDirectory</span></span>[<span class="type">T</span>: <span class="type">ClassTag</span>](</span><br><span class="line">      originalRDD: <span class="type">RDD</span>[<span class="type">T</span>],</span><br><span class="line">      checkpointDir: <span class="type">String</span>,</span><br><span class="line">      blockSize: <span class="type">Int</span> = <span class="number">-1</span>): <span class="type">ReliableCheckpointRDD</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> checkpointStartTimeNs = <span class="type">System</span>.nanoTime()</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> sc = originalRDD.sparkContext</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Create the output path for the checkpoint</span></span><br><span class="line">    <span class="keyword">val</span> checkpointDirPath = <span class="keyword">new</span> <span class="type">Path</span>(checkpointDir)</span><br><span class="line">    <span class="keyword">val</span> fs = checkpointDirPath.getFileSystem(sc.hadoopConfiguration)</span><br><span class="line">    <span class="keyword">if</span> (!fs.mkdirs(checkpointDirPath)) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">s"Failed to create checkpoint path <span class="subst">$checkpointDirPath</span>"</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Save to file, and reload it as an RDD</span></span><br><span class="line">    <span class="keyword">val</span> broadcastedConf = sc.broadcast(</span><br><span class="line">      <span class="keyword">new</span> <span class="type">SerializableConfiguration</span>(sc.hadoopConfiguration))</span><br><span class="line">    <span class="comment">// <span class="doctag">TODO:</span> This is expensive because it computes the RDD again unnecessarily (SPARK-8582)</span></span><br><span class="line">    <span class="comment">// 重新提交一个job，保持和原分区的一致性</span></span><br><span class="line">    sc.runJob(originalRDD,</span><br><span class="line">      writePartitionToCheckpointFile[<span class="type">T</span>](checkpointDirPath.toString, broadcastedConf) _)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (originalRDD.partitioner.nonEmpty) &#123;</span><br><span class="line">      writePartitionerToCheckpointDir(sc, originalRDD.partitioner.get, checkpointDirPath)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> checkpointDurationMs =</span><br><span class="line">      <span class="type">TimeUnit</span>.<span class="type">NANOSECONDS</span>.toMillis(<span class="type">System</span>.nanoTime() - checkpointStartTimeNs)</span><br><span class="line">    logInfo(<span class="string">s"Checkpointing took <span class="subst">$checkpointDurationMs</span> ms."</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> newRDD = <span class="keyword">new</span> <span class="type">ReliableCheckpointRDD</span>[<span class="type">T</span>](</span><br><span class="line">      sc, checkpointDirPath.toString, originalRDD.partitioner)</span><br><span class="line">    <span class="keyword">if</span> (newRDD.partitions.length != originalRDD.partitions.length) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(</span><br><span class="line">        <span class="string">"Checkpoint RDD has a different number of partitions from original RDD. Original "</span> +</span><br><span class="line">          <span class="string">s"RDD [ID: <span class="subst">$&#123;originalRDD.id&#125;</span>, num of partitions: <span class="subst">$&#123;originalRDD.partitions.length&#125;</span>]; "</span> +</span><br><span class="line">          <span class="string">s"Checkpoint RDD [ID: <span class="subst">$&#123;newRDD.id&#125;</span>, num of partitions: "</span> +</span><br><span class="line">          <span class="string">s"<span class="subst">$&#123;newRDD.partitions.length&#125;</span>]."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    newRDD</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>在真正写入磁盘的配置中，可以设置cp的具体压缩方式<code>spark.checkpoint.compress</code>, 一般使用<code>spark.io.compression.codec</code>，最后会重新返回一个带cp标识的新RDD。</p><p>到了这里是一个比较完整的过程了，当然还没有结束，因为在spark中还提供了另外一种方式的cp实现，那就是local cp，下面详细介绍local cp的作用以及使用场景。</p><h1 id="local-cp"><a href="#local-cp" class="headerlink" title="local cp"></a>local cp</h1><p>这个是将RDD进行本地cp，主要利用spark已经存在的缓存层来进行存储，该功能主要应用在用户想要截断lineage，避免在分布式文件系统下的物理数据expensive复制操作，一般用于lineage较长需要截断的场景（例如graph）。</p><p>local cp牺牲了容错性，其cp data并没有保存在高可靠、高容错的storage，而是保存在executor的短时本地存储，这样做的问题就是如果在计算过程中一个executor fail，那么cp data将不可以访问，造成不可恢复的job 失败。</p><p>在使用local cp特性时，如果使用spark的资源动态分配是不安全，因为动态分配会删除executor连同它的缓存块一起，所以如果要使用这两个特性，可以将参数<code>spark.dynamicAllocation.cachedExecutorIdleTimeout</code>的权值设置的比较大。</p><p>最后最重要的一点，<strong>local cp 并不会使用设置的cp path。</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">localCheckpoint</span></span>(): <span class="keyword">this</span>.<span class="keyword">type</span> = <span class="type">RDDCheckpointData</span>.synchronized &#123;</span><br><span class="line">    <span class="keyword">if</span> (conf.getBoolean(<span class="string">"spark.dynamicAllocation.enabled"</span>, <span class="literal">false</span>) &amp;&amp;</span><br><span class="line">        conf.contains(<span class="string">"spark.dynamicAllocation.cachedExecutorIdleTimeout"</span>)) &#123;</span><br><span class="line">      logWarning(<span class="string">"Local checkpointing is NOT safe to use with dynamic allocation, "</span> +</span><br><span class="line">        <span class="string">"which removes executors along with their cached blocks. If you must use both "</span> +</span><br><span class="line">        <span class="string">"features, you are advised to set `spark.dynamicAllocation.cachedExecutorIdleTimeout` "</span> +</span><br><span class="line">        <span class="string">"to a high value. E.g. If you plan to use the RDD for 1 hour, set the timeout to "</span> +</span><br><span class="line">        <span class="string">"at least 1 hour."</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Note: At this point we do not actually know whether the user will call persist() on</span></span><br><span class="line">    <span class="comment">// this RDD later, so we must explicitly call it here ourselves to ensure the cached</span></span><br><span class="line">    <span class="comment">// blocks are registered for cleanup later in the SparkContext.</span></span><br><span class="line">    <span class="comment">//</span></span><br><span class="line">    <span class="comment">// If, however, the user has already called persist() on this RDD, then we must adapt</span></span><br><span class="line">    <span class="comment">// the storage level he/she specified to one that is appropriate for local checkpointing</span></span><br><span class="line">    <span class="comment">// (i.e. uses disk) to guarantee correctness.</span></span><br><span class="line">  <span class="comment">// 简而言之，如果已经设置，则按照用户的设置级别来；如果没有那就是 StorageLevel.MEMORY_AND_DISK</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (storageLevel == <span class="type">StorageLevel</span>.<span class="type">NONE</span>) &#123;</span><br><span class="line">      persist(<span class="type">LocalRDDCheckpointData</span>.<span class="type">DEFAULT_STORAGE_LEVEL</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      persist(<span class="type">LocalRDDCheckpointData</span>.transformStorageLevel(storageLevel), allowOverride = <span class="literal">true</span>)</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// If this RDD is already checkpointed and materialized, its lineage is already truncated.</span></span><br><span class="line">    <span class="comment">// We must not override our `checkpointData` in this case because it is needed to recover</span></span><br><span class="line">    <span class="comment">// the checkpointed data. If it is overridden, next time materializing on this RDD will</span></span><br><span class="line">    <span class="comment">// cause error.</span></span><br><span class="line">    <span class="keyword">if</span> (isCheckpointedAndMaterialized) &#123;</span><br><span class="line">      logWarning(<span class="string">"Not marking RDD for local checkpoint because it was already "</span> +</span><br><span class="line">        <span class="string">"checkpointed and materialized"</span>)</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// Lineage is not truncated yet, so just override any existing checkpoint data with ours</span></span><br><span class="line">      checkpointData <span class="keyword">match</span> &#123;</span><br><span class="line">        <span class="keyword">case</span> <span class="type">Some</span>(_: <span class="type">ReliableRDDCheckpointData</span>[_]) =&gt; logWarning(</span><br><span class="line">          <span class="string">"RDD was already marked for reliable checkpointing: overriding with local checkpoint."</span>)</span><br><span class="line">        <span class="keyword">case</span> _ =&gt;</span><br><span class="line">      &#125;</span><br><span class="line">      checkpointData = <span class="type">Some</span>(<span class="keyword">new</span> <span class="type">LocalRDDCheckpointData</span>(<span class="keyword">this</span>))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">this</span></span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Ensure the RDD is fully cached so the partitions can be recovered later.</span></span><br><span class="line"><span class="comment">   * 这个特别重要，</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="keyword">protected</span> <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">doCheckpoint</span></span>(): <span class="type">CheckpointRDD</span>[<span class="type">T</span>] = &#123;</span><br><span class="line">    <span class="keyword">val</span> level = rdd.getStorageLevel</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Assume storage level uses disk; otherwise memory eviction may cause data loss</span></span><br><span class="line">    assume(level.useDisk, <span class="string">s"Storage level <span class="subst">$level</span> is not appropriate for local checkpointing"</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">// Not all actions compute all partitions of the RDD (e.g. take). For correctness, we</span></span><br><span class="line">    <span class="comment">// must cache any missing partitions. <span class="doctag">TODO:</span> avoid running another job here (SPARK-8582).</span></span><br><span class="line">    <span class="keyword">val</span> action = (tc: <span class="type">TaskContext</span>, iterator: <span class="type">Iterator</span>[<span class="type">T</span>]) =&gt; <span class="type">Utils</span>.getIteratorSize(iterator)</span><br><span class="line">    <span class="keyword">val</span> missingPartitionIndices = rdd.partitions.map(_.index).filter &#123; i =&gt;</span><br><span class="line">      <span class="comment">// 从blockManager的master中检查，不包含的那就missing了 </span></span><br><span class="line">      !<span class="type">SparkEnv</span>.get.blockManager.master.contains(<span class="type">RDDBlockId</span>(rdd.id, i))</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (missingPartitionIndices.nonEmpty) &#123;</span><br><span class="line">      <span class="comment">// 上面英文注释中说明避免重新提交的操作，但如果确实存在missingPartition，只能重新提交job</span></span><br><span class="line">      rdd.sparkContext.runJob(rdd, action, missingPartitionIndices)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">LocalCheckpointRDD</span>[<span class="type">T</span>](rdd)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>以上就是checkpoint的机制，spark中的两种实现方式reliable和local，针对不同的场景使用不同。</p><div class="note primary">            <h4 id="本文作者：tongtong"><a href="#本文作者：tongtong" class="headerlink" title="本文作者：tongtong"></a>本文作者：tongtong</h4><h4 id="本文链接：https-stongtong-github-io"><a href="#本文链接：https-stongtong-github-io" class="headerlink" title="本文链接：https://stongtong.github.io/"></a>本文链接：<a href="https://stongtong.github.io/">https://stongtong.github.io/</a></h4><h4 id="版权申明：网站内容为tongtong所有，转载请注明出处。"><a href="#版权申明：网站内容为tongtong所有，转载请注明出处。" class="headerlink" title="版权申明：网站内容为tongtong所有，转载请注明出处。"></a>版权申明：网站内容为tongtong所有，转载请注明出处。</h4>          </div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;摘要：&lt;/strong&gt;本文对spark中checkpoint机制进行分析说明，并对spark checkpoint的实现源码进行分析，使用到的spark版本为2.3.1。&lt;/p&gt;
    
    </summary>
    
      <category term="Spark" scheme="https://tongfan.xyz/categories/Spark/"/>
    
    
      <category term="Spark，CheckPoint" scheme="https://tongfan.xyz/tags/Spark%EF%BC%8CCheckPoint/"/>
    
  </entry>
  
  <entry>
    <title>HBASE中LSM Tree</title>
    <link href="https://tongfan.xyz/2017/04/05/hbase01-LSM%20Tree/"/>
    <id>https://tongfan.xyz/2017/04/05/hbase01-LSM Tree/</id>
    <published>2017-04-05T02:30:44.000Z</published>
    <updated>2019-02-17T11:00:00.615Z</updated>
    
    <content type="html"><![CDATA[<p><strong>摘要</strong>： HBASE最为出色的功能就是大数据量的写入性能，今天要介绍的就是HBASE中核心的数据结构LSM Tree，该数据结构保证了HBASE的数据写入的性能，废话不多说，开始正文。</p><a id="more"></a><h1 id="1-LSM-Tree由来"><a href="#1-LSM-Tree由来" class="headerlink" title="1 LSM Tree由来"></a>1 LSM Tree由来</h1><p>​    LSM Tree，全称<strong>Log-structured merge-tree</strong>，讲LSM树之前，需要提下三种基本的存储引擎，这样才能清楚LSM树的由来。</p><h2 id="（1）哈希存储引擎"><a href="#（1）哈希存储引擎" class="headerlink" title="（1）哈希存储引擎"></a>（1）哈希存储引擎</h2><p>​    哈希存储引擎，就是以哈希表为基础持久化实现，支持增、删、改以及随机读取操作，<strong>但不支持顺序扫描</strong>，对应的存储系统为key-value存储系统。对于key-value的插入、查询、删除操作，哈希表的复杂度都是O(1)；但如果在需求中需要进行顺序扫描或者范围查找，hash并不能够支持。</p><h2 id="（2）B树存储引擎"><a href="#（2）B树存储引擎" class="headerlink" title="（2）B树存储引擎"></a>（2）B树存储引擎</h2><p>​    B树存储引擎是以B树（一般都是B+树）的持久化实现，该种数据结构不仅支持单条记录的增、删、读、改操作，还支持顺序扫描（下面有详细的解释），这里对B树、B+树进行简单的复习。</p><p>​    走进搜索引擎的作者梁斌老师针对B树、B+树给出了他的意见，下面引用其文章中的话。</p><blockquote><p>​    “B+树还有一个最大的好处，方便扫库，B树必须用中序遍历的方法按序扫库，而B+树直接从叶子结点挨个扫一遍就完了，B+树支持range-query非常方便，而B树不支持。这是数据库选用B+树的最主要原因。“</p></blockquote><blockquote><p><strong>为什么说B+tree比B树更适合实际应用中操作系统的文件索引和数据库索引</strong>？</p></blockquote><blockquote><p>（1) B+ tree的磁盘读写代价更低<br>     B+tree的内部结点并没有指向关键字具体信息的指针。因此其内部结点相对B树更小。如果把所有同一内部结点的关键字存放在同一盘块中，那么盘块所能容纳的关键字数量也越多。一次性读入内存中的需要查找的关键字也就越多。相对来说IO读写次数也就降低了。</p><p>​    举个例子，假设磁盘中的一个盘块容纳16bytes，而一个关键字2bytes，一个关键字具体信息指针2bytes。一棵9阶B-tree(一个结点最多8个关键字)的内部结点需要2个盘快。而B+ 树内部结点只需要1个盘快。当需要把内部结点读入内存中的时候，B 树就比B+ 树多一次盘块查找时间(在磁盘中就是盘片旋转的时间)。</p><p>（2）B+tree的查询效率更加稳定<br>​     由于非叶子结点并不是最终指向文件内容的结点，而只是叶子结点中关键字的索引。所以任何关键字的查找必须走一条从根结点到叶子结点的路。所有关键字查询的路径长度相同，导致每一个数据的查询效率相当。</p><p>（3）B+ 树在顺序查询方面更加优秀</p><p>​    B树在提高了磁盘IO性能的同时并没有解决元素遍历的效率低下的问题。正是为了解决这个问题，B+树应运而生。B+树只要遍历叶子节点就可以实现整棵树的遍历。而且在数据库中基于范围的查询是非常频繁的，而B树不支持这样的操作（或者说效率太低）。</p></blockquote><blockquote><p><strong>任何一种数据结构都是针对特定场景的，B+树也不例外，其缺点为：</strong></p></blockquote><blockquote><p>​    B+树最大的性能问题是会产生大量的随机IO，随着新数据的插入，叶子节点会慢慢分裂，逻辑上连续的叶子节点在物理上往往不连续，甚至分离的很远，但做范围查询时，会产生大量读随机IO。对于大量的随机写也一样，举一个插入key跨度很大的例子，如7-&gt;1000-&gt;3-&gt;2000 … 新插入的数据存储在磁盘上相隔很远，会产生大量的随机写IO.</p><p>​    从上面可以看出，低下的磁盘寻道速度严重影响性能（近些年来，磁盘寻道速度的发展几乎处于停滞的状态）</p></blockquote><h2 id="（3）LSM树"><a href="#（3）LSM树" class="headerlink" title="（3）LSM树"></a>（3）LSM树</h2><p>​    LSM树存储引擎和B树存储引擎一样，同样支持增、删、读、改、顺序扫描操作。而且其通过批量存储技术，规避磁盘随机写入问题，但同时带来的问题就是<strong>LSM树和B+树相比，LSM树牺牲了部分读性能，用来大幅提高写性能。</strong></p><p>​    下面将对LSM tree进行详细介绍。</p><h1 id="2-LSM-Tree原理"><a href="#2-LSM-Tree原理" class="headerlink" title="2 LSM Tree原理"></a>2 LSM Tree原理</h1><h2 id="2-1-设计思想"><a href="#2-1-设计思想" class="headerlink" title="2.1 设计思想"></a>2.1 设计思想</h2><p>​    LSM树的设计思想非常朴素，简而言之就是<strong>将对数据的修改增量保持在内存中，达到指定的大小限制后将这些修改操作批量写入磁盘</strong>，其具体做法是把一棵大树拆分成N棵小树，它首先将内容写入内存中，随着小树越来越大，当达到内存写入阈值后，内存中的小树会flush到磁盘中，这样磁盘中的树定期做merge操作，合并成一棵稳定的大树（<strong>优化数据的读性能</strong>）。当进行数据读取时，需要合并磁盘中历史数据和内存中最近修改操作，综合两者才能进行完整的数据查找操作。</p><p>​    批量操作减少了磁盘磁臂的移动次数降低了进行数据插入时磁盘磁臂的开销，所以写入性能大大提升，同时读性能有所下降，LSM在进行需要即时响应的操作时会损失I/O效率，最适用于索引插入比查询操作多的情况。</p><h2 id="2-2-详细说明"><a href="#2-2-详细说明" class="headerlink" title="2.2 详细说明"></a>2.2 详细说明</h2><p>​    LSM-Tree主题思想为划分成不同等级的树。可以想象一份索引由两棵树组成：一个存在于内存（可以使其他树结构），一个存在于磁盘（如下图）。</p><p><img src="https://i.loli.net/2019/02/17/5c693e36000ee.png" alt></p><p>​    数据首先会插入到内存中的树，为了防止数据丢失，写内存的同时需要暂时持久化到磁盘即输入数据时数据会以完全有序的形式先存储在日志文件中（对应HBase的MemStore和HLog）。当日志文件被修改时，对应的更新会被先保存在内存中来加速查询。</p><p>​    当内存中树的数据达到阈值时，会进行合并操作。合并操作会从左至右遍历内存中的叶子节点与磁盘中树的叶子节点进行合并，当合并的数据量达到磁盘的存储页的大小时，会将合并的数据持久化到磁盘。同时更新父亲节点对叶子节点的指针（如下图）。     </p><p><img src="https://i.loli.net/2019/02/17/5c693e38cd05d.png" alt="lsm11"></p><p>​    之前存在于磁盘的叶子节点被合并后，这些数据并不会被删除，这些数据会复制一份并与内存中的数据一起顺序写到磁盘。查找通过合并的方式完成，首先搜索内存存储结构，接下来是磁盘存储结构。</p><p>​    LSM树所有节点都是满的并按页存储，经过多次的flush会创建很多数据存储文件，后台线程会将小文件聚合成大文件，因此磁盘的寻道操作就会被限制在一定数目的数据存储文件中，以优化读性能。磁盘上的树结构也可以分割成多个存储文件，因为所有的存储数据都是按照Key有序排列的，因此在现有节点中插入新的关键字不需要重新排序。</p><p>​    LSM-Tree属于传输型，在磁盘传输速率上进行文件的排序和合并以及日志操作，可以更好的拓展到更大的数据规模上，因为它会使用日志文件和一个内存存储结构把随机写操作转化为顺序写，读写独立，不会产生两种操作的竞争。 </p><h1 id="3-hbase中的LSM-tree"><a href="#3-hbase中的LSM-tree" class="headerlink" title="3 hbase中的LSM-tree"></a>3 hbase中的LSM-tree</h1><h2 id="3-1-原理"><a href="#3-1-原理" class="headerlink" title="3.1 原理"></a>3.1 原理</h2><p>HBASE中，数据会先写到内存中，为了防止内存数据丢失，写内存的同时需要持久化到磁盘，对应了HBase的MemStore和HLog（即HBASE中wal，write ahead log）。</p><p>​    MemStore中的数据达到一定的阈值之后，需要将数据刷写到磁盘，即生成HFile（也是一颗小的B+树）文件。</p><p>​    hbase中的minor merge（少量HFile小文件合并）、 major merge（一个region的所有HFile文件合并，可能包括数据删除操作）执行compact操作，同时删除无效数据（过期及删除的数据），多棵小树在这个时机合并成大树，来增强（稳定）读性能。</p><p>​    hbase在实现中，是把整个内存数据达到一定阈值后，flush到disk中，形成一个file，这个file的存储也就是一个小的B+树，因为hbase一般是部署在hdfs上，hdfs不支持对文件的update操作，所以hbase是这样全部内存flush，而不是和磁盘中的小树merge update，这个设计也就能讲通了。内存flush到磁盘上的小树，定期也会合并成一个大树。整体上hbase就是用了lsm tree的思路。</p><h2 id="3-2-优化"><a href="#3-2-优化" class="headerlink" title="3.2 优化"></a>3.2 优化</h2><p><strong>针对LSM树读性能hbase的优化：</strong></p><p>​    <strong>Bloom-filter</strong>:就是个带随机概率的bitmap,可以快速的告诉你，某一个小的有序结构里有没有指定的数据。于是就可以不用二分查找，而只需简单的计算几次就能知道数据是否在某个小集合里啦。效率得到提升，但付出空间代价。</p><p>​    <strong>compact</strong>：小树合并为大树，因为小树性能有问题，所以要有个进程不断地将小树合并到大树上，这样大部分的老数据查询也可以直接使用log2N的方式找到，不需要再进行(N/m)*log2n的查询了。</p><h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>​    HBASE中主要使用LSM-Tree来保证大量写入操作的性能，现在对HBASE的定位也主要集中在数据仓库的角色，存放海量数据，因此其写入需求远大于读取需求；任何一种数据库或者是一种组件，只是针对某些特定场景而存在，不可能针对所有的场景都是适合的，所以在进行技术选型或者产品定位时，在这一方面多考虑一点是特别重要的。</p><div class="note primary">            <h4 id="本文作者：tongtong"><a href="#本文作者：tongtong" class="headerlink" title="本文作者：tongtong"></a>本文作者：tongtong</h4><h4 id="本文链接：https-stongtong-github-io"><a href="#本文链接：https-stongtong-github-io" class="headerlink" title="本文链接：https://stongtong.github.io/"></a>本文链接：<a href="https://stongtong.github.io/">https://stongtong.github.io/</a></h4><h4 id="版权申明：网站内容为tongtong所有，转载请注明出处。"><a href="#版权申明：网站内容为tongtong所有，转载请注明出处。" class="headerlink" title="版权申明：网站内容为tongtong所有，转载请注明出处。"></a>版权申明：网站内容为tongtong所有，转载请注明出处。</h4>          </div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;&lt;strong&gt;摘要&lt;/strong&gt;： HBASE最为出色的功能就是大数据量的写入性能，今天要介绍的就是HBASE中核心的数据结构LSM Tree，该数据结构保证了HBASE的数据写入的性能，废话不多说，开始正文。&lt;/p&gt;
    
    </summary>
    
      <category term="Hbase" scheme="https://tongfan.xyz/categories/Hbase/"/>
    
    
      <category term="Hbase" scheme="https://tongfan.xyz/tags/Hbase/"/>
    
      <category term="LSM Tree" scheme="https://tongfan.xyz/tags/LSM-Tree/"/>
    
  </entry>
  
  <entry>
    <title>Hexo常用命令记录</title>
    <link href="https://tongfan.xyz/2016/08/30/Hexo%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E8%AE%B0%E5%BD%95/"/>
    <id>https://tongfan.xyz/2016/08/30/Hexo常用命令记录/</id>
    <published>2016-08-30T08:52:43.000Z</published>
    <updated>2018-11-11T02:20:13.754Z</updated>
    
    <content type="html"><![CDATA[<h3 id="简写"><a href="#简写" class="headerlink" title="简写"></a>简写</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">hexo n &quot;我的博客&quot; == hexo new &quot;我的博客&quot; //新建文章</span><br><span class="line">hexo new page &quot;我的类别&quot;  //新建目录下的page</span><br><span class="line">hexo p == hexo publish    //发布草稿到站点,其用法为hexo publish [layout] &lt;title&gt;</span><br><span class="line">hexo g == hexo generate  //生成</span><br><span class="line">hexo s == hexo server  //启动服务，当端口冲突时，其可以使用参数p更改端口，具体用法为hexo s -p 端口号</span><br><span class="line">hexo d == hexo deploy  //部署</span><br><span class="line">hexo c == hexo clean  //清楚生成文件</span><br></pre></td></tr></table></figure><a id="more"></a><h3 id="hexo中使用模板"><a href="#hexo中使用模板" class="headerlink" title="hexo中使用模板"></a>hexo中使用模板</h3><p>在hexo根目录下的scaffolds文件下，有生成创建文章的模板，在生成时，可以进行模板指定：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">hexo new photo &quot;My Gallery&quot;  //photo为指定模板，创建时其会到scaffolds文件下查找photo.md文件，进行相册的创建。</span><br></pre></td></tr></table></figure></p><h3 id="前置声明"><a href="#前置声明" class="headerlink" title="前置声明"></a>前置声明</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">layout: 布局，默认为post</span><br><span class="line">title: 文章名称</span><br><span class="line">date: 创建时间</span><br><span class="line">updated: 修改时间</span><br><span class="line">comments: 评论开关</span><br><span class="line">tags: 标签</span><br><span class="line">categories: 类别</span><br><span class="line">permalink: 永久链接</span><br></pre></td></tr></table></figure><h3 id="最后"><a href="#最后" class="headerlink" title="最后"></a>最后</h3><p>基本就是这么多了，以后用到在继续添加。<br><div class="note primary">            <h4 id="本文作者：tongtong"><a href="#本文作者：tongtong" class="headerlink" title="本文作者：tongtong"></a>本文作者：tongtong</h4><h4 id="本文链接：https-stongtong-github-io"><a href="#本文链接：https-stongtong-github-io" class="headerlink" title="本文链接：https://stongtong.github.io/"></a>本文链接：<a href="https://stongtong.github.io/">https://stongtong.github.io/</a></h4><h4 id="版权申明：网站内容为tongtong所有，转载请注明出处。"><a href="#版权申明：网站内容为tongtong所有，转载请注明出处。" class="headerlink" title="版权申明：网站内容为tongtong所有，转载请注明出处。"></a>版权申明：网站内容为tongtong所有，转载请注明出处。</h4>          </div></p>]]></content>
    
    <summary type="html">
    
      &lt;h3 id=&quot;简写&quot;&gt;&lt;a href=&quot;#简写&quot; class=&quot;headerlink&quot; title=&quot;简写&quot;&gt;&lt;/a&gt;简写&lt;/h3&gt;&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;2&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;3&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;4&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;5&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;6&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;7&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&quot;code&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;hexo n &amp;quot;我的博客&amp;quot; == hexo new &amp;quot;我的博客&amp;quot; //新建文章&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;hexo new page &amp;quot;我的类别&amp;quot;  //新建目录下的page&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;hexo p == hexo publish    //发布草稿到站点,其用法为hexo publish [layout] &amp;lt;title&amp;gt;&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;hexo g == hexo generate  //生成&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;hexo s == hexo server  //启动服务，当端口冲突时，其可以使用参数p更改端口，具体用法为hexo s -p 端口号&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;hexo d == hexo deploy  //部署&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;&gt;hexo c == hexo clean  //清楚生成文件&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
    
    </summary>
    
      <category term="note" scheme="https://tongfan.xyz/categories/note/"/>
    
    
      <category term="Hexo" scheme="https://tongfan.xyz/tags/Hexo/"/>
    
  </entry>
  
  <entry>
    <title>github搭建各站记录</title>
    <link href="https://tongfan.xyz/2015/08/29/GitHub%E6%90%AD%E5%BB%BA%E4%B8%AA%E4%BA%BA%E7%BD%91%E7%AB%99%E8%AE%B0%E5%BD%95/"/>
    <id>https://tongfan.xyz/2015/08/29/GitHub搭建个人网站记录/</id>
    <published>2015-08-29T08:52:43.000Z</published>
    <updated>2019-02-20T14:26:28.278Z</updated>
    
    <content type="html"><![CDATA[<p>本次搭建主要使用hexo，安装主题为next主题，包括站内搜索、站点、RSS、还有统计和评论系统，并实现 blog自动发布完成。</p><a id="more"></a><h1 id="主要步骤"><a href="#主要步骤" class="headerlink" title="主要步骤"></a>主要步骤</h1><p>1、下载node js和git，安装完成后，在node js中利用npm进行hexo的安装：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install -g hexo-cli</span><br></pre></td></tr></table></figure><p>2、在指定目录下初始化hexo目录， </p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">hexo ini hexo</span><br><span class="line">cd hexo</span><br><span class="line">npm install</span><br></pre></td></tr></table></figure><p>这里的目录名称为hexo,将以上软件安装好之后，将原hexo文件夹下的文件目录统一存放在新建的hexo文件下即可，基本的框架搭建完成。</p><h1 id="主题配置"><a href="#主题配置" class="headerlink" title="主题配置"></a>主题配置</h1><p>1、设置语言配置，主要在主题的目录下，查看language目录的支持的语言，之前next主题的中文语言文件为zh-Hans，但现在改成了zh-CN，这个一定要注意，要不然配置不会生效。</p><p>2、社交配置和支持的页面目录配置，这一部分主要是根据自己的需要 ，在主题配置文件中进行相应的指定，比较简单，需要说明的是，在指定图标的时候是在链接的后面用英文指定，next图标库支持600+图标这个能很好的满足；放开相关页面目录的配置，一定要新建目录页，比如categories，需要用<code>hexo new page</code>来创建这个页面，要不然会报错。</p><h1 id="部署功能"><a href="#部署功能" class="headerlink" title="部署功能"></a>部署功能</h1><p>要将写好的文件进行部署，需要安装插件，安装步骤为</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-deployer-git --save</span><br></pre></td></tr></table></figure><p>安装完毕使用hexo d命令进行部署。</p><h1 id="RSS功能和mapsite功能"><a href="#RSS功能和mapsite功能" class="headerlink" title="RSS功能和mapsite功能"></a>RSS功能和mapsite功能</h1><p>进行Rss功能扩展需要安装插件，</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-feed --save</span><br></pre></td></tr></table></figure><p>安装完毕后，会在hexo目录中出现node_modules文件夹，然后需要在hexo配置文件和主题配置文件中进行配置如下：<br>1、hexo配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">feed:</span><br><span class="line">  type: atom</span><br><span class="line">  path: atom.xml</span><br><span class="line">  limit: 20</span><br><span class="line">  hub:</span><br></pre></td></tr></table></figure><p>2、主题配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">rss: /atom.xml</span><br></pre></td></tr></table></figure><p>然后重启博客，在生成文件时，就会产生atom.xml文件，显示安装成功。<br>安装百度map-site功能<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-baidu-mapsite --save</span><br></pre></td></tr></table></figure></p><p>1、在hexo配置文件</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">baidusitemap:</span><br><span class="line">    path: baidusitemap.xml</span><br></pre></td></tr></table></figure><p>2、在主题配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sitemap: /baidusitemap.xml</span><br></pre></td></tr></table></figure></p><p>重启博客后，点击查看。(npm install hexo-baidu-url-push –save)</p><h1 id="自定义站点搜索"><a href="#自定义站点搜索" class="headerlink" title="自定义站点搜索"></a>自定义站点搜索</h1><p>安装插件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npm install hexo-generator-searchdb --save</span><br></pre></td></tr></table></figure></p><p>1、在hexo配置文件<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">search:</span><br><span class="line">  path: search.xml</span><br><span class="line">  field: post</span><br><span class="line">  format: html</span><br><span class="line">  limit: 10000</span><br></pre></td></tr></table></figure></p><p>2、在主题文件中配置<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">local_search:</span><br><span class="line">  enable: true</span><br></pre></td></tr></table></figure></p><h1 id="配置分类和标签"><a href="#配置分类和标签" class="headerlink" title="配置分类和标签"></a>配置分类和标签</h1><p>next主题中的分类和标签要通过新建index文件来产生。具体操作如下<br>运行 hexo new page tags 命令后会产生 source/tags/index.md 这个文件，你要修改这个文件，在里面添加一句话type: “tags”。<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">title: &quot;tags&quot;</span><br><span class="line">date: 2015-03-24 08:58:02</span><br></pre></td></tr></table></figure></p><p>添加：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">type: &quot;tags&quot;</span><br></pre></td></tr></table></figure></p><p>然后运行服务器即可访问tags页面了。</p><p>在 2015-05-29 06:22:57，”Leon Lin” <a href="mailto:notifications@github.com">notifications@github.com</a> 写道：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">$ cd path/to/hexo</span><br><span class="line">$ hexo new page tags</span><br><span class="line">$ hexo new page categories</span><br><span class="line">$ vim source/tags/index.md</span><br><span class="line"></span><br><span class="line">add line: type: &quot;tags&quot;</span><br><span class="line"></span><br><span class="line">$ vim source/categories/index.md</span><br><span class="line"></span><br><span class="line">add line: type: &quot;categories&quot;</span><br><span class="line"></span><br><span class="line">$ hexo d -g</span><br></pre></td></tr></table></figure></p><p>做完之後產生的 /tags/index.html 裡面沒有顯示出所用的tag…</p><h1 id="valine评论"><a href="#valine评论" class="headerlink" title="valine评论"></a>valine评论</h1><p>next主题支持多说、友言、畅言、来必力等评论系统，这里说一下，多说已经与17年6月关闭了自己的评论系统，原因估计是不赚钱，友言的评论系统很是难用这里也不多说了，畅言系统要求必须要有国内的ICP备案号，这一点比较坑，自己搭个各站，还要去备案，太过复杂，这里就只剩下来必力了，不过这几种，我都有尝试，尝试下来感觉都不太好，前段时间看到了一个基于LeanCloud的全新的评论系统valine，总结一个词就是<strong><em>好用</em></strong>，所以这里极力推荐使用valine作为评论系统，具体配置也很简单，基本你看了主题的这部分配置项就会明白，和阅读统计基本上差不多。</p><h1 id="添加阅读统计"><a href="#添加阅读统计" class="headerlink" title="添加阅读统计"></a>添加阅读统计</h1><p>在LeanCloud注册账号，创建应用并制定Counter，然后获取AppId和App key在主题配置文件中指定，需要注意的是，这里需要对创建的计数进行安全限制，相关操作hexo论坛有很多，这里不再说明。</p><p>在leancloud上注册账号，本账号具体为<br><img src="http://i.imgur.com/t9Aex4E.png" alt></p><p>然后新建一个应用，应用的名称可以随便选取，在该应用中新建一个class，class权限设置为最大，class名称为Counter，然后进入设置，可以获取ID和Key，</p><p>获取之后，在主题配置文件中进行设定：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">leancloud_visitors:</span><br><span class="line">  enable: true</span><br><span class="line">  app_id: C5p</span><br><span class="line">  app_key: iD</span><br></pre></td></tr></table></figure></p><p>重启blog即可。</p><div class="note primary">            <h4 id="本文作者：tongtong"><a href="#本文作者：tongtong" class="headerlink" title="本文作者：tongtong"></a>本文作者：tongtong</h4><h4 id="本文链接：https-stongtong-github-io"><a href="#本文链接：https-stongtong-github-io" class="headerlink" title="本文链接：https://stongtong.github.io/"></a>本文链接：<a href="https://stongtong.github.io/">https://stongtong.github.io/</a></h4><h4 id="版权申明：网站内容为tongtong所有，转载请注明出处。"><a href="#版权申明：网站内容为tongtong所有，转载请注明出处。" class="headerlink" title="版权申明：网站内容为tongtong所有，转载请注明出处。"></a>版权申明：网站内容为tongtong所有，转载请注明出处。</h4>          </div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;本次搭建主要使用hexo，安装主题为next主题，包括站内搜索、站点、RSS、还有统计和评论系统，并实现 blog自动发布完成。&lt;/p&gt;
    
    </summary>
    
      <category term="next" scheme="https://tongfan.xyz/categories/next/"/>
    
    
      <category term="github" scheme="https://tongfan.xyz/tags/github/"/>
    
      <category term="person" scheme="https://tongfan.xyz/tags/person/"/>
    
  </entry>
  
  <entry>
    <title>Next主题内置标签</title>
    <link href="https://tongfan.xyz/2015/03/23/Next%E4%B8%BB%E9%A2%98%E5%86%85%E7%BD%AE%E6%A0%87%E7%AD%BE/"/>
    <id>https://tongfan.xyz/2015/03/23/Next主题内置标签/</id>
    <published>2015-03-23T11:23:49.000Z</published>
    <updated>2019-02-12T11:13:46.496Z</updated>
    
    <content type="html"><![CDATA[<p>该文章测试next主题中的图片表示，因为看到官网上有很多的内置标签，测试一下，方便以后使用。</p><a id="more"></a><h3 id="字体居中"><a href="#字体居中" class="headerlink" title="字体居中"></a>字体居中</h3><h4 id="HTML方式"><a href="#HTML方式" class="headerlink" title="HTML方式"></a>HTML方式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;blockquote class=&quot;blockquote-center&quot;&gt;该部分为文字居中&lt;/blockquote&gt;</span><br></pre></td></tr></table></figure><p><blockquote class="blockquote-center">该部分为文字居中</blockquote></p><h4 id="内置标签方式"><a href="#内置标签方式" class="headerlink" title="内置标签方式"></a>内置标签方式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% centerquote %&#125;blah blah blah&#123;% endcenterquote %&#125;</span><br></pre></td></tr></table></figure><blockquote class="blockquote-center"><p>居中文字标签模式</p></blockquote><p>标签简写模式<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 标签别名 --&gt;</span><br><span class="line">&#123;% cq %&#125; blah blah blah &#123;% endcq %&#125;</span><br></pre></td></tr></table></figure></p><h3 id="突破宽度限制图片"><a href="#突破宽度限制图片" class="headerlink" title="突破宽度限制图片"></a>突破宽度限制图片</h3><p>当使用此标签引用图片时，图片将自动扩大 26%，并突破文章容器的宽度。 此标签使用于需要突出显示的图片, 图片的扩大与容器的偏差从视觉上提升图片的吸引力。 此标签有两种调用方式（详细参看底下示例）：</p><h4 id="HTML方式：使用这种方式时，为-img-添加属性-class-”full-image”即可。"><a href="#HTML方式：使用这种方式时，为-img-添加属性-class-”full-image”即可。" class="headerlink" title="HTML方式：使用这种方式时，为 img 添加属性 class=”full-image”即可。"></a>HTML方式：使用这种方式时，为 img 添加属性 class=”full-image”即可。</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;img src=&quot;/image-url&quot; class=&quot;full-image&quot; /&gt;</span><br></pre></td></tr></table></figure><p><img src="http://i.imgur.com/drFzc4r.jpg" class="full-image"><br><!-- more --></p><h4 id="标签方式：使用-fullimage-或者-简写-fi，-并传递图片地址、-alt-和-title-属性即可。-属性之间以逗号分隔。"><a href="#标签方式：使用-fullimage-或者-简写-fi，-并传递图片地址、-alt-和-title-属性即可。-属性之间以逗号分隔。" class="headerlink" title="标签方式：使用 fullimage 或者 简写 fi， 并传递图片地址、 alt 和 title 属性即可。 属性之间以逗号分隔。"></a>标签方式：使用 fullimage 或者 简写 fi， 并传递图片地址、 alt 和 title 属性即可。 属性之间以逗号分隔。</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% fullimage /image-url, alt, title %&#125;</span><br></pre></td></tr></table></figure><span itemprop="image" itemscope itemtype="http://schema.org/ImageObject"><img itemprop="url image" src="http://i.imgur.com/drFzc4r.jpg" class="full-image" alt="这里是alt" title="这是图片名称"><meta itemprop="width" content="auto"><meta itemprop="height" content="auto"></span><p>可以简写为：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&lt;!-- 别名 --&gt;</span><br><span class="line">&#123;% fi /image-url, alt, title %&#125;</span><br></pre></td></tr></table></figure></p><h4 id="这里是markdown自带的显示："><a href="#这里是markdown自带的显示：" class="headerlink" title="这里是markdown自带的显示："></a>这里是markdown自带的显示：</h4><p><img src="http://i.imgur.com/drFzc4r.jpg" alt="图片名称显示在哪"></p><p>以上三种比较，很是明显。</p><h3 id="bootstrap-callout"><a href="#bootstrap-callout" class="headerlink" title="bootstrap callout"></a>bootstrap callout</h3><h4 id="使用方式"><a href="#使用方式" class="headerlink" title="使用方式"></a>使用方式</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&#123;% note class_name %&#125; Content (md partial supported) &#123;% endnote %&#125;</span><br></pre></td></tr></table></figure><p>其中，class_name 可以是以下列表中的一个值：<br><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">default</span><br><span class="line">primary</span><br><span class="line">success</span><br><span class="line">info</span><br><span class="line">warning</span><br><span class="line">danger</span><br></pre></td></tr></table></figure></p><h4 id="演示效果"><a href="#演示效果" class="headerlink" title="演示效果"></a>演示效果</h4><div class="note default">            <p>此类别为default </p>          </div><div class="note primary">            <p>此类别为primary </p>          </div><div class="note success">            <p>此类别为success </p>          </div><div class="note info">            <p>此类别为info </p>          </div><div class="note warning">            <p>此类别为warning</p>          </div><div class="note danger">            <p>此类别为danger</p>          </div><div class="note primary">            <h4 id="本文作者：tongtong"><a href="#本文作者：tongtong" class="headerlink" title="本文作者：tongtong"></a>本文作者：tongtong</h4><h4 id="本文链接：https-stongtong-github-io"><a href="#本文链接：https-stongtong-github-io" class="headerlink" title="本文链接：https://stongtong.github.io/"></a>本文链接：<a href="https://stongtong.github.io/">https://stongtong.github.io/</a></h4><h4 id="版权申明：网站内容为tongtong所有，转载请注明出处。"><a href="#版权申明：网站内容为tongtong所有，转载请注明出处。" class="headerlink" title="版权申明：网站内容为tongtong所有，转载请注明出处。"></a>版权申明：网站内容为tongtong所有，转载请注明出处。</h4>          </div>]]></content>
    
    <summary type="html">
    
      &lt;p&gt;该文章测试next主题中的图片表示，因为看到官网上有很多的内置标签，测试一下，方便以后使用。&lt;/p&gt;
    
    </summary>
    
      <category term="next" scheme="https://tongfan.xyz/categories/next/"/>
    
    
      <category term="next" scheme="https://tongfan.xyz/tags/next/"/>
    
  </entry>
  
</feed>
